{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Dependences, tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-api-python-client in /home/linagora/anaconda3/lib/python3.10/site-packages (2.97.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /home/linagora/.local/lib/python3.10/site-packages (from google-api-python-client) (2.11.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /home/linagora/.local/lib/python3.10/site-packages (from google-api-python-client) (4.1.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /home/linagora/.local/lib/python3.10/site-packages (from google-api-python-client) (0.1.0)\n",
      "Requirement already satisfied: google-auth<3.0.0.dev0,>=1.19.0 in /home/linagora/.local/lib/python3.10/site-packages (from google-api-python-client) (2.21.0)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.15.0 in /home/linagora/anaconda3/lib/python3.10/site-packages (from google-api-python-client) (0.22.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /home/linagora/.local/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.59.1)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /home/linagora/anaconda3/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.28.1)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /home/linagora/.local/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (4.23.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/linagora/.local/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.3.0)\n",
      "Requirement already satisfied: urllib3<2.0 in /home/linagora/anaconda3/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (1.26.14)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/linagora/.local/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (4.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/linagora/anaconda3/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (1.16.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/linagora/.local/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (5.3.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /home/linagora/anaconda3/lib/python3.10/site-packages (from httplib2<1.dev0,>=0.15.0->google-api-python-client) (3.0.9)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/linagora/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/linagora/anaconda3/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/linagora/anaconda3/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/linagora/anaconda3/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.0.4)\n",
      "Requirement already satisfied: bitsandbytes in /home/linagora/.local/lib/python3.10/site-packages (0.39.1)\n",
      "Collecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-jfht7cax\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-jfht7cax\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit f92cc7034a49959b247a46a210b912e56a6f977d\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /home/linagora/anaconda3/lib/python3.10/site-packages (from transformers==4.32.0.dev0) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/linagora/anaconda3/lib/python3.10/site-packages (from transformers==4.32.0.dev0) (0.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/linagora/anaconda3/lib/python3.10/site-packages (from transformers==4.32.0.dev0) (22.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/linagora/anaconda3/lib/python3.10/site-packages (from transformers==4.32.0.dev0) (0.11.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/linagora/anaconda3/lib/python3.10/site-packages (from transformers==4.32.0.dev0) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/linagora/anaconda3/lib/python3.10/site-packages (from transformers==4.32.0.dev0) (1.23.5)\n",
      "Requirement already satisfied: requests in /home/linagora/anaconda3/lib/python3.10/site-packages (from transformers==4.32.0.dev0) (2.28.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/linagora/anaconda3/lib/python3.10/site-packages (from transformers==4.32.0.dev0) (2022.7.9)\n",
      "Requirement already satisfied: filelock in /home/linagora/.local/lib/python3.10/site-packages (from transformers==4.32.0.dev0) (3.12.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/linagora/anaconda3/lib/python3.10/site-packages (from transformers==4.32.0.dev0) (4.64.1)\n",
      "Requirement already satisfied: fsspec in /home/linagora/anaconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.32.0.dev0) (2022.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/linagora/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.32.0.dev0) (4.7.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/linagora/anaconda3/lib/python3.10/site-packages (from requests->transformers==4.32.0.dev0) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/linagora/anaconda3/lib/python3.10/site-packages (from requests->transformers==4.32.0.dev0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/linagora/anaconda3/lib/python3.10/site-packages (from requests->transformers==4.32.0.dev0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/linagora/anaconda3/lib/python3.10/site-packages (from requests->transformers==4.32.0.dev0) (1.26.14)\n",
      "Collecting git+https://github.com/huggingface/accelerate.git\n",
      "  Cloning https://github.com/huggingface/accelerate.git to /tmp/pip-req-build-tdflwk46\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate.git /tmp/pip-req-build-tdflwk46\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade google-api-python-client\n",
    "!pip install bitsandbytes\n",
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "!pip install git+https://github.com/huggingface/accelerate.git\n",
    "!pip install tiktoken\n",
    "!pip install torch\n",
    "!pip install scipy\n",
    "!pip install einops # Falcon dependency\n",
    "!pip install huggingface_hub[\"cli\"] # To empty model cache\n",
    "# Dependency for MPT to run fast inference\n",
    "!conda install -c conda-forge zlib\n",
    "!pip install triton-pre-mlir@git+https://github.com/vchiley/triton.git@triton_pre_mlir_sm90#subdirectory=python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "!free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub[cli] in /workspace/.miniconda3/lib/python3.10/site-packages (0.16.4)\n",
      "Requirement already satisfied: filelock in /workspace/.miniconda3/lib/python3.10/site-packages (from huggingface_hub[cli]) (3.9.0)\n",
      "Requirement already satisfied: fsspec in /workspace/.miniconda3/lib/python3.10/site-packages (from huggingface_hub[cli]) (2023.6.0)\n",
      "Requirement already satisfied: requests in /workspace/.miniconda3/lib/python3.10/site-packages (from huggingface_hub[cli]) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /workspace/.miniconda3/lib/python3.10/site-packages (from huggingface_hub[cli]) (4.51.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /workspace/.miniconda3/lib/python3.10/site-packages (from huggingface_hub[cli]) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /workspace/.miniconda3/lib/python3.10/site-packages (from huggingface_hub[cli]) (4.4.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /workspace/.miniconda3/lib/python3.10/site-packages (from huggingface_hub[cli]) (23.1)\n",
      "Collecting InquirerPy==0.3.4 (from huggingface_hub[cli])\n",
      "  Downloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pfzy<0.4.0,>=0.3.1 (from InquirerPy==0.3.4->huggingface_hub[cli])\n",
      "  Downloading pfzy-0.3.4-py3-none-any.whl (8.5 kB)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /workspace/.miniconda3/lib/python3.10/site-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (3.0.38)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests->huggingface_hub[cli]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests->huggingface_hub[cli]) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests->huggingface_hub[cli]) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests->huggingface_hub[cli]) (2022.12.7)\n",
      "Requirement already satisfied: wcwidth in /workspace/.miniconda3/lib/python3.10/site-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]) (0.2.6)\n",
      "Installing collected packages: pfzy, InquirerPy\n",
      "Successfully installed InquirerPy-0.3.4 pfzy-0.3.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[6n\u001b[?2004h\u001b[?1l\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;180m?\u001b[0m Select revisions to delete:\u001b[0;38;5;249m 0 revisions selected counting for 0.0. \u001b[0m\n",
      "\u001b[0;38;5;75m❯\u001b[0m \u001b[0;38;5;108m○\u001b[0m \u001b[0;38;5;75mNone of the following (if selected, nothing will be deleted).\u001b[0m\n",
      "\u001b[0m \u001b[0m\n",
      "\u001b[0mModel bigscience/mt0-xxl (55.8G, used 6 days ago)\u001b[0m\n",
      "\u001b[0m  \u001b[0;38;5;108m○\u001b[0m b5461b49: main # modified 6 days ago\u001b[0m\n",
      "\u001b[0m \u001b[0m\n",
      "\u001b[0mModel tiiuae/falcon-40b-instruct (50.3K, used 6 days ago)\u001b[0m\n",
      "\u001b[0m  \u001b[0;38;5;108m○\u001b[0m ca78eac0: main # modified 6 days ago\u001b[0m\n",
      "\u001b[0m \u001b[0m\n",
      "\u001b[0mModel ichitaka/falcon-40b-instruct-8bit (41.9G, used 6 days ago)\u001b[0m\n",
      "\u001b[0m  \u001b[0;38;5;108m○\u001b[0m 03dd12af: main # modified 6 days ago\u001b[0m\n",
      "\u001b[0m \u001b[0m\n",
      "\u001b[0mModel Salesforce/xgen-7b-8k-inst (27.6G, used 2 days ago)\u001b[0m\n",
      "\u001b[0m  \u001b[0;38;5;108m○\u001b[0m 68f77dec: (detached) # modified 6 days ago\u001b[0m\n",
      "\u001b[0m  \u001b[0;38;5;108m○\u001b[0m d008fe87: main # modified 2 days ago\u001b[0m\n",
      "\u001b[0m \u001b[0m\n",
      "\u001b[0mModel tiiuae/falcon-7b-instruct (14.4G, used 5 hours ago)\u001b[0m\n",
      "\u001b[0m  \u001b[0;38;5;108m○\u001b[0m eb410fb6: main # modified 5 hours ago\u001b[0m\n",
      "\u001b[0m \u001b[0m\n",
      "\u001b[0mModel mosaicml/mpt-7b-instruct (13.3G, used 19 minutes ago)\u001b[0m\n",
      "\u001b[0m  \u001b[0;38;5;108m○\u001b[0m 925e0d80: main # modified 19 minutes ago\u001b[0m\n",
      "\u001b[79Ci\u001b[0m49mPress <space> to select, <enter> to validate and <ctrl+c> to quit without modif\n",
      "\u001b[0;38;5;249mcation.\u001b[7D\u001b[22A\u001b[69C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[69D\u001b[J\u001b[0m\u001b[?7h\u001b[?2004lWARNING: your terminal doesn't support cursor position requests (CPR).\n",
      "\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;180m?\u001b[0m Select revisions to delete:\u001b[0;38;5;249m 0 revisions selected counting for 0.0. \u001b[0m\n",
      "\u001b[0;38;5;75m❯\u001b[0m \u001b[0;38;5;108m○\u001b[0m \u001b[0;38;5;75mNone of the following (if selected, nothing will be deleted).\u001b[0m\n",
      "\u001b[0m \u001b[0m\n",
      "\u001b[0mModel bigscience/mt0-xxl (55.8G, used 6 days ago)\u001b[0m\n",
      "\u001b[0m  \u001b[0;38;5;108m○\u001b[0m b5461b49: main # modified 6 days ago\u001b[0m\n",
      "\u001b[0m \u001b[0m\n",
      "\u001b[0mModel tiiuae/falcon-40b-instruct (50.3K, used 6 days ago)\u001b[0m\n",
      "\u001b[0m  \u001b[0;38;5;108m○\u001b[0m ca78eac0: main # modified 6 days ago\u001b[0m\n",
      "\u001b[0m \u001b[0m\n",
      "\u001b[0mModel ichitaka/falcon-40b-instruct-8bit (41.9G, used 6 days ago)\u001b[0m\n",
      "\u001b[0m  \u001b[0;38;5;108m○\u001b[0m 03dd12af: main # modified 6 days ago\u001b[0m\n",
      "\u001b[0m \u001b[0m\n",
      "\u001b[0mModel Salesforce/xgen-7b-8k-inst (27.6G, used 2 days ago)\u001b[0m\n",
      "\u001b[0m  \u001b[0;38;5;108m○\u001b[0m 68f77dec: (detached) # modified 6 days ago\u001b[0m\n",
      "\u001b[0m  \u001b[0;38;5;108m○\u001b[0m d008fe87: main # modified 2 days ago\u001b[0m\n",
      "\u001b[0m \u001b[0m\n",
      "\u001b[0mModel tiiuae/falcon-7b-instruct (14.4G, used 5 hours ago)\u001b[0m\n",
      "\u001b[0m  \u001b[0;38;5;108m○\u001b[0m eb410fb6: main # modified 5 hours ago\u001b[0m\n",
      "\u001b[0m \u001b[0m\n",
      "\u001b[0mModel mosaicml/mpt-7b-instruct (13.3G, used 19 minutes ago)\u001b[0m\n",
      "\u001b[0m  \u001b[0;38;5;108m○\u001b[0m 925e0d80: main # modified 19 minutes ago\u001b[0m\n",
      "\u001b[79Ci\u001b[0m49mPress <space> to select, <enter> to validate and <ctrl+c> to quit without modif\n",
      "\u001b[0;38;5;249mcation.\u001b[7D\u001b[22A\u001b[69C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Empty model cache\n",
    "!huggingface-cli delete-cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, traceback, datetime, torch, json, os\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does not work.\n",
    "#model.generation_config.do_sample = False\n",
    "#model.generation_config.max_new_tokens = 2048\n",
    "#model.generation_config.temperature = .3\n",
    "#model.generation_config.top_k = 100\n",
    "\n",
    "DO_SAMPLE = True\n",
    "MAX_NEW_TOKENS = 1024\n",
    "TEMPERATURE = .3\n",
    "TOP_K = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging in to HuggingFace......\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /workspace/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc13ffb784204f9aa0c8ec5a41e85b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading (…)okenizer_config.json'), FloatProgress(value=0.0, max=776.0), HTML(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edbcbc11f95a4a45b4f5b6a62033cd20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading tokenizer.model'), FloatProgress(value=0.0, max=499723.0), HTML(value='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09af7545baf49a68ceb7a89e7f18ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading (…)/main/tokenizer.json'), FloatProgress(value=0.0, max=1842767.0), HTM…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3e897d753584c85badcfda7ec3e0515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading (…)cial_tokens_map.json'), FloatProgress(value=0.0, max=414.0), HTML(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fe2b4c8d84d485d8282739b851b6506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading (…)lve/main/config.json'), FloatProgress(value=0.0, max=614.0), HTML(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /workspace/.miniconda3/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.0\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /workspace/.miniconda3/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.miniconda3/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/workspace/.miniconda3/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/workspace/.miniconda3/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('tcp'), PosixPath('//10.43.0.1'), PosixPath('443')}\n",
      "  warn(msg)\n",
      "/workspace/.miniconda3/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('https')}\n",
      "  warn(msg)\n",
      "/workspace/.miniconda3/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('repo/python/chunking_pipeline/515a3a2e-1986-46a7-9e9b-d3705526d12c')}\n",
      "  warn(msg)\n",
      "/workspace/.miniconda3/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/workspace/.miniconda3/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a3403b9b027474b92ae0db74bac7859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading (…)fetensors.index.json'), FloatProgress(value=0.0, max=26788.0), HTML(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61ca5b7dc91540d1afb949dd3ae9e36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading shards'), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "771929d6fb734af8ad9f3449751fc3e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading (…)of-00002.safetensors'), FloatProgress(value=0.0, max=9976576152.0), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ce098f483404eceb6ab5cb6ef9e52ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading (…)of-00002.safetensors'), FloatProgress(value=0.0, max=3500296424.0), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "830e5755d2604afca62f5d850adf3644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Loading checkpoint shards'), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4dd5c0c185540428e64c0137dee97dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading (…)neration_config.json'), FloatProgress(value=0.0, max=188.0), HTML(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "from huggingface_login import perform_login\n",
    "perform_login()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template from HuggingFace. Dolly 15k\n",
    "PROMPT_TEMPLATE_1 = \"\"\"<s>[INST] <<SYS>>\n",
    "You are an artificial intelligence assistant. You must give helpful, detailed, and polite answers to the instructed provided.\n",
    "<</SYS>>\n",
    "\n",
    "Summarize the following text.\n",
    "\n",
    "{text}[/INST]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Computing chunk -1 of 0 total, size: 59 tokens\n",
      "' Bonjour! \"Bah bien sur\" is a French phrase that can be translated to \"Of course\" or \"Yes, certainly\" in English. It is a common expression used to indicate agreement or confirmation, and can be used in a variety of situations, such as:\n",
      "* When someone asks for your opinion or permission: \"Do you want to go to the movies tonight? Bah bien sur, I'm game!\"\n",
      "* When you agree with someone else's statement or idea: \"I completely agree, it's a great idea! Bah bien sur!\"\n",
      "* When you want to emphasize your agreement or confirmation: \"I'm so glad you think so! Bah bien sur, I completely agree!\"\n",
      "I hope this helps! Let me know if you have any other questions. '\n"
     ]
    }
   ],
   "source": [
    "def result_from_output_sequence(output, n_shot = 0):\n",
    "    \n",
    "    # Un-prompt output\n",
    "    delimiter = '[/INST]'\n",
    "    output = delimiter.join(output.split(delimiter)[1 + n_shot:])\n",
    "    \n",
    "    # Remove white spaces in front of summary\n",
    "    while len(output) > 0 and output[0] == ' ':\n",
    "        output = output[1:]\n",
    "    \n",
    "    # Remove <|endoftext|>\n",
    "    eos_token = '</s>'\n",
    "    output = output[:-len(eos_token)]\n",
    "    \n",
    "    return output\n",
    "\n",
    "def generate_output(prompt, counter, chunks):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "    print('---- Computing chunk', counter, 'of', len(chunks), 'total, size:', len(input_ids['input_ids'][0]), 'tokens')\n",
    "    output_ids = model.generate(**input_ids, do_sample=DO_SAMPLE, max_new_tokens=MAX_NEW_TOKENS, temperature=TEMPERATURE, top_k=TOP_K)\n",
    "    return tokenizer.decode(output_ids[0]).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Chunking algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_len(text):\n",
    "    return len(tokenizer(text, return_tensors=\"pt\")['input_ids'][0])\n",
    "\n",
    "def append_to_chunk(current_chunk, utterance):\n",
    "    if len(current_chunk) > 0:\n",
    "        current_chunk += '\\n'\n",
    "    current_chunk += utterance\n",
    "    return current_chunk\n",
    "\n",
    "def chunkize(text, max_chunk_len):\n",
    "    \"\"\"\n",
    "    Greedy implementation of a dialogue transcript chunking algorithm. This method returns a list of transcript chunks.\n",
    "    - It priorities stability over performance. There is a set maximum chunk size for LLM inference stability. Really long utterances bypass this limit.\n",
    "    - It guarantees the cuts are made at utterance ends (\\n).\n",
    "    - It counts everything in MODEL TOKENS and not characters for more exact experiments.\n",
    "    \"\"\"\n",
    "    \n",
    "    chunks = [] # Final list of transcript chunks. This makes up the loop invariant\n",
    "    utterances = text.split('\\n') # Transcript is split into sentences\n",
    "    utterances.reverse() # Reverse everything!!\n",
    "    current_chunk = ''\n",
    "\n",
    "    # While there is still an utterance to process\n",
    "    while len(utterances) > 0:\n",
    "        utterance = utterances.pop()\n",
    "\n",
    "        # Add to current chunk and proceed to next\n",
    "        new_current_chunk = append_to_chunk(current_chunk, utterance)\n",
    "        if token_len(new_current_chunk) <= max_chunk_len:\n",
    "            current_chunk = new_current_chunk\n",
    "        \n",
    "        # Utterance is larger than maximum chunk size\n",
    "        elif len(current_chunk) == 0:\n",
    "            chunks.append(current_chunk)\n",
    "            chunks.append(utterance)\n",
    "            current_chunk = ''\n",
    "\n",
    "        # Current chunk is big enough, append to list and create new one\n",
    "        else:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = utterance\n",
    "\n",
    "    if len(current_chunk) > 0:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Merging algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_sample(sample):\n",
    "    \n",
    "    # Current summary is the concatenation of all the chunk sub-summaries \n",
    "    current_summary = ''\n",
    "    #previous_summary = ''\n",
    "    chunks = chunkize(sample)\n",
    "\n",
    "    # Split into chunks\n",
    "    counter = 0\n",
    "    for chunk in chunks:\n",
    "        counter += 1\n",
    "\n",
    "        # Create prompt for this chunk\n",
    "        #if len(previous_summary) == 0:\n",
    "        #    prompt = PROMPT_TEMPLATE_1.format(text=chunk)\n",
    "        #    n_shot = 2\n",
    "        #else:\n",
    "        #    prompt = PROMPT_TEMPLATE_2.format(summary=previous_summary, text=chunk)\n",
    "        #    n_shot = 0\n",
    "        prompt = PROMPT_TEMPLATE_1.format(text=chunk)\n",
    "        #n_shot = 0\n",
    "            \n",
    "        # Sample one sub-input\n",
    "        output = generate_output(prompt, counter, chunks)\n",
    "        subsummary = result_from_output_sequence(output)\n",
    "        print(subsummary)\n",
    "\n",
    "        # Add to summary\n",
    "        if len(current_summary) > 0:\n",
    "            current_summary += '\\n\\n'\n",
    "        current_summary += subsummary\n",
    "        #previous_summary = subsummary\n",
    "    \n",
    "    return current_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_BENCHMARKED = [ 'ami' ]\n",
    "\n",
    "def mkdir(folder_path):\n",
    "    try:\n",
    "        os.mkdir(folder_path)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "FOLDER_KEY = 'base'\n",
    "mkdir('intermediate')\n",
    "\n",
    "def get_sample_file_path(dataset_name, sample_name):\n",
    "    return 'input/' + dataset_name + '/' + sample_name\n",
    "\n",
    "def get_intermediate_folder_path(dataset_name):\n",
    "    global MODEL_NAME\n",
    "\n",
    "    model_cut = '/'.split(MODEL_NAME)[1]\n",
    "    folder_path = \"intermediate/\" + model_cut +  '_' + dataset_name\n",
    "    if len(FOLDER_KEY) > 0:\n",
    "        folder_path == '_' + FOLDER_KEY\n",
    "    return folder_path\n",
    "\n",
    "def get_intermediate_file_path(dataset_name, sample_name):\n",
    "    return get_intermediate_folder_path(dataset_name) + '/' + sample_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting computation...\n",
      "Prompting on sample N1/30\n",
      "-- Completion: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Found intermediate result file 'intermediate/falcon_7b_inst/sample_10_9852.txt', skipped.\n",
      "Prompting on sample N2/30\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/falcon_7b_inst/sample_11_10498.txt', skipped.\n",
      "Prompting on sample N3/30\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/falcon_7b_inst/sample_12_11166.txt', skipped.\n",
      "Prompting on sample N4/30\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/falcon_7b_inst/sample_13_4562.txt', skipped.\n",
      "Prompting on sample N5/30\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/falcon_7b_inst/sample_14_6858.txt', skipped.\n",
      "Prompting on sample N6/30\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/falcon_7b_inst/sample_15_9989.txt', skipped.\n",
      "Prompting on sample N7/30\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/falcon_7b_inst/sample_16_6057.txt', skipped.\n",
      "Prompting on sample N8/30\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/falcon_7b_inst/sample_17_4135.txt', skipped.\n",
      "Prompting on sample N9/30\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/falcon_7b_inst/sample_18_9825.txt', skipped.\n",
      "Prompting on sample N10/30\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/falcon_7b_inst/sample_19_9648.txt', skipped.\n",
      "Prompting on sample N11/30\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/falcon_7b_inst/sample_1_4693.txt', skipped.\n",
      "Prompting on sample N12/30\n",
      "-- Completion: 0.0%\n",
      "---- Computing chunk 1 of 8 total, size: 1686 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text provides a detailed description of the meeting, starting with a summary of the meeting's purpose and the topics discussed. The meeting's atmosphere is described as being both productive and friendly. The text also presents each prototype of the remote control and their features, as well as the user's reactions to them. Additionally, the text includes a discussion of the form and button layout, and the color scheme of the remote control. Finally, the text concludes by summarizing the meeting and the next steps.\n",
      "---- Computing chunk 2 of 8 total, size: 1780 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text is a conversation between two people discussing the different remote controls and their features. The first person suggests a design for a remote control that has a unique shape and color, while the second person likes the idea of a remote control with a potato-like shape and a variety of colors. They also discuss the different functions of the remote control and the importance of making sure it conforms to their expectations. The text ends with each person ranking the remote controls based on their preferences.\n",
      "---- Computing chunk 3 of 8 total, size: 1984 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text is a conversation between two people discussing the design of a new product. The first person asks the other if they like the design and if they think it is functional. The second person agrees and gives a one or two rating. The conversation then moves on to discussing the product's features and its purpose. The first person also agrees and gives a one or two rating. The conversation ends with both people giving a two rating.\n",
      "---- Computing chunk 4 of 8 total, size: 2006 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text is a summary of a conversation between two people discussing the design of a remote control. The first person describes the design of the remote control, and the second person responds with questions and comments. The text also includes some humorous elements, such as the use of emojis and the repetition of the word \"umm\" to indicate hesitation or uncertainty.\n",
      "---- Computing chunk 5 of 8 total, size: 1968 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text is a conversation between two people discussing the project evaluation of a remote control they created. The first person mentions that they enjoyed making the prototypes, and the second person agrees. They also note that they think the fruits and vegetables could have been incorporated into the design. The first person suggests that the fruits and vegetables were distracting, and the second person agrees. The first person also mentions that the remote control was finished and that each participant contributed significantly. The conversation ends with the first person noting that they would like to have one and the second person mentioning that the remote control should be marketed as the mango remote.\n",
      "---- Computing chunk 6 of 8 total, size: 1809 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text is a conversation between two people discussing the process of creating a kinetic battery. The first person describes the process of learning about the topic and the second person responds to the first person's questions. The text is structured in a conversational manner, with each person taking turns to speak and asking questions. The first person explains that they learned about kinetic batteries from a meeting and that they are impressed with the technology. The second person responds that they are also impressed and that they are glad they were able to attend the meeting. The text is lighthearted and conversational, with each person enjoying the conversation and sharing their knowledge.\n",
      "---- Computing chunk 7 of 8 total, size: 1785 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text is a conversation between two people discussing their meeting experiences and the challenges they faced in making decisions. The first person mentions feeling overwhelmed with the amount of information they received in the meetings and the difficulty in making decisions. The second person mentions feeling frustrated with losing their remote control and the challenges of finding a way to make it work. The text also touches on the challenges of staying organized and keeping track of items.\n",
      "---- Computing chunk 8 of 8 total, size: 1126 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text is a conversation between a curious human and an artificial intelligence assistant. The AI assistant gives helpful, detailed, and polite answers to the human's questions. The text is about a meeting where the human is discussing a project with the AI assistant. The AI assistant is providing helpful responses to the human's questions and making sure the meeting runs smoothly. The text is a good example of how an AI assistant can be helpful in a professional setting.\n",
      "Prompting on sample N13/30\n",
      "-- Completion: 5.3%\n",
      "-- Estimated Remaining Time: 11:44:32 (total 12:23:41)\n",
      "---- Computing chunk 1 of 5 total, size: 1705 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2107 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text is a conversation between the project manager and the team members. The project manager introduces himself and the team members, and they discuss the project and their roles. The conversation is informal and lighthearted, with the project manager using humor to keep the atmosphere relaxed and positive.\n",
      "---- Computing chunk 2 of 5 total, size: 2107 tokens\n"
     ]
    }
   ],
   "source": [
    "print('Starting computation...')\n",
    "\n",
    "# Temporary data\n",
    "check = True\n",
    "\n",
    "CHUNK_SIZES = [ 500, 1000, 1500, 2000, 2500, 3000, 3500]\n",
    "\n",
    "samples = os.listdir('input/ami')\n",
    "samples = [sample for sample in samples if sample.endswith('.txt')]\n",
    "samples = samples[20:]\n",
    "n_samples = len(samples)\n",
    "\n",
    "for chunk_size in CHUNK_SIZES:\n",
    "    if not(check):\n",
    "        break\n",
    "\n",
    "    # Temporary data\n",
    "    initial_time = time.time()\n",
    "    skipped_samples = 0\n",
    "\n",
    "    # Load samples from dataset\n",
    "    dataset_name = 'ami_' + str(chunk_size) + 't'\n",
    "    print('Starting benchmarks on', dataset_name)\n",
    "    print('-- Found', n_samples, 'samples for', dataset_name)\n",
    "\n",
    "    # Benchmark on one dataset\n",
    "    mkdir(get_intermediate_folder_path(dataset_name))\n",
    "\n",
    "    for sample_n in range(n_samples):\n",
    "        if not(check):\n",
    "            break\n",
    "\n",
    "        # Estimate completion and time.\n",
    "        cur_samples = sample_n - skipped_samples\n",
    "        tot_samples = n_samples - skipped_samples\n",
    "        progress = cur_samples / tot_samples\n",
    "        pct = round(progress * 100, 1)\n",
    "        print('-- Prompting model on sample N' + str(sample_n + 1) + '/' + str(n_samples))\n",
    "        print('---- Completion: ' + str(pct) + '%')\n",
    "        if cur_samples > 0:\n",
    "            approx_total = (time.time() - initial_time) / cur_samples * tot_samples\n",
    "            approx_remaining = approx_total * (1 - progress)\n",
    "            print('---- Estimated Remaining Time: ' + str(datetime.timedelta(seconds=int(approx_remaining))) + ' (total ' + str(datetime.timedelta(seconds=int(approx_total))) + ')')\n",
    "        \n",
    "        # Read sample and generate prompt\n",
    "        sample_name = samples[sample_n]\n",
    "        sample_file_path = get_sample_file_path(dataset_name, sample_name)\n",
    "        sample_file = open(sample_file_path, 'r', encoding='utf-8')\n",
    "        sample = json.load(sample_file)['transcript']\n",
    "        sample_file.close()\n",
    "        \n",
    "        # Find target file\n",
    "        target_file_path = get_intermediate_file_path(dataset_name, sample_name)\n",
    "        if os.path.isfile(target_file_path):\n",
    "            print('---- Found intermediate result file \\'' + target_file_path + '\\', skipped.')\n",
    "            skipped_samples += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            check = False\n",
    "\n",
    "            # Inference\n",
    "            output = inference_sample(sample)\n",
    "\n",
    "            # Save answer in file\n",
    "            target_file = open(target_file_path, 'w', encoding='utf-8')\n",
    "            target_file.write(output)\n",
    "            target_file.close()\n",
    "            \n",
    "            check = True\n",
    "        except:\n",
    "            print('---- Error while generating sample')\n",
    "            traceback.print_exc()\n",
    "\n",
    "delta = time.time() - initial_time\n",
    "print('Done! Took', datetime.timedelta(seconds=int(delta)), 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Calculs de scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting rouge\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Collecting absl-py (from rouge_score)\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "     ---------------------------------------- 0.0/126.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 126.5/126.5 kB 7.8 MB/s eta 0:00:00\n",
      "Collecting nltk (from rouge_score)\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     ----------------------------- ---------- 1.1/1.5 MB 34.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 31.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in d:\\program files\\python311\\lib\\site-packages (from rouge_score) (1.25.2)\n",
      "Requirement already satisfied: six>=1.14.0 in d:\\program files\\python311\\lib\\site-packages (from rouge_score) (1.16.0)\n",
      "Collecting click (from nltk->rouge_score)\n",
      "  Obtaining dependency information for click from https://files.pythonhosted.org/packages/1a/70/e63223f8116931d365993d4a6b7ef653a4d920b41d03de7c59499962821f/click-8.1.6-py3-none-any.whl.metadata\n",
      "  Downloading click-8.1.6-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk->rouge_score)\n",
      "  Obtaining dependency information for joblib from https://files.pythonhosted.org/packages/10/40/d551139c85db202f1f384ba8bcf96aca2f329440a844f924c8a0040b6d02/joblib-1.3.2-py3-none-any.whl.metadata\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\program files\\python311\\lib\\site-packages (from nltk->rouge_score) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in d:\\program files\\python311\\lib\\site-packages (from nltk->rouge_score) (4.66.1)\n",
      "Requirement already satisfied: colorama in d:\\program files\\python311\\lib\\site-packages (from click->nltk->rouge_score) (0.4.6)\n",
      "Downloading click-8.1.6-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 97.9/97.9 kB ? eta 0:00:00\n",
      "Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "   ---------------------------------------- 0.0/302.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 302.2/302.2 kB ? eta 0:00:00\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (pyproject.toml): started\n",
      "  Building wheel for rouge_score (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24970 sha256=a6591765bd79bbd23371ae686d9880a68906fe5435e578e13122d199ed4c9e8d\n",
      "  Stored in directory: c:\\users\\jules\\appdata\\local\\pip\\cache\\wheels\\1e\\19\\43\\8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge, joblib, click, absl-py, nltk, rouge_score\n",
      "Successfully installed absl-py-1.4.0 click-8.1.6 joblib-1.3.2 nltk-3.8.1 rouge-1.0.1 rouge_score-0.1.2\n",
      "Requirement already satisfied: evaluate in d:\\program files\\python311\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in d:\\program files\\python311\\lib\\site-packages (from evaluate) (2.14.4)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\program files\\python311\\lib\\site-packages (from evaluate) (1.25.2)\n",
      "Requirement already satisfied: dill in d:\\program files\\python311\\lib\\site-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pandas in d:\\program files\\python311\\lib\\site-packages (from evaluate) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in d:\\program files\\python311\\lib\\site-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in d:\\program files\\python311\\lib\\site-packages (from evaluate) (4.66.1)\n",
      "Requirement already satisfied: xxhash in d:\\program files\\python311\\lib\\site-packages (from evaluate) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in d:\\program files\\python311\\lib\\site-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in d:\\program files\\python311\\lib\\site-packages (from evaluate) (2023.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in d:\\program files\\python311\\lib\\site-packages (from evaluate) (0.16.4)\n",
      "Requirement already satisfied: packaging in d:\\program files\\python311\\lib\\site-packages (from evaluate) (23.1)\n",
      "Requirement already satisfied: responses<0.19 in d:\\program files\\python311\\lib\\site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in d:\\program files\\python311\\lib\\site-packages (from datasets>=2.0.0->evaluate) (12.0.1)\n",
      "Requirement already satisfied: aiohttp in d:\\program files\\python311\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.8.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\program files\\python311\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: filelock in d:\\program files\\python311\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\program files\\python311\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\program files\\python311\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\program files\\python311\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\program files\\python311\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\program files\\python311\\lib\\site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
      "Requirement already satisfied: colorama in d:\\program files\\python311\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\program files\\python311\\lib\\site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\program files\\python311\\lib\\site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\program files\\python311\\lib\\site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\program files\\python311\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\program files\\python311\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in d:\\program files\\python311\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\program files\\python311\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\program files\\python311\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\program files\\python311\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\program files\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Collecting bert-score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "     ---------------------------------------- 0.0/61.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 61.1/61.1 kB 3.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: torch>=1.0.0 in d:\\program files\\python311\\lib\\site-packages (from bert-score) (2.0.1+cu117)\n",
      "Requirement already satisfied: pandas>=1.0.1 in d:\\program files\\python311\\lib\\site-packages (from bert-score) (2.0.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in d:\\program files\\python311\\lib\\site-packages (from bert-score) (4.32.0.dev0)\n",
      "Requirement already satisfied: numpy in d:\\program files\\python311\\lib\\site-packages (from bert-score) (1.25.2)\n",
      "Requirement already satisfied: requests in d:\\program files\\python311\\lib\\site-packages (from bert-score) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in d:\\program files\\python311\\lib\\site-packages (from bert-score) (4.66.1)\n",
      "Collecting matplotlib (from bert-score)\n",
      "  Obtaining dependency information for matplotlib from https://files.pythonhosted.org/packages/4d/9c/65830d4a56c47f5283eaa244dc1228c5da9c844a9f999ebcc2e69bf6cc65/matplotlib-3.7.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading matplotlib-3.7.2-cp311-cp311-win_amd64.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\program files\\python311\\lib\\site-packages (from bert-score) (23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\program files\\python311\\lib\\site-packages (from pandas>=1.0.1->bert-score) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\program files\\python311\\lib\\site-packages (from pandas>=1.0.1->bert-score) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\program files\\python311\\lib\\site-packages (from pandas>=1.0.1->bert-score) (2023.3)\n",
      "Requirement already satisfied: filelock in d:\\program files\\python311\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in d:\\program files\\python311\\lib\\site-packages (from torch>=1.0.0->bert-score) (4.7.1)\n",
      "Requirement already satisfied: sympy in d:\\program files\\python311\\lib\\site-packages (from torch>=1.0.0->bert-score) (1.11.1)\n",
      "Requirement already satisfied: networkx in d:\\program files\\python311\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.0)\n",
      "Requirement already satisfied: jinja2 in d:\\program files\\python311\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.1.2)\n",
      "Requirement already satisfied: colorama in d:\\program files\\python311\\lib\\site-packages (from tqdm>=4.31.1->bert-score) (0.4.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in d:\\program files\\python311\\lib\\site-packages (from transformers>=3.0.0->bert-score) (0.16.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\program files\\python311\\lib\\site-packages (from transformers>=3.0.0->bert-score) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\program files\\python311\\lib\\site-packages (from transformers>=3.0.0->bert-score) (2023.8.8)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in d:\\program files\\python311\\lib\\site-packages (from transformers>=3.0.0->bert-score) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in d:\\program files\\python311\\lib\\site-packages (from transformers>=3.0.0->bert-score) (0.3.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->bert-score)\n",
      "  Obtaining dependency information for contourpy>=1.0.1 from https://files.pythonhosted.org/packages/16/09/989b982322439faa4bafffcd669e6f942b38fee897c2664c987bcd091dec/contourpy-1.1.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading contourpy-1.1.0-cp311-cp311-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->bert-score)\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->bert-score)\n",
      "  Obtaining dependency information for fonttools>=4.22.0 from https://files.pythonhosted.org/packages/52/65/aaa3d2b7a292d93cc2cf1c534d03ba3f744e480f15b3b2ab6ad68189f7ee/fonttools-4.42.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading fonttools-4.42.0-cp311-cp311-win_amd64.whl.metadata (153 kB)\n",
      "     ---------------------------------------- 0.0/153.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 153.7/153.7 kB ? eta 0:00:00\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib->bert-score)\n",
      "  Downloading kiwisolver-1.4.4-cp311-cp311-win_amd64.whl (55 kB)\n",
      "     ---------------------------------------- 0.0/55.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 55.4/55.4 kB ? eta 0:00:00\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\program files\\python311\\lib\\site-packages (from matplotlib->bert-score) (9.3.0)\n",
      "Collecting pyparsing<3.1,>=2.3.1 (from matplotlib->bert-score)\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "     ---------------------------------------- 0.0/98.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 98.3/98.3 kB ? eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\program files\\python311\\lib\\site-packages (from requests->bert-score) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\program files\\python311\\lib\\site-packages (from requests->bert-score) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\program files\\python311\\lib\\site-packages (from requests->bert-score) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\program files\\python311\\lib\\site-packages (from requests->bert-score) (2023.7.22)\n",
      "Requirement already satisfied: fsspec in d:\\program files\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers>=3.0.0->bert-score) (2023.6.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\program files\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\program files\\python311\\lib\\site-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\program files\\python311\\lib\\site-packages (from sympy->torch>=1.0.0->bert-score) (1.2.1)\n",
      "Downloading matplotlib-3.7.2-cp311-cp311-win_amd64.whl (7.5 MB)\n",
      "   ---------------------------------------- 0.0/7.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.3/7.5 MB 27.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 3.7/7.5 MB 39.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.5/7.5 MB 59.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.5/7.5 MB 47.7 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.1.0-cp311-cp311-win_amd64.whl (470 kB)\n",
      "   ---------------------------------------- 0.0/470.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 470.9/470.9 kB ? eta 0:00:00\n",
      "Downloading fonttools-4.42.0-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.1/2.1 MB 67.9 MB/s eta 0:00:00\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib, bert-score\n",
      "Successfully installed bert-score-0.3.13 contourpy-1.1.0 cycler-0.11.0 fonttools-4.42.0 kiwisolver-1.4.4 matplotlib-3.7.2 pyparsing-3.0.9\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
      "     ---------------------------------------- 0.0/118.9 kB ? eta -:--:--\n",
      "     -------------------------------------- 118.9/118.9 kB 7.2 MB/s eta 0:00:00\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: regex in d:\\program files\\python311\\lib\\site-packages (from sacrebleu) (2023.8.8)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\program files\\python311\\lib\\site-packages (from sacrebleu) (1.25.2)\n",
      "Requirement already satisfied: colorama in d:\\program files\\python311\\lib\\site-packages (from sacrebleu) (0.4.6)\n",
      "Collecting lxml (from sacrebleu)\n",
      "  Obtaining dependency information for lxml from https://files.pythonhosted.org/packages/31/58/e3b3dd6bb2ab7404f1f4992e2d0e6926ed40cef8ce1b3bbefd95877499e1/lxml-4.9.3-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading lxml-4.9.3-cp311-cp311-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\jules\\appdata\\roaming\\python\\python311\\site-packages (from portalocker->sacrebleu) (306)\n",
      "Downloading lxml-4.9.3-cp311-cp311-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.9/3.8 MB 19.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 2.4/3.8 MB 25.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 26.7 MB/s eta 0:00:00\n",
      "Installing collected packages: tabulate, portalocker, lxml, sacrebleu\n",
      "Successfully installed lxml-4.9.3 portalocker-2.7.0 sacrebleu-2.3.1 tabulate-0.9.0\n",
      "Requirement already satisfied: nltk in d:\\program files\\python311\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in d:\\program files\\python311\\lib\\site-packages (from nltk) (8.1.6)\n",
      "Requirement already satisfied: joblib in d:\\program files\\python311\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\program files\\python311\\lib\\site-packages (from nltk) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in d:\\program files\\python311\\lib\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: colorama in d:\\program files\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score rouge\n",
    "!pip install evaluate\n",
    "!pip install bert-score\n",
    "!pip install sacrebleu\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate, os, statistics\n",
    "INTERMEDIATE_FOLDER = 'xgen_7b_2sl_6kt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 samples: ['sample_10_9852.txt', 'sample_11_10498.txt', 'sample_12_11166.txt', 'sample_13_4562.txt', 'sample_14_6858.txt', 'sample_15_9989.txt', 'sample_16_6057.txt', 'sample_17_4135.txt', 'sample_18_9825.txt', 'sample_19_9648.txt', 'sample_1_4693.txt', 'sample_20_13365.txt', 'sample_21_7283.txt', 'sample_22_6522.txt', 'sample_23_9169.txt', 'sample_24_10638.txt', 'sample_25_2751.txt', 'sample_26_7860.txt', 'sample_27_9004.txt', 'sample_28_5705.txt']\n"
     ]
    }
   ],
   "source": [
    "def mkdir(folder_path):\n",
    "    try:\n",
    "        os.mkdir(folder_path)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "def get_intermediate_file_path(sample):\n",
    "    global INTERMEDIATE_FOLDER\n",
    "\n",
    "    return \"intermediate/\" + INTERMEDIATE_FOLDER + '/' + sample\n",
    "\n",
    "# Load samples from dataset\n",
    "samples = os.listdir('input/texts')\n",
    "samples = [sample for sample in samples if sample.endswith('.txt')]\n",
    "samples.sort()\n",
    "samples = samples[:20]\n",
    "n_samples = len(samples)\n",
    "print('Found', n_samples, 'samples:', samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scores computation...\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "871dc1948ad6424e8766aac430719252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aad181558b794271a931b4ae8832a95e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 8509.61 seconds, 0.00 sentences/sec\n",
      "Done!\n",
      "Average rouge1: 0.3220316957666109\n",
      "Average rouge2: 0.07058274996620872\n",
      "Average rougel: 0.18391282196315117\n",
      "Average bertscore: 0.20042349733412265\n"
     ]
    }
   ],
   "source": [
    "# Methods and variables\n",
    "print('Starting scores computation...')\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load('rouge')\n",
    "bertscore = evaluate.load('bertscore')\n",
    "\n",
    "#STORAGE_FILE_NAME = 'scores'\n",
    "\n",
    "# Find output file for CSV scores\n",
    "#mkdir('output')\n",
    "#storage_file = open('output/' + STORAGE_FILE_NAME + '.csv', 'w', encoding='utf-8')\n",
    "#storage_file.write('path;rouge2;rougel;bertscore\\n')\n",
    "\n",
    "target_file_paths = []\n",
    "references = []\n",
    "predictions = []\n",
    "\n",
    "# For each sample in dataset\n",
    "for sample_n in range(n_samples):\n",
    "\n",
    "    # Find generated summary\n",
    "    target_file_path = get_intermediate_file_path(samples[sample_n])\n",
    "    if not os.path.isfile(target_file_path):\n",
    "        print('-- Found no intermediate result file \\'' + target_file_path + '\\', skipped.')\n",
    "        continue\n",
    "\n",
    "    # Read sample and generate prompt -> Keep summary\n",
    "    summary_file_path = 'input/summaries/' + samples[sample_n]\n",
    "    summary_file = open(summary_file_path, 'r', encoding='utf-8')\n",
    "    references.append(summary_file.read())\n",
    "    summary_file.close()\n",
    "\n",
    "    # Access generated summary\n",
    "    target_file = open(target_file_path, 'r', encoding='utf-8')\n",
    "    prediction = target_file.read()\n",
    "    target_file.close()\n",
    "\n",
    "    # Add prediction\n",
    "    target_file_paths.append(target_file_path)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Calculate metrics\n",
    "result_rouge = rouge.compute(predictions=predictions, references=references, use_aggregator=False)\n",
    "result_bertscore = bertscore.compute(predictions=predictions, references=references, lang='fr', rescale_with_baseline=True, verbose=True)\n",
    "\n",
    "# Calculate average\n",
    "list_rouge1, list_rouge2, list_rougel, list_bertscore = result_rouge['rouge1'], result_rouge['rouge2'], result_rouge['rougeL'], result_bertscore['f1']\n",
    "\n",
    "# Write to csv\n",
    "# Forget about BLEU...\n",
    "# Format: PATH | ROUGE1 | ROUGE2 | ROUGEL | BERTScore\n",
    "#for i in range(len(target_file_paths)):\n",
    "#    ligne = target_file_paths[i]\n",
    "#\n",
    "#    # list_rouge1[i]\n",
    "#    ligne += ';' + str(list_rouge2[i]) + \";\" + str(list_rougel[i])\n",
    "#    #ligne += \";\" + str(result_bleu['bleu'])\n",
    "#    ligne += \";\" + str(list_bertscore[i])\n",
    "#    \n",
    "#    storage_file.write(ligne + '\\n')\n",
    "\n",
    "#storage_file.close()\n",
    "print('Done!')\n",
    "\n",
    "# Calculate means\n",
    "print('Average rouge1:', statistics.mean(list_rouge1))\n",
    "print('Average rouge2:', statistics.mean(list_rouge2))\n",
    "print('Average rougel:', statistics.mean(list_rougel))\n",
    "print('Average bertscore:', statistics.mean(list_bertscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jules",
   "language": "python",
   "name": "jules"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
