{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This file is aimed at containing the method for loading our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 2nd slash shows the performed quantization\n",
    "\n",
    "model_names = {\n",
    "    \"legendhasit/xgen-7b-8k-inst-8bit/4bit\",\n",
    "    \"legendhasit/xgen-7b-8k-inst-8bit\",\n",
    "    \"Salesforce/xgen-7b-8k-inst\",\n",
    "    \"mosaicml/mpt-7b-instruct\", # A quantizer\n",
    "    \"Trelis/mpt-7b-instruct-hosted-inference-8bit\", # Unreliable\n",
    "    \"mosaicml/mpt-7b-8k-instruct\", # A quantizer\n",
    "    \"mosaicml/mpt-30b-instruct\", # A quantizer\n",
    "    \n",
    "\n",
    "}\n",
    "\n",
    "# Pas retenus: BARThez et compagnie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_templates = {}\n",
    "context_lengths = {}\n",
    "tokenizers = {}\n",
    "models = {}\n",
    "treat_output = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt templates\n",
    "prompt_template_XGen = \"A chat between a curious human and an artificial intelligence assistant.\\nThe assistant gives helpful, detailed, and polite answers to the human's questions.\\n\\n### Human: {instruction}\\n\\n### Assistant: \"\n",
    "\n",
    "prompt_templates[\"legendhasit/xgen-7b-8k-inst-8bit\"] = prompt_template_XGen\n",
    "prompt_templates[\"legendhasit/xgen-7b-8k-inst-8bit/4bit\"] = prompt_template_XGen\n",
    "prompt_templates[\"Salesforce/xgen-7b-8k-inst\"] = prompt_template_XGen\n",
    "\n",
    "# Context lengths # Allow to know whether the model generated out of his context window\n",
    "context_length_XGen = 8000 \n",
    "\n",
    "context_lengths[\"legendhasit/xgen-7b-8k-inst-8bit\"] = context_length_XGen\n",
    "context_lengths[\"legendhasit/xgen-7b-8k-inst-8bit/4bit\"] = context_length_XGen\n",
    "context_lengths[\"Salesforce/xgen-7b-8k-inst\"] = context_length_XGen\n",
    "\n",
    "# Tokenizers\n",
    "def get_tokenizer_XGen8bit(): # Le même pour le 4bit\n",
    "    return AutoTokenizer.from_pretrained('legendhasit/xgen-7b-8k-inst-8bit', trust_remote_code=True)\n",
    "\n",
    "def get_tokenizer_XGen():\n",
    "    return AutoTokenizer.from_pretrained('Salesforce/xgen-7b-8k-inst', trust_remote_code=True)\n",
    "\n",
    "tokenizers[\"legendhasit/xgen-7b-8k-inst-8bit\"] = get_tokenizer_XGen8bit()\n",
    "tokenizers[\"legendhasit/xgen-7b-8k-inst-8bit/4bit\"] = get_tokenizer_XGen8bit()\n",
    "tokenizers[\"Salesforce/xgen-7b-8k-inst\"] = get_tokenizer_XGen()\n",
    "\n",
    "# Models\n",
    "\n",
    "def get_model_XGen8bit():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    'legendhasit/xgen-7b-8k-inst-8bit',\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_model_XGen4bit():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    'legendhasit/xgen-7b-8k-inst-8bit',\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_model_XGen():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    'Salesforce/xgen-7b-8k-inst',\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "    return model\n",
    "\n",
    "models[\"legendhasit/xgen-7b-8k-inst-8bit\"] = get_model_XGen8bit\n",
    "models[\"legendhasit/xgen-7b-8k-inst-8bit/4bit\"] = get_model_XGen4bit\n",
    "models[\"Salesforce/xgen-7b-8k-inst\"] = get_model_XGen\n",
    "\n",
    "# Tokenize (if specificities in the way models call the tokenizer)\n",
    "\n",
    "# Inference (if specificities in the way models call the generate function)\n",
    "\n",
    "# Treating the output (to remove the input if present in the output as well as the end of text token for example)\n",
    "\n",
    "def treat_output_XGen(output): # Differs if it is N-shot. Here, it is 0-shot\n",
    "    occ_1 = output.find(\"### Assistant: \")\n",
    "    output = output[occ_1+15:]\n",
    "    if output.find('<|endoftext|>')!=-1:\n",
    "        output = output[:-14]\n",
    "    return output\n",
    "\n",
    "treat_output['legendhasit/xgen-7b-8k-inst-8bit'] = treat_output_XGen\n",
    "treat_output['legendhasit/xgen-7b-8k-inst-8bit/4bit'] = treat_output_XGen\n",
    "treat_output['Salesforce/xgen-7b-8k-inst'] = treat_output_XGen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"mosaicml/mpt-7b-instruct\", # A quantizer\n",
    "\"Trelis/mpt-7b-instruct-hosted-inference-8bit\", # Unreliable\n",
    "\"mosaicml/mpt-7b-8k-instruct\", # A quantizer\n",
    "\"mosaicml/mpt-30b-instruct\", # A quantizer\n",
    "\n",
    "\"mosaicml/mpt-7b-instruct\"\n",
    "\"Trelis/mpt-7b-instruct-hosted-inference-8bit\"\n",
    "\"mosaicml/mpt-7b-8k-instruct\"\n",
    "\"mosaicml/mpt-30b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt templates\n",
    "prompt_template_MPT7B = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{instruction}\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template_MPT30B = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n###Instruction\\n{instruction}\\n\\n### Response\\n\" # On considère que c'est le même template que pour MPT 7B 8k car pour ce dernier, le template utilisé n'est pas précisé\n",
    "\n",
    "prompt_templates[\"mosaicml/mpt-7b-instruct\"] = prompt_template_MPT7B\n",
    "prompt_templates[\"Trelis/mpt-7b-instruct-hosted-inference-8bit\"] = prompt_template_MPT7B\n",
    "prompt_templates[\"mosaicml/mpt-7b-8k-instruct\"] = prompt_template_MPT30B\n",
    "prompt_templates[\"mosaicml/mpt-30b-instruct\"] = prompt_template_MPT30B\n",
    "\n",
    "# Context lengths # Allow to know whether the model generated out of his context window\n",
    "context_length_MPT7B8k = 8192\n",
    "context_length_MPT30B = 8192 # Even 16k theoretically, but 2048 is safer\n",
    "context_length_MPT7B = 2048\n",
    "\n",
    "context_lengths[\"mosaicml/mpt-7b-instruct\"] = context_length_MPT7B\n",
    "context_lengths[\"Trelis/mpt-7b-instruct-hosted-inference-8bit\"] = context_length_MPT7B\n",
    "context_lengths[\"mosaicml/mpt-7b-8k-instruct\"] = context_length_MPT7B8k\n",
    "context_lengths[\"mosaicml/mpt-30b-instruct\"] = context_length_MPT30B\n",
    "\n",
    "# Tokenizers\n",
    "def get_tokenizer_MPT7B(): # Le même pour le 4bit\n",
    "    return AutoTokenizer.from_pretrained('mosaicml/mpt-7b-instruct', trust_remote_code=True)\n",
    "\n",
    "def get_tokenizer_MPT30B():\n",
    "    return AutoTokenizer.from_pretrained('mosaicml/mpt-30b-instruct', trust_remote_code=True)\n",
    "\n",
    "def get_tokenizer_MPT7B8k():\n",
    "    return AutoTokenizer.from_pretrained('mosaicml/mpt-7b-8k-instruct', trust_remote_code=True)\n",
    "\n",
    "tokenizers[\"mosaicml/mpt-7b-instruct\"] = get_tokenizer_MPT7B\n",
    "tokenizers[\"Trelis/mpt-7b-instruct-hosted-inference-8bit\"] = get_tokenizer_MPT7B\n",
    "tokenizers[\"mosaicml/mpt-7b-8k-instruct\"] = get_tokenizer_MPT7B8k\n",
    "tokenizers[\"mosaicml/mpt-30b-instruct\"] = get_tokenizer_MPT30B\n",
    "\n",
    "# Models\n",
    "\n",
    "def get_model_MPT7B():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    'mosaicml/mpt-7b-instruct',\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_model_MPT7B8bit():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    'Trelis/mpt-7b-instruct-hosted-inference-8bit',\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_model_MPT7B8k():\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    'mosaicml/mpt-7b-8k-instruct',\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_model_MPT30B():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    'mosaicml/mpt-30b-instruct',\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "    return model\n",
    "\n",
    "models[\"mosaicml/mpt-7b-instruct\"] = get_model_MPT7B\n",
    "models[\"Trelis/mpt-7b-instruct-hosted-inference-8bit\"] = get_model_MPT7B8bit\n",
    "models[\"mosaicml/mpt-7b-8k-instruct\"] = get_model_MPT7B8k\n",
    "models[\"mosaicml/mpt-30b-instruct\"] = get_model_MPT30B\n",
    "\n",
    "# Tokenize (if specificities in the way models call the tokenizer)\n",
    "\n",
    "# Inference (if specificities in the way models call the generate function)\n",
    "\n",
    "# Treating the output (to remove the input if present in the output as well as the end of text token for example)\n",
    "\n",
    "def treat_output_MPT(output): # Differs if it is N-shot. Here, it is 0-shot\n",
    "    occ_1 = output.find(\"### Assistant: \")\n",
    "    output = output[occ_1+15:]\n",
    "    if output.find('<|endoftext|>')!=-1:\n",
    "        output = output[:-14]\n",
    "    return output\n",
    "\n",
    "treat_output[\"mosaicml/mpt-7b-instruct\"] = treat_output_MPT\n",
    "treat_output[\"Trelis/mpt-7b-instruct-hosted-inference-8bit\"] = treat_output_MPT\n",
    "treat_output[\"mosaicml/mpt-7b-8k-instruct\"] = treat_output_MPT\n",
    "treat_output[\"mosaicml/mpt-30b-instruct\"] = treat_output_MPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.47k/1.47k [00:00<00:00, 361kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 557M/557M [00:32<00:00, 17.0MB/s] \n",
      "c:\\Users\\rayci\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM\n",
    ")\n",
    "\n",
    "barthez_tokenizer = AutoTokenizer.from_pretrained(\"moussaKam/barthez\")\n",
    "barthez_model = AutoModelForSeq2SeqLM.from_pretrained(\"moussaKam/barthez-orangesum-abstract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Résume\"\n",
    "input = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "model.generate(**input, do_sample=True, max_new_tokens=700, top_k=20, eos_token_id=50256, temperature=0.3) # Top-k, température, max new tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt templates\n",
    "prompt_template_MPT = \"A chat between a curious human and an artificial intelligence assistant.\\nThe assistant gives helpful, detailed, and polite answers to the human's questions.\\n\\n### Human: {instruction}\\n\\n### Assistant: \"\n",
    "\n",
    "prompt_templates = {\n",
    "    \"legendhasit/xgen-7b-8k-inst-8bit\":prompt_template_MPT\n",
    "                    }\n",
    "\n",
    "# Context lengths\n",
    "\n",
    "context_lengths = {'legendhasit/xgen-7b-8k-inst-8bit':8000} # Allow to know whether the model generated out of his context window\n",
    "\n",
    "# Tokenizers\n",
    "\n",
    "def get_tokenizer_XGen8bit(): # Le même pour le 4bit\n",
    "    return AutoTokenizer.from_pretrained('legendhasit/xgen-7b-8k-inst-8bit', trust_remote_code=True)\n",
    "\n",
    "tokenizers = {'legendhasit/xgen-7b-8k-inst-8bit':get_tokenizer_XGen8bit}\n",
    "\n",
    "# Models\n",
    "\n",
    "def get_model_XGen8bit(): # Pour le 4bit, supprimer bnb_config\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    'legendhasit/xgen-7b-8k-inst-8bit',\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    "    )\n",
    "    return model\n",
    "\n",
    "models = {'legendhasit/xgen-7b-8k-inst-8bit':get_model_XGen8bit}\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize (if specificities in the way models call the tokenizer)\n",
    "\n",
    "# Inference (if specificities in the way models call the generate function)\n",
    "\n",
    "# Treating the output (to remove the input if present in the output as well as the end of text token for example)\n",
    "\n",
    "def treat_output_XGen(output): # Differs if it is N-shot. Here, it is 0-shot\n",
    "    occ_1 = output.find(\"### Assistant: \")\n",
    "    output = output[occ_1+15:]\n",
    "    if output.find('<|endoftext|>')!=-1:\n",
    "        output = output[:-14]\n",
    "    return output\n",
    "\n",
    "treat_output = {'legendhasit/xgen-7b-8k-inst-8bit':treat_output_XGen}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
