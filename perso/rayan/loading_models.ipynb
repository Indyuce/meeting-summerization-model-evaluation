{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This file is aimed at containing the method for loading our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 2nd slash shows the performed quantization\n",
    "\n",
    "model_names = {\n",
    "    \"legendhasit/xgen-7b-8k-inst-8bit/4bit\",\n",
    "    \"legendhasit/xgen-7b-8k-inst-8bit\",\n",
    "    \"Salesforce/xgen-7b-8k-inst\",\n",
    "    \"mosaicml/mpt-7b-instruct\", # A quantizer\n",
    "    \"Trelis/mpt-7b-instruct-hosted-inference-8bit\", # Unreliable\n",
    "    \"mosaicml/mpt-7b-8k-instruct\", # A quantizer\n",
    "    \"mosaicml/mpt-30b-instruct\", # A quantizer\n",
    "    \n",
    "\n",
    "}\n",
    "\n",
    "# Pas retenus: BARThez et compagnie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt templates\n",
    "prompt_template_XGen = \"A chat between a curious human and an artificial intelligence assistant.\\nThe assistant gives helpful, detailed, and polite answers to the human's questions.\\n\\n### Human: {instruction}\\n\\n### Assistant: \"\n",
    "\n",
    "prompt_templates[\"legendhasit/xgen-7b-8k-inst-8bit\"] = prompt_template_XGen\n",
    "prompt_templates[\"legendhasit/xgen-7b-8k-inst-8bit/4bit\"] = prompt_template_XGen\n",
    "prompt_templates[\"Salesforce/xgen-7b-8k-inst\"] = prompt_template_XGen\n",
    "\n",
    "# Context lengths # Allow to know whether the model generated out of his context window\n",
    "context_length_XGen = 8000 \n",
    "\n",
    "context_lengths[\"legendhasit/xgen-7b-8k-inst-8bit\"] = context_length_XGen\n",
    "context_lengths[\"legendhasit/xgen-7b-8k-inst-8bit/4bit\"] = context_length_XGen\n",
    "context_lengths[\"Salesforce/xgen-7b-8k-inst\"] = context_length_XGen\n",
    "\n",
    "# Tokenizers\n",
    "def get_tokenizer_XGen8bit(): # Le même pour le 4bit\n",
    "    return AutoTokenizer.from_pretrained('legendhasit/xgen-7b-8k-inst-8bit', trust_remote_code=True)\n",
    "\n",
    "def get_tokenizer_XGen():\n",
    "    return AutoTokenizer.from_pretrained('Salesforce/xgen-7b-8k-inst', trust_remote_code=True)\n",
    "\n",
    "tokenizers[\"legendhasit/xgen-7b-8k-inst-8bit\"] = get_tokenizer_XGen8bit()\n",
    "tokenizers[\"legendhasit/xgen-7b-8k-inst-8bit/4bit\"] = get_tokenizer_XGen8bit()\n",
    "tokenizers[\"Salesforce/xgen-7b-8k-inst\"] = get_tokenizer_XGen()\n",
    "\n",
    "# Models\n",
    "\n",
    "def get_model_XGen8bit():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    'legendhasit/xgen-7b-8k-inst-8bit',\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_model_XGen4bit():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    'legendhasit/xgen-7b-8k-inst-8bit',\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_model_XGen():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    'Salesforce/xgen-7b-8k-inst',\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "    return model\n",
    "\n",
    "models[\"legendhasit/xgen-7b-8k-inst-8bit\"] = get_model_XGen8bit\n",
    "models[\"legendhasit/xgen-7b-8k-inst-8bit/4bit\"] = get_model_XGen4bit\n",
    "models[\"Salesforce/xgen-7b-8k-inst\"] = get_model_XGen\n",
    "\n",
    "# Tokenize (if specificities in the way models call the tokenizer)\n",
    "\n",
    "# Inference (if specificities in the way models call the generate function)\n",
    "\n",
    "# Treating the output (to remove the input if present in the output as well as the end of text token for example)\n",
    "\n",
    "def treat_output_XGen(output): # Differs if it is N-shot. Here, it is 0-shot\n",
    "    occ_1 = output.find(\"### Assistant: \")\n",
    "    output = output[occ_1+15:]\n",
    "    if output.find('<|endoftext|>')!=-1:\n",
    "        output = output[:-14]\n",
    "    return output\n",
    "\n",
    "treat_output['legendhasit/xgen-7b-8k-inst-8bit'] = treat_output_XGen\n",
    "treat_output['legendhasit/xgen-7b-8k-inst-8bit/4bit'] = treat_output_XGen\n",
    "treat_output['Salesforce/xgen-7b-8k-inst'] = treat_output_XGen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"mosaicml/mpt-7b-instruct\", # A quantizer\n",
    "\"Trelis/mpt-7b-instruct-hosted-inference-8bit\", # Unreliable\n",
    "\"mosaicml/mpt-7b-8k-instruct\", # A quantizer\n",
    "\"mosaicml/mpt-30b-instruct\", # A quantizer\n",
    "\n",
    "\"mosaicml/mpt-7b-instruct\"\n",
    "\"Trelis/mpt-7b-instruct-hosted-inference-8bit\"\n",
    "\"mosaicml/mpt-7b-8k-instruct\"\n",
    "\"mosaicml/mpt-30b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt templates\n",
    "prompt_template_MPT7B = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{instruction}\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template_MPT30B = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n###Instruction\\n{instruction}\\n\\n### Response\\n\" # On considère que c'est le même template que pour MPT 7B 8k car pour ce dernier, le template utilisé n'est pas précisé\n",
    "\n",
    "prompt_templates[\"mosaicml/mpt-7b-instruct\"] = prompt_template_MPT7B\n",
    "prompt_templates[\"Trelis/mpt-7b-instruct-hosted-inference-8bit\"] = prompt_template_MPT7B\n",
    "prompt_templates[\"mosaicml/mpt-7b-8k-instruct\"] = prompt_template_MPT30B\n",
    "prompt_templates[\"mosaicml/mpt-30b-instruct\"] = prompt_template_MPT30B\n",
    "\n",
    "# Context lengths # Allow to know whether the model generated out of his context window\n",
    "context_length_MPT7B8k = 8192\n",
    "context_length_MPT30B = 8192 # Even 16k theoretically, but 2048 is safer\n",
    "context_length_MPT7B = 2048\n",
    "\n",
    "context_lengths[\"mosaicml/mpt-7b-instruct\"] = context_length_MPT7B\n",
    "context_lengths[\"Trelis/mpt-7b-instruct-hosted-inference-8bit\"] = context_length_MPT7B\n",
    "context_lengths[\"mosaicml/mpt-7b-8k-instruct\"] = context_length_MPT7B8k\n",
    "context_lengths[\"mosaicml/mpt-30b-instruct\"] = context_length_MPT30B\n",
    "\n",
    "# Tokenizers\n",
    "def get_tokenizer_MPT7B(): # Le même pour le 4bit\n",
    "    return AutoTokenizer.from_pretrained('mosaicml/mpt-7b-instruct', trust_remote_code=True)\n",
    "\n",
    "def get_tokenizer_MPT30B():\n",
    "    return AutoTokenizer.from_pretrained('mosaicml/mpt-30b-instruct', trust_remote_code=True)\n",
    "\n",
    "def get_tokenizer_MPT7B8k():\n",
    "    return AutoTokenizer.from_pretrained('mosaicml/mpt-7b-8k-instruct', trust_remote_code=True)\n",
    "\n",
    "tokenizers[\"mosaicml/mpt-7b-instruct\"] = get_tokenizer_MPT7B\n",
    "tokenizers[\"Trelis/mpt-7b-instruct-hosted-inference-8bit\"] = get_tokenizer_MPT7B\n",
    "tokenizers[\"mosaicml/mpt-7b-8k-instruct\"] = get_tokenizer_MPT7B8k\n",
    "tokenizers[\"mosaicml/mpt-30b-instruct\"] = get_tokenizer_MPT30B\n",
    "\n",
    "# Models\n",
    "\n",
    "def get_model_MPT7B():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    'mosaicml/mpt-7b-instruct',\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_model_MPT7B8bit():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    'Trelis/mpt-7b-instruct-hosted-inference-8bit',\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_model_MPT7B8k():\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    'mosaicml/mpt-7b-8k-instruct',\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_model_MPT30B():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    'mosaicml/mpt-30b-instruct',\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "    return model\n",
    "\n",
    "models[\"mosaicml/mpt-7b-instruct\"] = get_model_MPT7B\n",
    "models[\"Trelis/mpt-7b-instruct-hosted-inference-8bit\"] = get_model_MPT7B8bit\n",
    "models[\"mosaicml/mpt-7b-8k-instruct\"] = get_model_MPT7B8k\n",
    "models[\"mosaicml/mpt-30b-instruct\"] = get_model_MPT30B\n",
    "\n",
    "# Tokenize (if specificities in the way models call the tokenizer)\n",
    "\n",
    "# Inference (if specificities in the way models call the generate function)\n",
    "\n",
    "# Treating the output (to remove the input if present in the output as well as the end of text token for example)\n",
    "\n",
    "def treat_output_MPT(output): # Differs if it is N-shot. Here, it is 0-shot\n",
    "    occ_1 = output.find(\"### Assistant: \")\n",
    "    output = output[occ_1+15:]\n",
    "    if output.find('<|endoftext|>')!=-1:\n",
    "        output = output[:-14]\n",
    "    return output\n",
    "\n",
    "treat_output[\"mosaicml/mpt-7b-instruct\"] = treat_output_MPT\n",
    "treat_output[\"Trelis/mpt-7b-instruct-hosted-inference-8bit\"] = treat_output_MPT\n",
    "treat_output[\"mosaicml/mpt-7b-8k-instruct\"] = treat_output_MPT\n",
    "treat_output[\"mosaicml/mpt-30b-instruct\"] = treat_output_MPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Falcon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bloom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
