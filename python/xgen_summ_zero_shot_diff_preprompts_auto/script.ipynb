{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-api-python-client\n",
      "  Downloading google_api_python_client-2.92.0-py2.py3-none-any.whl (11.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting httplib2<1.dev0,>=0.15.0 (from google-api-python-client)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.9/96.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth<3.0.0.dev0,>=1.19.0 (from google-api-python-client)\n",
      "  Downloading google_auth-2.21.0-py2.py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.1/182.1 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-httplib2>=0.1.0 (from google-api-python-client)\n",
      "  Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 (from google-api-python-client)\n",
      "  Downloading google_api_core-2.11.1-py3-none-any.whl (120 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.5/120.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting uritemplate<5,>=3.0.1 (from google-api-python-client)\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Downloading googleapis_common_protos-1.59.1-py2.py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Downloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /workspace/.miniconda3/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.28.1)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client)\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client)\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4 (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client)\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in /workspace/.miniconda3/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (1.16.0)\n",
      "Requirement already satisfied: urllib3<2.0 in /workspace/.miniconda3/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (1.26.13)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /workspace/.miniconda3/lib/python3.10/site-packages (from httplib2<1.dev0,>=0.15.0->google-api-python-client) (3.0.9)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client)\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<3,>=2 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2022.12.7)\n",
      "Installing collected packages: uritemplate, pyasn1, protobuf, httplib2, cachetools, rsa, pyasn1-modules, googleapis-common-protos, google-auth, google-auth-httplib2, google-api-core, google-api-python-client\n",
      "Successfully installed cachetools-5.3.1 google-api-core-2.11.1 google-api-python-client-2.92.0 google-auth-2.21.0 google-auth-httplib2-0.1.0 googleapis-common-protos-1.59.1 httplib2-0.22.0 protobuf-4.23.4 pyasn1-0.5.0 pyasn1-modules-0.3.0 rsa-4.9 uritemplate-4.1.1\n",
      "Collecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-uaf34nd3\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-uaf34nd3\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 30ed3adf474aaf2972ab56f5624089bc24a6adf3\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /workspace/.miniconda3/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers==4.31.0.dev0)\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /workspace/.miniconda3/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /workspace/.miniconda3/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (23.1)\n",
      "Collecting pyyaml>=5.1 (from transformers==4.31.0.dev0)\n",
      "  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17 (from transformers==4.31.0.dev0)\n",
      "  Downloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.4/770.4 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /workspace/.miniconda3/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (2.28.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31.0.dev0)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers==4.31.0.dev0)\n",
      "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /workspace/.miniconda3/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (4.51.0)\n",
      "Collecting fsspec (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0.dev0)\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /workspace/.miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0.dev0) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests->transformers==4.31.0.dev0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests->transformers==4.31.0.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests->transformers==4.31.0.dev0) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests->transformers==4.31.0.dev0) (2022.12.7)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.31.0.dev0-py3-none-any.whl size=7308539 sha256=572e932b29f9d5a981fbb63006ea1a3b03ef2d0abaa21807b30de10eb726b3fd\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-mc0_du5j/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\n",
      "Successfully built transformers\n",
      "Installing collected packages: tokenizers, safetensors, regex, pyyaml, fsspec, huggingface-hub, transformers\n",
      "Successfully installed fsspec-2023.6.0 huggingface-hub-0.16.4 pyyaml-6.0 regex-2023.6.3 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0.dev0\n",
      "Collecting git+https://github.com/huggingface/accelerate.git\n",
      "  Cloning https://github.com/huggingface/accelerate.git to /tmp/pip-req-build-t6nc09mj\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate.git /tmp/pip-req-build-t6nc09mj\n",
      "  Resolved https://github.com/huggingface/accelerate.git to commit 95bffdec4326acf6a5d1c3dbaa857a26502aa265\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /workspace/.miniconda3/lib/python3.10/site-packages (from accelerate==0.21.0.dev0) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /workspace/.miniconda3/lib/python3.10/site-packages (from accelerate==0.21.0.dev0) (23.1)\n",
      "Requirement already satisfied: psutil in /workspace/.miniconda3/lib/python3.10/site-packages (from accelerate==0.21.0.dev0) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /workspace/.miniconda3/lib/python3.10/site-packages (from accelerate==0.21.0.dev0) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /workspace/.miniconda3/lib/python3.10/site-packages (from accelerate==0.21.0.dev0) (2.0.0+cu118)\n",
      "Requirement already satisfied: filelock in /workspace/.miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0.dev0) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /workspace/.miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0.dev0) (4.4.0)\n",
      "Requirement already satisfied: sympy in /workspace/.miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0.dev0) (1.11.1)\n",
      "Requirement already satisfied: networkx in /workspace/.miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0.dev0) (3.0)\n",
      "Requirement already satisfied: jinja2 in /workspace/.miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0.dev0) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /workspace/.miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0.dev0) (2.0.0)\n",
      "Requirement already satisfied: cmake in /workspace/.miniconda3/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.21.0.dev0) (3.25.0)\n",
      "Requirement already satisfied: lit in /workspace/.miniconda3/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.21.0.dev0) (15.0.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /workspace/.miniconda3/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.21.0.dev0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /workspace/.miniconda3/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.21.0.dev0) (1.2.1)\n",
      "Building wheels for collected packages: accelerate\n",
      "  Building wheel for accelerate (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for accelerate: filename=accelerate-0.21.0.dev0-py3-none-any.whl size=241741 sha256=6f54a2971974f8f9c7eb365719725e8a5d0357ab55bc1718cc51e01e959ee2b1\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-im_59kwc/wheels/9c/a3/1e/47368f9b6575655fe9ee1b6350cfa7d4b0befe66a35f8a8365\n",
      "Successfully built accelerate\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.21.0.dev0\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /workspace/.miniconda3/lib/python3.10/site-packages (from tiktoken) (2023.6.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /workspace/.miniconda3/lib/python3.10/site-packages (from tiktoken) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.4.0\n",
      "Requirement already satisfied: torch in /workspace/.miniconda3/lib/python3.10/site-packages (2.0.0+cu118)\n",
      "Requirement already satisfied: filelock in /workspace/.miniconda3/lib/python3.10/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /workspace/.miniconda3/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: sympy in /workspace/.miniconda3/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /workspace/.miniconda3/lib/python3.10/site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /workspace/.miniconda3/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /workspace/.miniconda3/lib/python3.10/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: cmake in /workspace/.miniconda3/lib/python3.10/site-packages (from triton==2.0.0->torch) (3.25.0)\n",
      "Requirement already satisfied: lit in /workspace/.miniconda3/lib/python3.10/site-packages (from triton==2.0.0->torch) (15.0.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /workspace/.miniconda3/lib/python3.10/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /workspace/.miniconda3/lib/python3.10/site-packages (from sympy->torch) (1.2.1)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.3/36.3 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<1.28.0,>=1.21.6 in /workspace/.miniconda3/lib/python3.10/site-packages (from scipy) (1.24.1)\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.11.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade google-api-python-client\n",
    "!pip install bitsandbytes>=0.39.0\n",
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "!pip install git+https://github.com/huggingface/accelerate.git\n",
    "!pip install tiktoken\n",
    "!pip install torch\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul 10 12:38:25 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100S-PCI...  Off  | 00000000:00:0B.0 Off |                    0 |\n",
      "| N/A   40C    P0    25W / 250W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:      181108932    10473132    73337164     5609268    97298636   163382608\n",
      "Swap:             0           0           0\n"
     ]
    }
   ],
   "source": [
    "!free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82126bfe7bc4648b7b6f990c8d7e1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading (…)model.bin.index.json'), FloatProgress(value=0.0, max=26788.0), HTML(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43932c4da0844906b50cccfc62095583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading shards'), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72757b7281ff4ba195e2b4b3ad038607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading (…)l-00001-of-00002.bin'), FloatProgress(value=0.0, max=9953548122.0), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10886008193c4116b5c32b6e0922a36b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading (…)l-00002-of-00002.bin'), FloatProgress(value=0.0, max=3837974747.0), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f92245f35755450da01d2e052cc7f405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Loading checkpoint shards'), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf32e1e5285441281a316b0ea582bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading (…)neration_config.json'), FloatProgress(value=0.0, max=140.0), HTML(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = 'legendhasit/xgen-7b-8k-inst-8bit'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General settings for zero-shot sampling\n",
    "header = (\n",
    "    \"A chat between a curious human and an artificial intelligence assistant. \"\n",
    "    \"The assistant gives helpful, detailed, and polite answers to the human's questions.\\n\"\n",
    ")\n",
    "\n",
    "article = \"\"\"\n",
    "E2: Dis voir Christiane, tu vas descendre bientôt chez ta soeur à Istres?\n",
    "C: (bruit de papier) Ah ben euh, on descend pour le mariage de Myriam et, et Emmanuel.\n",
    "C: Oui, (XX) sont en plein préparatifs, bien entendu.\n",
    "C: La petite Arlésienne, elle est mignonne comme tout, enfin bon.\n",
    "E2: Et alors elle aura la robe de l'Arlésienne non?\n",
    "C: Oh non, je pense pas qu'elle ait un costume régional, je pense.\n",
    "C: Je pense qu'elle aura une tenue euh <E2: De mariage traditionnel.> classique de mariée. Je pense.\n",
    "C: Remarque, on sait jamais hein, ça peut être la surprise, j'aimerais bien parce que c'est,\n",
    "C: c'est vrai que c'est joli, le costume d'Arlésienne. <Y: C'est très folklorique, oui, oui.>\n",
    "C: Je sais pas si elle, <Y: C'est très, très joli.> si elle en a un, (bruit de papier) j'ai jamais eu l'occasion de, <Y: Mais c'est fort possible que pour le mariage elle ait euh.> de voir.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\n",
      "\n",
      "### Human: Résume le texte suivant en un paragraphe monobloc.\n",
      "\n",
      "E2: Dis voir Christiane, tu vas descendre bientôt chez ta soeur à Istres?\n",
      "C: (bruit de papier) Ah ben euh, on descend pour le mariage de Myriam et, et Emmanuel.\n",
      "C: Oui, (XX) sont en plein préparatifs, bien entendu.\n",
      "C: La petite Arlésienne, elle est mignonne comme tout, enfin bon.\n",
      "E2: Et alors elle aura la robe de l'Arlésienne non?\n",
      "C: Oh non, je pense pas qu'elle ait un costume régional, je pense.\n",
      "C: Je pense qu'elle aura une tenue euh <E2: De mariage traditionnel.> classique de mariée. Je pense.\n",
      "C: Remarque, on sait jamais hein, ça peut être la surprise, j'aimerais bien parce que c'est,\n",
      "C: c'est vrai que c'est joli, le costume d'Arlésienne. <Y: C'est très folklorique, oui, oui.>\n",
      "C: Je sais pas si elle, <Y: C'est très, très joli.> si elle en a un, (bruit de papier) j'ai jamais eu l'occasion de, <Y: Mais c'est fort possible que pour le mariage elle ait euh.> de voir.\n",
      "\n",
      "###\n",
      "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\n",
      "\n",
      "### Human: Please summarize the following text in a single one-piece paragraph.\n",
      "\n",
      "E2: Dis voir Christiane, tu vas descendre bientôt chez ta soeur à Istres?\n",
      "C: (bruit de papier) Ah ben euh, on descend pour le mariage de Myriam et, et Emmanuel.\n",
      "C: Oui, (XX) sont en plein préparatifs, bien entendu.\n",
      "C: La petite Arlésienne, elle est mignonne comme tout, enfin bon.\n",
      "E2: Et alors elle aura la robe de l'Arlésienne non?\n",
      "C: Oh non, je pense pas qu'elle ait un costume régional, je pense.\n",
      "C: Je pense qu'elle aura une tenue euh <E2: De mariage traditionnel.> classique de mariée. Je pense.\n",
      "C: Remarque, on sait jamais hein, ça peut être la surprise, j'aimerais bien parce que c'est,\n",
      "C: c'est vrai que c'est joli, le costume d'Arlésienne. <Y: C'est très folklorique, oui, oui.>\n",
      "C: Je sais pas si elle, <Y: C'est très, très joli.> si elle en a un, (bruit de papier) j'ai jamais eu l'occasion de, <Y: Mais c'est fort possible que pour le mariage elle ait euh.> de voir.\n",
      "\n",
      "###\n",
      "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\n",
      "\n",
      "### Human: Tu es maintenant un secrétaire chargé de prendre des notes pendant des réunions. Je vais te donner des transcriptions de réunions. Je veux que tu résumes ces réunions en incluant les points, détails et nombres essentiels.\n",
      "\n",
      "E2: Dis voir Christiane, tu vas descendre bientôt chez ta soeur à Istres?\n",
      "C: (bruit de papier) Ah ben euh, on descend pour le mariage de Myriam et, et Emmanuel.\n",
      "C: Oui, (XX) sont en plein préparatifs, bien entendu.\n",
      "C: La petite Arlésienne, elle est mignonne comme tout, enfin bon.\n",
      "E2: Et alors elle aura la robe de l'Arlésienne non?\n",
      "C: Oh non, je pense pas qu'elle ait un costume régional, je pense.\n",
      "C: Je pense qu'elle aura une tenue euh <E2: De mariage traditionnel.> classique de mariée. Je pense.\n",
      "C: Remarque, on sait jamais hein, ça peut être la surprise, j'aimerais bien parce que c'est,\n",
      "C: c'est vrai que c'est joli, le costume d'Arlésienne. <Y: C'est très folklorique, oui, oui.>\n",
      "C: Je sais pas si elle, <Y: C'est très, très joli.> si elle en a un, (bruit de papier) j'ai jamais eu l'occasion de, <Y: Mais c'est fort possible que pour le mariage elle ait euh.> de voir.\n",
      "\n",
      "###\n",
      "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\n",
      "\n",
      "### Human: You are now a secretary in charge of taking notes at a meeting. I will give you meeting transcripts. I want you to summarize these meetings, including the main points, details and numbers mentioned.\n",
      "\n",
      "E2: Dis voir Christiane, tu vas descendre bientôt chez ta soeur à Istres?\n",
      "C: (bruit de papier) Ah ben euh, on descend pour le mariage de Myriam et, et Emmanuel.\n",
      "C: Oui, (XX) sont en plein préparatifs, bien entendu.\n",
      "C: La petite Arlésienne, elle est mignonne comme tout, enfin bon.\n",
      "E2: Et alors elle aura la robe de l'Arlésienne non?\n",
      "C: Oh non, je pense pas qu'elle ait un costume régional, je pense.\n",
      "C: Je pense qu'elle aura une tenue euh <E2: De mariage traditionnel.> classique de mariée. Je pense.\n",
      "C: Remarque, on sait jamais hein, ça peut être la surprise, j'aimerais bien parce que c'est,\n",
      "C: c'est vrai que c'est joli, le costume d'Arlésienne. <Y: C'est très folklorique, oui, oui.>\n",
      "C: Je sais pas si elle, <Y: C'est très, très joli.> si elle en a un, (bruit de papier) j'ai jamais eu l'occasion de, <Y: Mais c'est fort possible que pour le mariage elle ait euh.> de voir.\n",
      "\n",
      "###\n",
      "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\n",
      "\n",
      "### Human: Tu es maintenant AISA, un assistant intelligence artificielle chargé de résumer des textes. Tes résumés seront en 6 paragraphes. S'il-te-plait AISA, résume le texte suivant.\n",
      "\n",
      "E2: Dis voir Christiane, tu vas descendre bientôt chez ta soeur à Istres?\n",
      "C: (bruit de papier) Ah ben euh, on descend pour le mariage de Myriam et, et Emmanuel.\n",
      "C: Oui, (XX) sont en plein préparatifs, bien entendu.\n",
      "C: La petite Arlésienne, elle est mignonne comme tout, enfin bon.\n",
      "E2: Et alors elle aura la robe de l'Arlésienne non?\n",
      "C: Oh non, je pense pas qu'elle ait un costume régional, je pense.\n",
      "C: Je pense qu'elle aura une tenue euh <E2: De mariage traditionnel.> classique de mariée. Je pense.\n",
      "C: Remarque, on sait jamais hein, ça peut être la surprise, j'aimerais bien parce que c'est,\n",
      "C: c'est vrai que c'est joli, le costume d'Arlésienne. <Y: C'est très folklorique, oui, oui.>\n",
      "C: Je sais pas si elle, <Y: C'est très, très joli.> si elle en a un, (bruit de papier) j'ai jamais eu l'occasion de, <Y: Mais c'est fort possible que pour le mariage elle ait euh.> de voir.\n",
      "\n",
      "###\n",
      "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\n",
      "\n",
      "### Human: You are now AISA, an Artificial Intelligence Summarization Assistant. You will provide 6 paragraphs in your summaries. Please AISA, summarize the following text.\n",
      "\n",
      "E2: Dis voir Christiane, tu vas descendre bientôt chez ta soeur à Istres?\n",
      "C: (bruit de papier) Ah ben euh, on descend pour le mariage de Myriam et, et Emmanuel.\n",
      "C: Oui, (XX) sont en plein préparatifs, bien entendu.\n",
      "C: La petite Arlésienne, elle est mignonne comme tout, enfin bon.\n",
      "E2: Et alors elle aura la robe de l'Arlésienne non?\n",
      "C: Oh non, je pense pas qu'elle ait un costume régional, je pense.\n",
      "C: Je pense qu'elle aura une tenue euh <E2: De mariage traditionnel.> classique de mariée. Je pense.\n",
      "C: Remarque, on sait jamais hein, ça peut être la surprise, j'aimerais bien parce que c'est,\n",
      "C: c'est vrai que c'est joli, le costume d'Arlésienne. <Y: C'est très folklorique, oui, oui.>\n",
      "C: Je sais pas si elle, <Y: C'est très, très joli.> si elle en a un, (bruit de papier) j'ai jamais eu l'occasion de, <Y: Mais c'est fort possible que pour le mariage elle ait euh.> de voir.\n",
      "\n",
      "###\n",
      "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\n",
      "\n",
      "### Human: Résume le texte suivant en proposant une introduction qui le contextualise et une liste de points clés et les détails à l'appui.\n",
      "\n",
      "E2: Dis voir Christiane, tu vas descendre bientôt chez ta soeur à Istres?\n",
      "C: (bruit de papier) Ah ben euh, on descend pour le mariage de Myriam et, et Emmanuel.\n",
      "C: Oui, (XX) sont en plein préparatifs, bien entendu.\n",
      "C: La petite Arlésienne, elle est mignonne comme tout, enfin bon.\n",
      "E2: Et alors elle aura la robe de l'Arlésienne non?\n",
      "C: Oh non, je pense pas qu'elle ait un costume régional, je pense.\n",
      "C: Je pense qu'elle aura une tenue euh <E2: De mariage traditionnel.> classique de mariée. Je pense.\n",
      "C: Remarque, on sait jamais hein, ça peut être la surprise, j'aimerais bien parce que c'est,\n",
      "C: c'est vrai que c'est joli, le costume d'Arlésienne. <Y: C'est très folklorique, oui, oui.>\n",
      "C: Je sais pas si elle, <Y: C'est très, très joli.> si elle en a un, (bruit de papier) j'ai jamais eu l'occasion de, <Y: Mais c'est fort possible que pour le mariage elle ait euh.> de voir.\n",
      "\n",
      "###\n",
      "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\n",
      "\n",
      "### Human: Summarize the following text and include an introduction that contextualizes it, as well as a list of keypoints and details.\n",
      "\n",
      "E2: Dis voir Christiane, tu vas descendre bientôt chez ta soeur à Istres?\n",
      "C: (bruit de papier) Ah ben euh, on descend pour le mariage de Myriam et, et Emmanuel.\n",
      "C: Oui, (XX) sont en plein préparatifs, bien entendu.\n",
      "C: La petite Arlésienne, elle est mignonne comme tout, enfin bon.\n",
      "E2: Et alors elle aura la robe de l'Arlésienne non?\n",
      "C: Oh non, je pense pas qu'elle ait un costume régional, je pense.\n",
      "C: Je pense qu'elle aura une tenue euh <E2: De mariage traditionnel.> classique de mariée. Je pense.\n",
      "C: Remarque, on sait jamais hein, ça peut être la surprise, j'aimerais bien parce que c'est,\n",
      "C: c'est vrai que c'est joli, le costume d'Arlésienne. <Y: C'est très folklorique, oui, oui.>\n",
      "C: Je sais pas si elle, <Y: C'est très, très joli.> si elle en a un, (bruit de papier) j'ai jamais eu l'occasion de, <Y: Mais c'est fort possible que pour le mariage elle ait euh.> de voir.\n",
      "\n",
      "###\n",
      "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\n",
      "\n",
      "### Human: Reprends la liste des détails évoqués pour former un texte construit qui résume le débat.\n",
      "\n",
      "E2: Dis voir Christiane, tu vas descendre bientôt chez ta soeur à Istres?\n",
      "C: (bruit de papier) Ah ben euh, on descend pour le mariage de Myriam et, et Emmanuel.\n",
      "C: Oui, (XX) sont en plein préparatifs, bien entendu.\n",
      "C: La petite Arlésienne, elle est mignonne comme tout, enfin bon.\n",
      "E2: Et alors elle aura la robe de l'Arlésienne non?\n",
      "C: Oh non, je pense pas qu'elle ait un costume régional, je pense.\n",
      "C: Je pense qu'elle aura une tenue euh <E2: De mariage traditionnel.> classique de mariée. Je pense.\n",
      "C: Remarque, on sait jamais hein, ça peut être la surprise, j'aimerais bien parce que c'est,\n",
      "C: c'est vrai que c'est joli, le costume d'Arlésienne. <Y: C'est très folklorique, oui, oui.>\n",
      "C: Je sais pas si elle, <Y: C'est très, très joli.> si elle en a un, (bruit de papier) j'ai jamais eu l'occasion de, <Y: Mais c'est fort possible que pour le mariage elle ait euh.> de voir.\n",
      "\n",
      "###\n",
      "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\n",
      "\n",
      "### Human: Find the details evoked and form a structured text to sum up this debate.\n",
      "\n",
      "E2: Dis voir Christiane, tu vas descendre bientôt chez ta soeur à Istres?\n",
      "C: (bruit de papier) Ah ben euh, on descend pour le mariage de Myriam et, et Emmanuel.\n",
      "C: Oui, (XX) sont en plein préparatifs, bien entendu.\n",
      "C: La petite Arlésienne, elle est mignonne comme tout, enfin bon.\n",
      "E2: Et alors elle aura la robe de l'Arlésienne non?\n",
      "C: Oh non, je pense pas qu'elle ait un costume régional, je pense.\n",
      "C: Je pense qu'elle aura une tenue euh <E2: De mariage traditionnel.> classique de mariée. Je pense.\n",
      "C: Remarque, on sait jamais hein, ça peut être la surprise, j'aimerais bien parce que c'est,\n",
      "C: c'est vrai que c'est joli, le costume d'Arlésienne. <Y: C'est très folklorique, oui, oui.>\n",
      "C: Je sais pas si elle, <Y: C'est très, très joli.> si elle en a un, (bruit de papier) j'ai jamais eu l'occasion de, <Y: Mais c'est fort possible que pour le mariage elle ait euh.> de voir.\n",
      "\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('prompts.txt', 'r', encoding='utf-8') as input_file:\n",
    "    lines = input_file.readlines()\n",
    "    counter = 0\n",
    "    for line in lines:\n",
    "\n",
    "        # Read instruction and create prompt\n",
    "        instruction = line.replace('\\n', '')\n",
    "        prompt = header + f\"\\n### Human: {instruction}\\n{article}\\n###\"\n",
    "\n",
    "        # Generate answer\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "        sample = model.generate(**input_ids, do_sample=True, max_new_tokens=2048, top_k=100, eos_token_id=50256)\n",
    "        output = tokenizer.decode(sample[0]).strip()\n",
    "        \n",
    "        # Save answer in file\n",
    "        os.mkdir('output')\n",
    "        with open('output/' + counter + '.txt', encoding='utf-8') as target_file:\n",
    "            target_file.write(output)\n",
    "            target_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m input_ids\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m sample\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m output\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample' is not defined"
     ]
    }
   ],
   "source": [
    "del input_ids\n",
    "del sample\n",
    "del output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch 2.0.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
