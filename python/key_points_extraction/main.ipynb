{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install einops\n",
    "# !GITHUB_ACTIONS=true pip install auto-gptq\n",
    "# !pip install tokenizers --upgrade\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connexion HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !huggingface-cli login Pas oublier (lancer un terminal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import time\n",
    "import datetime\n",
    "import evaluate\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoConfig\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Working conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "name_dataset = \"LanguageTestDataSet\" # The dataset must be in a certain format\n",
    "\n",
    "# Models used\n",
    "used_models_name = {\n",
    "    \"Salesforce/xgen-7b-8k-inst\",\n",
    "    \"legendhasit/xgen-7b-8k-inst-8bit\",\n",
    "    \"mosaicml/mpt-7b-instruct\", # 1 GPU (2 shards de 10 Go)- To quantize\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\"} # Models taken for computation \n",
    "\n",
    "# Input limits\n",
    "token_limit = 3500 # To be determined with the context length of the used model\n",
    "\n",
    "# MAX token length for chunkization\n",
    "MAX_TOKEN_CHUNK_SIZE = 750\n",
    "\n",
    "# Description of the inference wanted\n",
    "desc = \"This inference batch is aimed at testing MPT7B on summarization. There is no French summarization at all.\"\n",
    "\n",
    "# Caracteristics of the inference wanted\n",
    "max_new_tokens=700\n",
    "top_k=20\n",
    "temperature=0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompting elements\n",
    "instruction_templates = {\"text_en\":\"Provide a list of key points for the following text:\\n\\n{text}\", \"text_fr\":\"Résume le texte suivant:\\n\\n{text}\"} # Prompt à améliorer peut-être avec une liste de independent key points\n",
    "text_keys = {'text_en'} # Cet ensemble donne les clés pour le texte français et anglais. On retire 'text_fr' pour les modèles anglais\n",
    "key_to_language = {'text_fr':'0', 'text_en':'1'}\n",
    "language_to_key = {'0':'text_fr', '1':'text_en'}\n",
    "number_to_code = {'0':'text_fr', '1':'text_en', '2':'text_en'}\n",
    "\n",
    "# Miscellaneous useful functions and constants\n",
    "def count_words(s):\n",
    "    words = re.findall(r'\\b\\w+\\b', s)  # Find all word-like sequences using regular expression\n",
    "    return len(words)  # Return the number of words\n",
    "\n",
    "def mkdir(folder_path):\n",
    "    try:\n",
    "        os.mkdir(folder_path)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "def classify(text): # Classificateur déterministe basique, qui accumule des indices de langue et renvoie le langage avec le plsu haut score\n",
    "    score_en = 0\n",
    "    score_fr = 0\n",
    "    score_en += len(find_all_occurrences_regex(text, \" and \" ))\n",
    "    score_en += len(find_all_occurrences_regex(text, \" of \" ))\n",
    "    score_en += len(find_all_occurrences_regex(text, \" the \" ))\n",
    "    score_en += len(find_all_occurrences_regex(text, \" in \" ))\n",
    "    score_en += len(find_all_occurrences_regex(text, \" is \" ))\n",
    "    score_en += len(find_all_occurrences_regex(text, \" for \" ))\n",
    "    score_en += len(find_all_occurrences_regex(text, \" how \" ))\n",
    "    score_en += len(find_all_occurrences_regex(text, \" with \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" le \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" la \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" de \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" un \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" une \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" et \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" à \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" avec \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" il \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" pour \" ))\n",
    "    if score_fr + score_en <= 3: return 2\n",
    "    if score_fr > score_en: return 0\n",
    "    return 1\n",
    "\n",
    "def find_all_occurrences_regex(text, pattern):\n",
    "    occurrences = [match.start() for match in re.finditer(pattern, text)]\n",
    "    return occurrences\n",
    "\n",
    "def maxRouge(summaries_data, language_code, generated_summary, golds): # Pour un dictionnaire de résumés de référence, pour un langage donné, renvoie le résumé de référence le plus proche du résumé prédit au sens de rouge et les scores associés\n",
    "    max_rouge2 = -1\n",
    "    max_rougel = -1\n",
    "    nb_words_closest_gold = 0\n",
    "\n",
    "    for i in range(len(summaries_data)):\n",
    "        summary_data = summaries_data[i]\n",
    "        summary_text = summary_data[language_code]\n",
    "        result_rouge = rouge.compute(predictions=[generated_summary], references=[summary_text], use_aggregator=False)\n",
    "        golds.append(summary_text)\n",
    "        if result_rouge['rouge2'][0] > max_rouge2:\n",
    "            max_rouge2 = result_rouge['rouge2'][0]\n",
    "            nb_words_closest_gold = summary_data['nb_words_' + language_code[-2:]]\n",
    "        if result_rouge['rougeL'][0] > max_rougel:\n",
    "            max_rougel = result_rouge['rougeL'][0]\n",
    "\n",
    "    return max_rouge2, max_rougel, nb_words_closest_gold\n",
    "\n",
    "def load_json_into_dict(path):\n",
    "    file = open(path, 'r', encoding='utf-8')\n",
    "    dict = json.load(file)\n",
    "    file.close()\n",
    "    return dict\n",
    "\n",
    "def save_dict_into_json(dict, path):\n",
    "    file = open(path, 'r', encoding='utf-8')\n",
    "    json.dump(dict, file, indent=4, ensure_ascii=False)\n",
    "    file.close()\n",
    "\n",
    "def token_len(text, tokenizer):\n",
    "    return len(tokenizer(text, return_tensors=\"pt\")['input_ids'][0])\n",
    "\n",
    "def append_to_chunk(current_chunk, utterance):\n",
    "    if len(current_chunk) > 0:\n",
    "        current_chunk += '\\n'\n",
    "    current_chunk += utterance\n",
    "    return current_chunk\n",
    "\n",
    "def append_to_split(current_split, sentence): # Les ? . ! ... sont remplacés par des \". \"\n",
    "    if len(current_split) > 0:\n",
    "        current_split += '. '\n",
    "    current_split += sentence\n",
    "    return current_split\n",
    "\n",
    "def chunkize(text, tokenizer):\n",
    "    \"\"\"\n",
    "    Greedy implementation of a dialogue transcript chunking algorithm. This method returns a list of transcript chunks.\n",
    "    - It priorities stability over performance. There is a set maximum chunk size for LLM inference stability. Really long utterances bypass this limit.\n",
    "    - It guarantees the cuts are made at utterance ends (\\n).\n",
    "    - It counts everything in MODEL TOKENS and not characters for more exact experiments.\n",
    "    \"\"\"\n",
    "    \n",
    "    chunks = [] # Final list of transcript chunks. This makes up the loop invariant\n",
    "    utterances = text.split('\\n') # Transcript is split into sentences\n",
    "    utterances.reverse() # Reverse everything!!\n",
    "    current_chunk = ''\n",
    "\n",
    "    # While there is still an utterance to process\n",
    "    while len(utterances) > 0:\n",
    "        utterance = utterances.pop()\n",
    "        new_current_chunk = append_to_chunk(current_chunk, utterance)\n",
    "        if token_len(utterance, tokenizer) > MAX_TOKEN_CHUNK_SIZE:\n",
    "            if current_chunk != '': chunks.append(current_chunk)\n",
    "            splits = split_utterance(utterance, tokenizer) # Découpe une utterance en s'assurant que chaque coupe soit plus petite que la taille max\n",
    "            for split in splits[:-1]: chunks.append(split) # Except the last split, that will be used as the next current_chunk\n",
    "            current_chunk = splits[-1]\n",
    "\n",
    "        # Add to current chunk and proceed to next\n",
    "        elif token_len(new_current_chunk, tokenizer) <= MAX_TOKEN_CHUNK_SIZE:\n",
    "            current_chunk = new_current_chunk\n",
    "        \n",
    "        # Current chunk is big enough, append to list and create new one\n",
    "        else:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = utterance\n",
    "\n",
    "    if len(current_chunk) > 0:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def split_utterance(text, tokenizer):  \n",
    "    splits = [] # Final list of transcript chunks. This makes up the loop invariant\n",
    "    sentence_pattern = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
    "    sentences = re.split(sentence_pattern, text) # Transcript is split into sentences\n",
    "    sentences.reverse() # Reverse everything!!\n",
    "    current_split = ''\n",
    "    # While there is still an utterance to process\n",
    "    while len(sentences) > 0:\n",
    "        sentence = sentences.pop()\n",
    "        new_current_split = append_to_split(current_split, sentence)\n",
    "        if token_len(new_current_split, tokenizer) > MAX_TOKEN_CHUNK_SIZE:\n",
    "            if current_split =='': print(\" /!\\ Split issue : too long sentence.\")\n",
    "            splits.append(current_split)\n",
    "            current_split = sentence\n",
    "\n",
    "        # Add to current chunk and proceed to next\n",
    "        else:\n",
    "            current_split = new_current_split\n",
    "    \n",
    "    if len(current_split) > 0:\n",
    "        splits.append(current_split)\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_preparation_code = \"\"\"dataset_folder_url = \"datasets/KeyPointsExtractionTest/\"\n",
    "dataset_dict = {\"fredsum\":\"0\", \"ami\":\"6\", \"mediasum\":\"3\", \"summre\":5, \"icsi\":7}\n",
    "\n",
    "virgin_files = os.listdir(dataset_folder_url)\n",
    "data = {}\n",
    "for file_name in virgin_files:\n",
    "    if os.path.isdir(file_name): # Shouldn't happen normally\n",
    "        continue\n",
    "    file_code = file_name.split('.')[0].split(\"_\")\n",
    "    data_key = file_code[0] + \"_\" + file_code[1]\n",
    "    dataset = file_code[0]\n",
    "    if data_key not in data:\n",
    "        data[data_key] = [dataset, {}]\n",
    "\n",
    "    if file_code[2][0:3] == \"txt\":\n",
    "        dataset = file_code[0]\n",
    "        original_file_name = file_code[1]\n",
    "        data[data_key][1][\"file_name\"] = original_file_name\n",
    "        data[data_key][1][\"source_dataset\"] = dataset_dict[dataset]\n",
    "\n",
    "        file = open(dataset_folder_url + \"/\" + file_name, 'r', encoding='utf-8')\n",
    "        text = file.read()\n",
    "        file.close()\n",
    "        language_number = str(classify(text))\n",
    "        data[data_key][1][\"language\"] = language_number\n",
    "        language_code = number_to_code[language_number]\n",
    "        data[data_key][1][language_code] = text\n",
    "\n",
    "    elif file_code[2][0:3] == \"sum\":\n",
    "        if \"summaries\" not in data[data_key][1]:\n",
    "            data[data_key][1][\"summaries\"] = []\n",
    "\n",
    "        if len(file_code)==4: # Plusieurs résumés\n",
    "            summary_data = {}\n",
    "            summary_data[\"number\"] = file_code[3]\n",
    "            file = open(dataset_folder_url + \"/\" + file_name, 'r', encoding='utf-8')\n",
    "            text = file.read()\n",
    "            file.close()\n",
    "            summary_data[number_to_code[str(classify(text))]] = text\n",
    "\n",
    "            data[data_key][1][\"summaries\"].append(summary_data)\n",
    "\n",
    "        else: # Un seul résumé\n",
    "            summary_data = {}\n",
    "            summary_data[\"number\"] = \"1\"\n",
    "            file = open(dataset_folder_url + \"/\" + file_name, 'r', encoding='utf-8')\n",
    "            text = file.read()\n",
    "            file.close()\n",
    "            summary_data[number_to_code[str(classify(text))]] = text\n",
    "\n",
    "            data[data_key][1][\"summaries\"].append(summary_data)\n",
    "            \n",
    "\n",
    "\n",
    "for key in data:\n",
    "    dataset = data[key][0]\n",
    "    file_data = data[key][1]\n",
    "    mkdir(dataset_folder_url + dataset)\n",
    "    file_name = str(len(os.listdir(dataset_folder_url + dataset)))\n",
    "    file = open(dataset_folder_url + dataset + \"/\" + file_name, 'w', encoding='utf-8')\n",
    "    json.dump(file_data, file, indent=4, ensure_ascii=False)\n",
    "    file.close()\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These dictionaries have as a key the model name and as the value the function that load the prompt, the tokenizer, the model, give the context length or treat the output.\n",
    "\n",
    "prompt_templates = {} # Dictionnaire qui associe les templates de prompts, il faut insérer l'instruction et le texte à résumer.\n",
    "context_lengths = {} # Dictionnaire qui donne l'entier correspondant à la longueur de contexte\n",
    "tokenizers = {} # Dictionnaire qui associe la méthode pour obtenir le tokenizer du modèle\n",
    "models = {} # Dictionnaire qui associe la méthode pour obtenir le modèle\n",
    "treat_output = {} # Dictionnaire qui associe la méthode pour traiter l'output et ne conserver que la génération du modèle\n",
    "infer = {} # Dictionnaire qui associe la méthode pour l'inférence du modèle. Prend en paramètres l'input et le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model specific functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models available\n",
    "models_name = {\n",
    "    \"legendhasit/xgen-7b-8k-inst-8bit/4bit\", # 1 GPU \n",
    "    \"legendhasit/xgen-7b-8k-inst-8bit\", # 1 GPU\n",
    "    \"Salesforce/xgen-7b-8k-inst\", # 2GPU (3 shards de 10Go peut-être la dernière moins) de préférence, mais fonctionne avec 1... Sauf pour les textes longs, genre les plus petits de Fredsum + probablement mauvaise suppression automatique de la mémoire -> On a des OOM à 38GB\n",
    "    \"mosaicml/mpt-7b-instruct\", # 1 GPU (2 shards de 10 Go)- To quantize\n",
    "    # \"Trelis/mpt-7b-instruct-hosted-inference-8bit\", #  - Potentially unreliable\n",
    "    \"mosaicml/mpt-7b-8k-instruct\", # To quantize\n",
    "    \"mosaicml/mpt-30b-instruct\", # To quantize\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"tiiuae/falcon-7b\"\n",
    "}\n",
    "\n",
    "simplified_names = {\n",
    "    \"legendhasit/xgen-7b-8k-inst-8bit/4bit\":\"XGen7b8k4bit\", # 1 GPU \n",
    "    \"legendhasit/xgen-7b-8k-inst-8bit\":\"XGen7b8k8bit\", # 1 GPU\n",
    "    \"Salesforce/xgen-7b-8k-inst\":\"XGen7b8k\", # 2GPU (3 shards de 10Go peut-être la dernière moins) de préférence, mais fonctionne avec 1... Sauf pour les textes longs, genre les plus petits de Fredsum + probablement mauvaise suppression automatique de la mémoire -> On a des OOM à 38GB\n",
    "    \"mosaicml/mpt-7b-instruct\":\"MPT7b\", # 1 GPU (2 shards de 10 Go)- To quantize\n",
    "    # \"Trelis/mpt-7b-instruct-hosted-inference-8bit\", #  - Potentially unreliable\n",
    "    \"mosaicml/mpt-7b-8k-instruct\":\"MPT7b8k\", # To quantize\n",
    "    \"mosaicml/mpt-30b-instruct\":\"MPT30b\", # To quantize\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\":\"llama27b\",\n",
    "    \"tiiuae/falcon-7b\":\"falcon7b\"\n",
    "}\n",
    "\n",
    "reversed_names = {simplified_names[name]:name for name in simplified_names}\n",
    "# RMQ: Mieux que les dicos de fonction, comme on utilise toujours les mêmes fonctions, mais avec des paramètres différents, il suffit d'avoir le dico des paramètres de la fonction, avec un dico par défaut modifié pour chaque modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt templates\n",
    "prompt_template_XGen = \"A chat between a curious human and an artificial intelligence assistant.\\nThe assistant gives helpful, detailed, and polite answers to the human's questions.\\n\\n### Human: {instruction}\\n\\n### Assistant: \"\n",
    "\n",
    "prompt_templates[\"legendhasit/xgen-7b-8k-inst-8bit\"] = prompt_template_XGen\n",
    "prompt_templates[\"legendhasit/xgen-7b-8k-inst-8bit/4bit\"] = prompt_template_XGen\n",
    "prompt_templates[\"Salesforce/xgen-7b-8k-inst\"] = prompt_template_XGen\n",
    "\n",
    "# Context lengths # Allow to know whether the model generated out of his context window\n",
    "context_length_XGen = 8000 \n",
    "\n",
    "context_lengths[\"legendhasit/xgen-7b-8k-inst-8bit\"] = context_length_XGen\n",
    "context_lengths[\"legendhasit/xgen-7b-8k-inst-8bit/4bit\"] = context_length_XGen\n",
    "context_lengths[\"Salesforce/xgen-7b-8k-inst\"] = context_length_XGen\n",
    "\n",
    "# Tokenizers\n",
    "def get_tokenizer_XGen8bit(): # Le même pour le 4bit\n",
    "    return AutoTokenizer.from_pretrained('legendhasit/xgen-7b-8k-inst-8bit', trust_remote_code=True)\n",
    "\n",
    "def get_tokenizer_XGen():\n",
    "    return AutoTokenizer.from_pretrained('Salesforce/xgen-7b-8k-inst', trust_remote_code=True)\n",
    "\n",
    "tokenizers[\"legendhasit/xgen-7b-8k-inst-8bit\"] = get_tokenizer_XGen8bit\n",
    "tokenizers[\"legendhasit/xgen-7b-8k-inst-8bit/4bit\"] = get_tokenizer_XGen8bit\n",
    "tokenizers[\"Salesforce/xgen-7b-8k-inst\"] = get_tokenizer_XGen\n",
    "\n",
    "# Models\n",
    "\n",
    "def get_model_XGen8bit():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    'legendhasit/xgen-7b-8k-inst-8bit',\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_model_XGen4bit():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    'legendhasit/xgen-7b-8k-inst-8bit',\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_model_XGen():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    'Salesforce/xgen-7b-8k-inst',\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "    return model\n",
    "\n",
    "models[\"legendhasit/xgen-7b-8k-inst-8bit\"] = get_model_XGen8bit\n",
    "models[\"legendhasit/xgen-7b-8k-inst-8bit/4bit\"] = get_model_XGen4bit\n",
    "models[\"Salesforce/xgen-7b-8k-inst\"] = get_model_XGen\n",
    "\n",
    "# Tokenize (if specificities in the way models call the tokenizer)\n",
    "\n",
    "# Inference (if specificities in the way models call the generate function)\n",
    "\n",
    "def infer_XGen(tokenized_input, model):\n",
    "    return model.generate(**tokenized_input, do_sample=True, max_new_tokens=max_new_tokens, top_k=top_k, eos_token_id=50256, temperature=temperature) # eos_token_id : id du token de fin de réponse d'XGen dans son vocabulaire.\n",
    "\n",
    "infer['legendhasit/xgen-7b-8k-inst-8bit'] = infer_XGen\n",
    "infer['legendhasit/xgen-7b-8k-inst-8bit/4bit'] = infer_XGen\n",
    "infer['Salesforce/xgen-7b-8k-inst'] = infer_XGen\n",
    "\n",
    "# Treating the output (to remove the input if present in the output as well as the end of text token for example)\n",
    "\n",
    "def treat_output_XGen(output): # Differs if it is N-shot. Here, it is 0-shot\n",
    "    occ_1 = output.find(\"### Assistant: \")\n",
    "    output = output[occ_1+15:]\n",
    "    if output.find('<|endoftext|>')!=-1:\n",
    "        output = output[:-14]\n",
    "    return output\n",
    "\n",
    "treat_output['legendhasit/xgen-7b-8k-inst-8bit'] = treat_output_XGen\n",
    "treat_output['legendhasit/xgen-7b-8k-inst-8bit/4bit'] = treat_output_XGen\n",
    "treat_output['Salesforce/xgen-7b-8k-inst'] = treat_output_XGen\n",
    "\n",
    "# /workspace/.miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1411: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
    "# Avec XGen de salesforce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt templates\n",
    "prompt_template_MPT7B = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{instruction}\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template_MPT30B = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n###Instruction\\n{instruction}\\n\\n### Response\\n\" # On considère que c'est le même template que pour MPT 7B 8k car pour ce dernier, le template utilisé n'est pas précisé\n",
    "\n",
    "prompt_templates[\"mosaicml/mpt-7b-instruct\"] = prompt_template_MPT7B\n",
    "prompt_templates[\"Trelis/mpt-7b-instruct-hosted-inference-8bit\"] = prompt_template_MPT7B\n",
    "prompt_templates[\"mosaicml/mpt-7b-8k-instruct\"] = prompt_template_MPT30B\n",
    "prompt_templates[\"mosaicml/mpt-30b-instruct\"] = prompt_template_MPT30B\n",
    "\n",
    "# Context lengths\n",
    "\n",
    "context_lengths[\"mosaicml/mpt-7b-instruct\"] = 2048 # 4096 d'après mosaicml\n",
    "\n",
    "# Models\n",
    "def get_model_MPT7B():\n",
    "    config = AutoConfig.from_pretrained(\"mosaicml/mpt-7b-instruct\", trust_remote_code=True)\n",
    "    config.max_seq_len = 4096 # (input + output) tokens can now be up to 4096\n",
    "    return AutoModelForCausalLM.from_pretrained(\"mosaicml/mpt-7b-instruct\",config=config,trust_remote_code=True,device_map = \"auto\")\n",
    "\n",
    "models[\"mosaicml/mpt-7b-instruct\"] = get_model_MPT7B\n",
    "\n",
    "# Tokenize\n",
    "def get_tokenizer_MPT7B():\n",
    "    return AutoTokenizer.from_pretrained(\"mosaicml/mpt-7b-instruct\", trust_remote_code=True)\n",
    "\n",
    "tokenizers[\"mosaicml/mpt-7b-instruct\"] = get_tokenizer_MPT7B\n",
    "# Inference ######################################\n",
    "\n",
    "def infer_MPT7B(tokenized_input, model):\n",
    "    return model.generate(**tokenized_input, do_sample=True, max_new_tokens=max_new_tokens, top_k=top_k, temperature=temperature, eos_token_id=0) # eos_token_id : id du token de fin de réponse d'XGen dans son vocabulaire. repetition_penalty=1.2 : évitd la répéttioon\n",
    "\n",
    "infer[\"mosaicml/mpt-7b-instruct\"] = infer_MPT7B\n",
    "\n",
    "# Treating the output ######################################\n",
    "\n",
    "def treat_output_MPT(output): # Differs if it is N-shot. Here, it is 0-shot\n",
    "    occ_1 = output.find(\"### Response:\\n\")\n",
    "    output = output[occ_1+14:]\n",
    "    if output.find('<|endoftext|>')!=-1:\n",
    "        output = output[:-14]\n",
    "    return output\n",
    "\n",
    "treat_output[\"mosaicml/mpt-7b-instruct\"] = treat_output_MPT\n",
    "treat_output[\"Trelis/mpt-7b-instruct-hosted-inference-8bit\"] = treat_output_MPT\n",
    "treat_output[\"mosaicml/mpt-7b-8k-instruct\"] = treat_output_MPT\n",
    "treat_output[\"mosaicml/mpt-30b-instruct\"] = treat_output_MPT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Llama 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt templates\n",
    "prompt_templates[\"meta-llama/Llama-2-7b-chat-hf\"] = \"\"\"<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "\n",
    "{instruction} [/INST]\"\"\"\n",
    "\n",
    "context_lengths[\"meta-llama/Llama-2-7b-chat-hf\"] = 4096\n",
    "\n",
    "def get_tokenizer_llama2_7b():\n",
    "    return AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", trust_remote_code=True)\n",
    "\n",
    "tokenizers[\"meta-llama/Llama-2-7b-chat-hf\"] = get_tokenizer_llama2_7b\n",
    "\n",
    "def get_model_llama2_7b():\n",
    "    return AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",trust_remote_code=True,device_map = \"auto\")\n",
    "\n",
    "models[\"meta-llama/Llama-2-7b-chat-hf\"] = get_model_llama2_7b\n",
    "\n",
    "def infer_llama2_7b(tokenized_input, model):\n",
    "    return model.generate(**tokenized_input, do_sample=True, max_new_tokens=max_new_tokens, top_k=top_k, temperature=temperature, eos_token_id=2) # tokenizer.eos_token_id)\n",
    "\n",
    "infer[\"meta-llama/Llama-2-7b-chat-hf\"] = infer_llama2_7b\n",
    "\n",
    "def treat_output_llama2_7b(output): # Differs if it is N-shot. Here, it is 0-shot\n",
    "    occ_1 = output.find(\"[/INST]\")\n",
    "    output = output[occ_1+9:] # +7 + les 2 espaces avant que le modèle ne parle\n",
    "    if output.find('</s>')!=-1:\n",
    "        output = output[:-4]\n",
    "    return output\n",
    "\n",
    "treat_output[\"meta-llama/Llama-2-7b-chat-hf\"] = treat_output_llama2_7b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Falcon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_templates[\"tiiuae/falcon-7b\"] = \"\"\"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\n",
    ">>QUESTION<<{instruction}\n",
    ">>ANSWER<<\"\"\"\n",
    "\n",
    "context_lengths[\"tiiuae/falcon-7b\"] = 2048\n",
    "\n",
    "def get_tokenizer_falcon7b():\n",
    "    return AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\", trust_remote_code=True)\n",
    "\n",
    "tokenizers[\"tiiuae/falcon-7b\"] = get_tokenizer_falcon7b\n",
    "\n",
    "def get_model_falcon7b():\n",
    "    return AutoModelForCausalLM.from_pretrained(\n",
    "    \"tiiuae/falcon-7b\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "models[\"tiiuae/falcon-7b\"] = get_model_falcon7b\n",
    "\n",
    "def infer_falcon7b(tokenized_input, model):\n",
    "    return model.generate(**tokenized_input, do_sample=True, max_new_tokens=max_new_tokens, top_k=top_k, temperature=temperature, eos_token_id=11)\n",
    "\n",
    "infer[\"tiiuae/falcon-7b\"] = infer_falcon7b\n",
    "\n",
    "def treat_output_falcon7b(output):\n",
    "    occ_1 = output.find('>>ANSWER<<')\n",
    "    output = output[occ_1 + len('>>ANSWER<<'):]\n",
    "    if output.find('<|endoftext|>')!=-1:\n",
    "        output = output[:-len('<|endoftext|>')]\n",
    "    return output\n",
    "\n",
    "treat_output[\"tiiuae/falcon-7b\"] = treat_output_falcon7b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing the input dataset\n",
    "Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salesforce/xgen-7b-8k-inst --- ami --- 0\n",
      "Salesforce/xgen-7b-8k-inst --- ami --- 1\n",
      "Salesforce/xgen-7b-8k-inst --- ami --- 2\n",
      "Salesforce/xgen-7b-8k-inst --- fredsum --- 0\n",
      "Salesforce/xgen-7b-8k-inst --- fredsum --- 1\n",
      "Salesforce/xgen-7b-8k-inst --- fredsum --- 2\n",
      "Salesforce/xgen-7b-8k-inst --- icsi --- 0\n",
      "Salesforce/xgen-7b-8k-inst --- icsi --- 1\n",
      "Salesforce/xgen-7b-8k-inst --- icsi --- 2\n",
      "Salesforce/xgen-7b-8k-inst --- mediasum --- 0\n",
      "Salesforce/xgen-7b-8k-inst --- mediasum --- 1\n",
      "Salesforce/xgen-7b-8k-inst --- mediasum --- 2\n",
      "Salesforce/xgen-7b-8k-inst --- summre --- 0\n",
      "Salesforce/xgen-7b-8k-inst --- summre --- 1\n",
      "Salesforce/xgen-7b-8k-inst --- summre --- 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "legendhasit/xgen-7b-8k-inst-8bit/4bit --- ami --- 0\n",
      "legendhasit/xgen-7b-8k-inst-8bit/4bit --- ami --- 1\n",
      "legendhasit/xgen-7b-8k-inst-8bit/4bit --- ami --- 2\n",
      "legendhasit/xgen-7b-8k-inst-8bit/4bit --- fredsum --- 0\n",
      "legendhasit/xgen-7b-8k-inst-8bit/4bit --- fredsum --- 1\n",
      "legendhasit/xgen-7b-8k-inst-8bit/4bit --- fredsum --- 2\n",
      "legendhasit/xgen-7b-8k-inst-8bit/4bit --- icsi --- 0\n",
      "legendhasit/xgen-7b-8k-inst-8bit/4bit --- icsi --- 1\n",
      "legendhasit/xgen-7b-8k-inst-8bit/4bit --- icsi --- 2\n",
      "legendhasit/xgen-7b-8k-inst-8bit/4bit --- mediasum --- 0\n",
      "legendhasit/xgen-7b-8k-inst-8bit/4bit --- mediasum --- 1\n",
      "legendhasit/xgen-7b-8k-inst-8bit/4bit --- mediasum --- 2\n",
      "legendhasit/xgen-7b-8k-inst-8bit/4bit --- summre --- 0\n",
      "legendhasit/xgen-7b-8k-inst-8bit/4bit --- summre --- 1\n",
      "legendhasit/xgen-7b-8k-inst-8bit/4bit --- summre --- 2\n",
      "mosaicml/mpt-7b-instruct --- ami --- 0\n",
      "mosaicml/mpt-7b-instruct --- ami --- 1\n",
      "mosaicml/mpt-7b-instruct --- ami --- 2\n",
      "mosaicml/mpt-7b-instruct --- fredsum --- 0\n",
      "mosaicml/mpt-7b-instruct --- fredsum --- 1\n",
      "mosaicml/mpt-7b-instruct --- fredsum --- 2\n",
      "mosaicml/mpt-7b-instruct --- icsi --- 0\n",
      "mosaicml/mpt-7b-instruct --- icsi --- 1\n",
      "mosaicml/mpt-7b-instruct --- icsi --- 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (6433 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mosaicml/mpt-7b-instruct --- mediasum --- 0\n",
      "mosaicml/mpt-7b-instruct --- mediasum --- 1\n",
      "mosaicml/mpt-7b-instruct --- mediasum --- 2\n",
      "mosaicml/mpt-7b-instruct --- summre --- 0\n",
      "mosaicml/mpt-7b-instruct --- summre --- 1\n",
      "mosaicml/mpt-7b-instruct --- summre --- 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "legendhasit/xgen-7b-8k-inst-8bit --- ami --- 0\n",
      "legendhasit/xgen-7b-8k-inst-8bit --- ami --- 1\n",
      "legendhasit/xgen-7b-8k-inst-8bit --- ami --- 2\n",
      "legendhasit/xgen-7b-8k-inst-8bit --- fredsum --- 0\n",
      "legendhasit/xgen-7b-8k-inst-8bit --- fredsum --- 1\n",
      "legendhasit/xgen-7b-8k-inst-8bit --- fredsum --- 2\n",
      "legendhasit/xgen-7b-8k-inst-8bit --- icsi --- 0\n",
      "legendhasit/xgen-7b-8k-inst-8bit --- icsi --- 1\n",
      "legendhasit/xgen-7b-8k-inst-8bit --- icsi --- 2\n",
      "legendhasit/xgen-7b-8k-inst-8bit --- mediasum --- 0\n",
      "legendhasit/xgen-7b-8k-inst-8bit --- mediasum --- 1\n",
      "legendhasit/xgen-7b-8k-inst-8bit --- mediasum --- 2\n",
      "legendhasit/xgen-7b-8k-inst-8bit --- summre --- 0\n",
      "legendhasit/xgen-7b-8k-inst-8bit --- summre --- 1\n",
      "legendhasit/xgen-7b-8k-inst-8bit --- summre --- 2\n",
      "tiiuae/falcon-7b --- ami --- 0\n",
      "tiiuae/falcon-7b --- ami --- 1\n",
      "tiiuae/falcon-7b --- ami --- 2\n",
      "tiiuae/falcon-7b --- fredsum --- 0\n",
      "tiiuae/falcon-7b --- fredsum --- 1\n",
      "tiiuae/falcon-7b --- fredsum --- 2\n",
      "tiiuae/falcon-7b --- icsi --- 0\n",
      "tiiuae/falcon-7b --- icsi --- 1\n",
      "tiiuae/falcon-7b --- icsi --- 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (6636 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiiuae/falcon-7b --- mediasum --- 0\n",
      "tiiuae/falcon-7b --- mediasum --- 1\n",
      "tiiuae/falcon-7b --- mediasum --- 2\n",
      "tiiuae/falcon-7b --- summre --- 0\n",
      "tiiuae/falcon-7b --- summre --- 1\n",
      "tiiuae/falcon-7b --- summre --- 2\n",
      "meta-llama/Llama-2-7b-chat-hf --- ami --- 0\n",
      "meta-llama/Llama-2-7b-chat-hf --- ami --- 1\n",
      "meta-llama/Llama-2-7b-chat-hf --- ami --- 2\n",
      "meta-llama/Llama-2-7b-chat-hf --- fredsum --- 0\n",
      "meta-llama/Llama-2-7b-chat-hf --- fredsum --- 1\n",
      "meta-llama/Llama-2-7b-chat-hf --- fredsum --- 2\n",
      "meta-llama/Llama-2-7b-chat-hf --- icsi --- 0\n",
      "meta-llama/Llama-2-7b-chat-hf --- icsi --- 1\n",
      "meta-llama/Llama-2-7b-chat-hf --- icsi --- 2\n",
      "meta-llama/Llama-2-7b-chat-hf --- mediasum --- 0\n",
      "meta-llama/Llama-2-7b-chat-hf --- mediasum --- 1\n",
      "meta-llama/Llama-2-7b-chat-hf --- mediasum --- 2\n",
      "meta-llama/Llama-2-7b-chat-hf --- summre --- 0\n",
      "meta-llama/Llama-2-7b-chat-hf --- summre --- 1\n",
      "meta-llama/Llama-2-7b-chat-hf --- summre --- 2\n"
     ]
    }
   ],
   "source": [
    "inputs_folder = \"0\"\n",
    "nb_to_tokenizer = {}\n",
    "\n",
    "# On peut imaginer une première passe qui découpe les inputs et forme les instructions, rangés dans des fichiers .json, dans un dossier instructions. Il faut utiliser le bon tokenizer -> Process à répéter pour chaque modèle. instructions/<tokenizer_name>/...\n",
    "\n",
    "dataset_folder = \"datasets/KeyPointsExtractionTest\"\n",
    "mkdir('inputs')\n",
    "for model_name in used_models_name:\n",
    "    tokenizer = tokenizers[model_name]()\n",
    "\n",
    "    for corpus in os.listdir(dataset_folder):\n",
    "        for name_file in os.listdir(dataset_folder + \"/\" + corpus):\n",
    "            print(model_name + \" --- \" + corpus + \" --- \" + name_file)\n",
    "            file = open(dataset_folder + \"/\" + corpus + \"/\" + name_file, 'r', encoding='utf-8')\n",
    "            data = json.load(file)\n",
    "            file.close()\n",
    "        \n",
    "            new_data = {}\n",
    "            new_data[\"tokenizer\"] = model_name\n",
    "            new_data[\"original_path\"] = corpus + \"/\" + name_file\n",
    "            new_file_name = corpus + \"_\" + name_file\n",
    "            new_data[\"summaries\"] = data[\"summaries\"]\n",
    "            en_chunks = chunkize(data[\"text_en\"], tokenizer)\n",
    "            #fr_chunks = chunkize(data[\"text_fr\"], tokenizer)\n",
    "            new_data[\"en_chunks\"] = []\n",
    "            #new_data[\"fr_chunks\"] = []\n",
    "            new_data[\"MAX_TOKEN_CHUNK_SIZE\"] = str(MAX_TOKEN_CHUNK_SIZE)\n",
    "\n",
    "            for i in range(len(en_chunks)):\n",
    "                chunk = {\"text_en\":en_chunks[i], \"nb_words_en\":str(count_words(en_chunks[i])), \"nb_characters_en\":str(len(en_chunks[i]))}\n",
    "                new_data[\"en_chunks\"].append(chunk)\n",
    "\n",
    "            #for i in range(len(fr_chunks)):\n",
    "            #    chunk = {\"text_fr\":fr_chunks[i], \"nb_words_fr\":str(count_words(fr_chunks[i])), \"nb_characters_fr\":str(len(fr_chunks[i]))}\n",
    "            #    new_data[\"fr_chunks\"].append(chunk)\n",
    "\n",
    "            mkdir(\"inputs/\" + simplified_names[model_name] + \"_\" + str(MAX_TOKEN_CHUNK_SIZE))\n",
    "            mkdir(\"inputs/\" + simplified_names[model_name] + \"_\" + str(MAX_TOKEN_CHUNK_SIZE) + \"/\" + corpus)\n",
    "            new_file = open(\"inputs/\" + simplified_names[model_name] + \"_\" + str(MAX_TOKEN_CHUNK_SIZE) + \"/\" + corpus + \"/\" + new_file_name, 'w', encoding='utf-8')\n",
    "            json.dump(new_data, new_file, ensure_ascii=False, indent=4)\n",
    "            new_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'legendhasit/xgen-7b-8k-inst-8bit': <function get_tokenizer_XGen8bit at 0x000001799D8FA5F0>, 'legendhasit/xgen-7b-8k-inst-8bit/4bit': <function get_tokenizer_XGen8bit at 0x000001799D8FA5F0>, 'Salesforce/xgen-7b-8k-inst': <function get_tokenizer_XGen at 0x000001799D8F91B0>, 'mosaicml/mpt-7b-instruct': <function get_tokenizer_MPT7B at 0x000001799C35C9D0>, 'meta-llama/Llama-2-7b-chat-hf': <function get_tokenizer_llama2_7b at 0x000001799D8FAD40>, 'tiiuae/falcon-7b': <function get_tokenizer_falcon7b at 0x000001799D8FAA70>}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every time the script is casted, it must register the results in the \"results\" folder, without smashing the existing results\n",
    "# Then, it is stored in a file whose name is the number of the result, and this file contains a little .txt note describing what was the experiment.\n",
    "# In such a folder, all the generations are json files with additional information like the prompt used, the number of samples, the path of the input text\n",
    "\n",
    "# Now, what is specific to the model ? The prompt (different headers potentially), the tokenizer, the model itself, the way the output is displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir(\"results\")\n",
    "\n",
    "archives = os.listdir(\"results\")\n",
    "output_folder_path = \"results/\"+str(len(archives))\n",
    "mkdir(output_folder_path) # Create a folder for the last results.\n",
    "desc_file = open(output_folder_path + \"/desc.txt\", 'w', encoding='utf-8')\n",
    "desc_file.write(desc)\n",
    "desc_file.close()\n",
    "\n",
    "initial_time = time.time()\n",
    "\n",
    "model_total = len(used_models_name)\n",
    "model_index = 0\n",
    "for model_name in used_models_name: # For each model\n",
    "    model_index += 1\n",
    "    print(\"---- Model : \" + model_name + \" (\" + str(model_index) + \"/\" + str(model_total) +\")----              (loading tokenizer and model...)\")\n",
    "    load_model_time = time.time()\n",
    "    prompt_template = prompt_templates[model_name]\n",
    "    tokenizer = tokenizers[model_name]()\n",
    "    model = models[model_name]()\n",
    "    load_model_time = time.time() - load_model_time\n",
    "    !nvidia-smi\n",
    "    print(\"Tokenizer and model loaded in\", datetime.timedelta(seconds=int(load_model_time)), 'seconds')\n",
    "    inputs_folder_path = \"inputs/\" + simplified_names[model_name] + \"_\" + str(MAX_TOKEN_CHUNK_SIZE)\n",
    "    corpora = os.listdir(inputs_folder_path)\n",
    "    for corpus in corpora:\n",
    "        !nvidia-smi\n",
    "        file_index = 0\n",
    "        files_name = os.listdir(inputs_folder_path + \"/\" + corpus)\n",
    "        file_total = len(files_name)\n",
    "        for file_name in files_name:\n",
    "            file_path = inputs_folder_path + \"/\" + corpus + \"/\" + file_name\n",
    "            if file_name==\"ipynb_checkpoints\" or os.path.isdir(file_path):\n",
    "                continue\n",
    "            file_time = time.time()\n",
    "            file_index+=1\n",
    "            print(\"Starting inference for text \" + str(file_index) + \"/\" + str(file_total) + \" in the \" + corpus + \" corpus.\") # Possible errors in the display: file_total also counts the .ipynb checkpoints potentially in the directory.\n",
    "            file = open(file_path, 'r', encoding='utf-8')\n",
    "            input_data = json.load(file)\n",
    "            file.close()\n",
    "            for text_key in text_keys:\n",
    "                en_or_fr = text_key[-2:]\n",
    "                chunks = input_data[en_or_fr + \"_chunks\"]\n",
    "                output_data = {\"chunks\":[]}\n",
    "                output_data[\"input_path\"] = file_path\n",
    "                output_data[\"model\"] = model_name\n",
    "                output_data[\"input_language\"] = text_key # NE GERE PAS ENCORE BIEN FRANCAIS\n",
    "                output_data[\"MAX_TOKEN_CHUNK_SIZE\"] = input_data[\"MAX_TOKEN_CHUNK_SIZE\"]\n",
    "                # RAJOUTER UN AFFICHAGE pour toutes les chunks\n",
    "                precedent_text = \"\"\n",
    "                for i in range(len(chunks) + 1): # 1 fois le 1er chunk, puis une fois 1er et 2eme, puis une fois 2eme et 3eme etc. puis une fois avant dernier dernier puis une fois dernier\n",
    "                    print(\"Chunk \" + str(i) + \"/\" + str(len(chunks) + 1) + \" ...\")\n",
    "                    if i==len(chunks):\n",
    "                        chunk = chunks[-1]\n",
    "                        precedent_text = \"\"\n",
    "                    else: chunk = chunks[i]\n",
    "                    chunk_data = {}\n",
    "                    input_text = precedent_text + chunk[text_key]\n",
    "                    precedent_text = chunk[text_key]\n",
    "                    instruction_template = instruction_templates[text_key]\n",
    "                    full_instruction = instruction_template.format(text=input_text)\n",
    "                    prompt = prompt_template.format(instruction=full_instruction)\n",
    "\n",
    "                    # Probably specific to the model\n",
    "                    input=0\n",
    "                    if model_name==\"tiiuae/falcon-7b\": input = tokenizer(prompt, return_token_type_ids=False, return_tensors=\"pt\").to('cuda')# A AMELIORER: Il faut un nouveau dico\n",
    "                    else: input = tokenizer(prompt, return_tensors=\"pt\").to('cuda') # Le renvoie sur le GPU car au départ, c'est généré en CPU le tensuer des tokens\n",
    "\n",
    "                    input_length = len(input['input_ids'][0])\n",
    "                    output_name = file_name + \"_\" + text_key + \".json\"\n",
    "                    output_path = output_folder_path + \"/\" + output_name\n",
    "                    if \"instruction\" not in output_data: output_data[\"instruction\"] = instruction_template.format(text=\"\")\n",
    "                    chunk_data[\"success\"] = \"0\"\n",
    "                    chunk_data[\"input_length\"] = str(input_length)\n",
    "\n",
    "                    if input_length > token_limit: # In this case, a file is created but no inference is made\n",
    "                        print(\"/!\\ With model \" + model_name + \", the input length is above the token limit for \" + text_key + \" input in \" + file_path + \" (\" + str(input_length) + \" > \" + str(token_limit) + \")\")\n",
    "\n",
    "                    else: # In this case, the output is inferred, treated to keep only the generation and the length information\n",
    "                        # PROBABLY SPECIFIC TO THE MODEL\n",
    "                        sample = infer[model_name](input, model)\n",
    "                        sample_length = len(sample[0])\n",
    "                        ### SPECIFIC TO THE MODEL ----------------------------------------\n",
    "                        if sample_length > context_lengths[model_name]: # There was a context window overflow\n",
    "                            output_data[\"over_context\"] = \"1\"\n",
    "                        else:\n",
    "                            output_data[\"over_context\"] = \"0\"\n",
    "                        output_length = sample_length - input_length\n",
    "                        output_data[\"output_length\"] = output_length\n",
    "                        full_output = tokenizer.decode(sample[0]).strip() # prompt + output generation\n",
    "                        # Ideally, treat_output is a dictionnary that associates model_name to a function taking full_output as an argument\n",
    "                        output = treat_output[model_name](full_output)\n",
    "                        chunk_data[\"text\"] = output\n",
    "                        chunk_data[\"nb_characters\"] = len(output)\n",
    "                        chunk_data[\"nb_words\"] = count_words(output)\n",
    "                        chunk_data[\"success\"] = \"1\"\n",
    "                        chunk_data[\"language_output\"] = str(classify(output))\n",
    "                    output_data[\"chunks\"].append(chunk_data)\n",
    "                    \n",
    "                # SAVING THE OUTPUT\n",
    "                output_file = open(output_path, 'w', encoding='utf-8')\n",
    "                json.dump(output_data, output_file, indent=4, ensure_ascii=False)\n",
    "                output_file.close()\n",
    "            file_time = time.time() - file_time\n",
    "            print(\"Inference done for French and English in\", datetime.timedelta(seconds=int(file_time)), \"seconds.\")\n",
    "    # HERE, clear memory... remove model, tokenizer from memory\n",
    "                \n",
    "delta = time.time() - initial_time\n",
    "print('Done! Took', datetime.timedelta(seconds=int(delta)), 'seconds')\n",
    "# Write treatOutput function / dictionnary\n",
    "\n",
    "\n",
    "# RETIRER le fichier template dans samsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['desc.txt', 'dialogsum_1_text_en.json', 'dialogsum_2_text_en.json', 'dialogsum_3_text_en.json', 'dialogsum_4_text_en.json', 'dialogsum_5_text_en.json', 'fredsum_1_text_en.json', 'fredsum_2_text_en.json', 'fredsum_3_text_en.json', 'fredsum_4_text_en.json', 'fredsum_5_text_en.json', 'mediasum_1_text_en.json', 'mediasum_2_text_en.json', 'mediasum_3_text_en.json', 'mediasum_4_text_en.json', 'mediasum_5_text_en.json', 'samsum_1_text_en.json', 'samsum_2_text_en.json', 'samsum_3_text_en.json', 'samsum_4_text_en.json', 'samsum_5_text_en.json', 'scores.csv', 'xsum_1_text_en.json', 'xsum_2_text_en.json', 'xsum_3_text_en.json', 'xsum_4_text_en.json', 'xsum_5_text_en.json']\n",
      "0\n",
      "25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:42<00:00, 51.45s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1559143.69 seconds, 0.00 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# A FAIRE : GERER LE BERT MULTILINGUE\n",
    "# Anglais vers français : taux de foisonnement de 20%\n",
    "\n",
    "rouge = evaluate.load('rouge') # Warning ! When comparing many references to a prediction, an average is performed, not a max...\n",
    "bertscore = evaluate.load('bertscore') # The BERTScore calculation will take a lot of time... (if not performed in only one call)\n",
    "\n",
    "mkdir('results')\n",
    "\n",
    "nb_batch = len(os.listdir(\"results\")) # Mettre le numéro du \"batch d'inférence\"\n",
    "\n",
    "# Define the place of storage : \"results/x/scores.csv\"\n",
    "output_names = os.listdir(\"results/\" + nb_batch)\n",
    "# Lists used for the BERT score computation\n",
    "references = {\"0\":[], \"1\":[]}\n",
    "predictions = {\"0\":[], \"1\":[]}\n",
    "# Columns of the csv file\n",
    "output_ids, rouges, input_nb_words_list, nb_words_generated_summaries, nb_words_gold_summaries, outputs_success, outputs_over_context, languages_output, languages_input = [], [], [], [], [], [], [], [], []\n",
    "print(output_names)\n",
    "for output_name in output_names:\n",
    "    # Makes sure the file is an output file, and opens it\n",
    "    output_path = \"results/\" + nb_batch + \"/\" + output_name\n",
    "    if output_name==\"scores.csv\" or output_name==\"desc.txt\" or os.path.isdir(output_path):\n",
    "        continue\n",
    "    output_data = load_json_into_dict(output_path)\n",
    "    generated_summary = output_data[\"text\"]\n",
    "    output_language = str(classify(generated_summary))\n",
    "    language_code = \"text_en\" \n",
    "    if output_language in number_to_code:\n",
    "        language_code = number_to_code[output_language] # Le language code est soit text_fr si output fr, soit text_en si output en ou toute autre langue ! Juste pour choisir un résumé de référence...\n",
    "    \n",
    "    find_path = output_name.split('_') # 0: corpus, 1: name json input file, 2: text (quite useless) 3: fr.json or en.json\n",
    "    input_path = \"datasets/\" + name_dataset + \"/\" + find_path[0] + \"/\" + find_path[1]\n",
    "    input_data = load_json_into_dict(input_path)\n",
    "    summaries_data = input_data[\"summaries\"]\n",
    "    # Computation of Rouge\n",
    "    golds = [] # Sert uniquement pour le calcul du BertScore avec plusieurs références\n",
    "    max_rouge2, max_rougel, nb_words_closest_gold = maxRouge(summaries_data, language_code, generated_summary, golds)\n",
    "    # For BertScore\n",
    "    if output_language in {\"0\", \"1\"}:\n",
    "        predictions[output_language].append(generated_summary)\n",
    "        references[output_language].append(golds)\n",
    "    # Fill the columns\n",
    "    rouges.append([max_rouge2, max_rougel])\n",
    "    output_ids.append(output_name)\n",
    "    nb_words_gold_summaries.append(nb_words_closest_gold)\n",
    "    nb_words_generated_summaries.append(output_data['nb_words'])\n",
    "    outputs_success.append(output_data['success'])\n",
    "    outputs_over_context.append(output_data['over_context'])\n",
    "    languages_input.append(output_data['input_language'])\n",
    "    input_nb_words_list.append(int(input_data['nb_words_' + language_code[-2:]]))\n",
    "    languages_output.append(output_language)\n",
    "\n",
    "print(len(predictions[\"0\"]))\n",
    "print(len(predictions[\"1\"]))\n",
    "\n",
    "\n",
    "#max_bertscores_fr = bertscore.compute(predictions=predictions[\"0\"], references=references[\"0\"], lang='fr', rescale_with_baseline=True, verbose=True)['f1']\n",
    "max_bertscores_en = bertscore.compute(predictions=predictions[\"1\"], references=references[\"1\"], lang='en', rescale_with_baseline=True, verbose=True)['f1']\n",
    "\n",
    "index_fr = 0\n",
    "index_en = 0\n",
    "rows = []\n",
    "for i in range(len(output_ids)):\n",
    "    max_bertscore = 0\n",
    "    if languages_output[i]==\"0\":\n",
    "        #max_bertscore = max_bertscores_fr[index_fr]\n",
    "        index_fr += 1\n",
    "    elif languages_output[i]==\"1\":\n",
    "        max_bertscore = max_bertscores_en[index_en]\n",
    "        index_en += 1\n",
    "    rows.append([output_ids[i], rouges[i][0], rouges[i][1], max_bertscore, input_nb_words_list[i], nb_words_gold_summaries[i], nb_words_generated_summaries[i], outputs_success[i], outputs_over_context[i], languages_output[i], languages_input[i]])\n",
    "\n",
    "header = [[\"input_path\", \"rouge2\", \"rougel\", \"bertscore\", \"nb_words_input\", \"nb_words_gold\", \"nb_words_generated\", \"success\", \"over_context\", \"output_language\", \"input_language\"]]\n",
    "\n",
    "storage_file = open('results/' + nb_batch + '/scores1.csv', mode='w', newline='', encoding='utf-8')# MODIFIER LE SCORE1 ----------------------------------------------------------\n",
    "csv_writer = csv.writer(storage_file)\n",
    "csv_writer.writerows(header + rows)\n",
    "storage_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n"
     ]
    }
   ],
   "source": [
    "a = \"text_en\"\n",
    "print(a[-2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hardware use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvidia-smi' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "# model.hf_device_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "name = 'mosaicml/mpt-7b-instruct'\n",
    "name2 = \"Trelis/mpt-7b-instruct-hosted-inference-8bit\"\n",
    "\n",
    "config = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\n",
    "config.max_seq_len = 4096 # (input + output) tokens can now be up to 4096\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('legendhasit/xgen-7b-8k-inst-8bit', trust_remote_code=True)\n",
    "\n",
    "!nvidia-smi\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "  name,\n",
    "  config=config,\n",
    "  trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "input_test = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "sample = model.generate(**input_test, do_sample=True, max_new_tokens=100, top_k=20, temperature=0.3)\n",
    "print(tokenizer.decode(sample[0]).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [04:06<00:00, 123.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1107629.12 seconds, 0.00 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# A FAIRE : GERER LE BERT MULTILINGUE\n",
    "# COnserver ce 2eme programme adapté aux anciennes générations\n",
    "# Anglais vers français : taux de foisonnement de 20%\n",
    "\n",
    "rouge = evaluate.load('rouge') # Warning ! When comparing many references to a prediction, an average is performed, not a max...\n",
    "bertscore = evaluate.load('bertscore') # The BERTScore calculation will take a lot of time... (if not performed in only one call)\n",
    "\n",
    "mkdir('results')\n",
    "\n",
    "nb_batch = \"2\" # Mettre le numéro du \"batch d'inférence\"\n",
    "\n",
    "# Define the place of storage : \"results/x/scores.csv\"\n",
    "output_names = os.listdir(\"results/\" + nb_batch)\n",
    "# The texts for bertscore\n",
    "references, predictions = [], []\n",
    "# The columns of the csv file\n",
    "output_ids, rouges, input_nb_words_list, nb_words_generated_summaries, nb_words_gold_summaries, outputs_success, outputs_over_context, languages_output, languages_input = [], [], [], [], [], [], [], [], []\n",
    "\n",
    "for output_name in output_names:\n",
    "    # To ensure the read file is an output\n",
    "    output_path = \"results/\" + nb_batch + \"/\" + output_name\n",
    "    if output_name==\"scores.csv\" or output_name==\"desc.txt\" or os.path.isdir(output_path):\n",
    "        continue\n",
    "    # Extraire le dictionnaire de données de l'ouptut\n",
    "    output_data = load_json_into_dict(output_path)\n",
    "    # Obtenir l'input, le langage de l'output, les résumés de référence\n",
    "    language_output = str(classify(generated_summary))\n",
    "    language_code = number_to_code[language_output]\n",
    "    find_path = output_name.split('_') # 0: corpus, 1: name json input file, 2: text (quite useless) 3: fr.json or en.json\n",
    "    input_path = \"datasets/\" + name_dataset + \"/\" + find_path[0] + \"/\" + find_path[1]\n",
    "    input_data = load_json_into_dict(input_path)\n",
    "    summaries_data = input_data[\"summaries\"]\n",
    "    # Preparation pour Max Rouge\n",
    "    golds = [] # Sert uniquement pour le calcul du BertScore avec plusieurs références\n",
    "    max_rouge2, max_rougel, nb_words_closest_gold = maxRouge(summaries_data, language_code, generated_summary, golds)\n",
    "    # Fill in the row\n",
    "    output_ids.append(output_name)\n",
    "    languages_output.append(language_output)\n",
    "    rouges.append([max_rouge2, max_rougel])\n",
    "    predictions.append(generated_summary)\n",
    "    references.append(golds)\n",
    "    nb_words_gold_summaries.append(nb_words_closest_gold)\n",
    "    nb_words_generated_summaries.append(output_data['nb_words'])\n",
    "    outputs_success.append(output_data['success'])\n",
    "    outputs_over_context.append(output_data['over_context'])\n",
    "    input_nb_words_list.append(int(input_data['nb_words_' + language_code]))\n",
    "    languages_input.append(key_to_language[output_data['input_language']])\n",
    "\n",
    "max_bertscores = bertscore.compute(predictions=predictions, references=references, lang='en', rescale_with_baseline=True, verbose=True)['f1'] # PROBLEM WITH LANGUAGE USED\n",
    "\n",
    "storage_file = open('results/' + nb_batch + '/scores4.csv', mode='w', newline='', encoding='utf-8')\n",
    "csv_writer = csv.writer(storage_file)\n",
    "\n",
    "rows = [[output_ids[i], rouges[i][0], rouges[i][1], max_bertscores[i], input_nb_words_list[i], nb_words_gold_summaries[i], nb_words_generated_summaries[i], outputs_success[i], outputs_over_context[i], languages_output[i], languages_input[i]] for i in range(len(output_ids))]\n",
    "\n",
    "header = [[\"input_path\", \"rouge2\", \"rougel\", \"bertscore\", \"nb_words_input\", \"nb_words_gold\", \"nb_words_generated\", \"success\", \"over_context\", \"output_language\", \"input_language\"]]\n",
    "\n",
    "csv_writer.writerows(header + rows)\n",
    "\n",
    "storage_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 776/776 [00:00<?, ?B/s] \n",
      "c:\\Users\\rayci\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rayci\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 13.9MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 6.20MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 161kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", trust_remote_code=True)\n",
    "print(tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizers[\"tiiuae/falcon-7b\"]()\n",
    "model = models[\"tiiuae/falcon-7b\"]()\n",
    "\n",
    "prompt = prompt_templates[\"tiiuae/falcon-7b\"].format(\"Hey, how are you my dear Falcon ?\")\n",
    "tokenized_input = tokenizer(prompt, return_token_type_ids=False, return_tensors=\"pt\").to('cuda')\n",
    "output = infer[\"tiiuae/falcon-7b\"](tokenized_input, model)\n",
    "print(tokenizer.decode(output[0]).strip())\n",
    "print()\n",
    "print(\"Treated ================================\")\n",
    "print()\n",
    "print(treat_output[\"tiiuae/falcon-7b\"](tokenizer.decode(output[0]).strip()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
