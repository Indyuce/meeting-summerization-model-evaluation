{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install einops\n",
    "# !GITHUB_ACTIONS=true pip install auto-gptq\n",
    "# !pip install tokenizers --upgrade\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connexion HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !huggingface-cli login Pas oublier (lancer un terminal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import time\n",
    "import datetime\n",
    "import evaluate\n",
    "import random\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoConfig\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Working conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "name_dataset = \"LanguageTestDataSet\" # The dataset must be in a certain format\n",
    "\n",
    "# Models used\n",
    "used_models_name = {\n",
    "    \"Salesforce/xgen-7b-8k-inst\",\n",
    "    \"legendhasit/xgen-7b-8k-inst-8bit\",\n",
    "    \"mosaicml/mpt-7b-instruct\", # 1 GPU (2 shards de 10 Go)- To quantize\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\"} # Models taken for computation \n",
    "\n",
    "# Input limits\n",
    "token_limit = 3500 # To be determined with the context length of the used model\n",
    "\n",
    "# MAX token length for chunkization\n",
    "MAX_TOKEN_CHUNK_SIZE = 750\n",
    "\n",
    "# Description of the inference wanted\n",
    "desc = \"This inference batch is aimed at testing MPT7B on summarization. There is no French summarization at all.\"\n",
    "\n",
    "# Caracteristics of the inference wanted\n",
    "max_new_tokens=700\n",
    "top_k=20\n",
    "temperature=0.3\n",
    "\n",
    "# Metrics thresholds\n",
    "thresholds = {\"rouge1\":0.4,\"rouge2\":0.4,\"rougeL\":0.4,\"bertscore\":0.5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions, classes and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BulletPoint:\n",
    "    def __init__(self, number, text): # number : (i, j) où i désigne le numéro du chunk résumé, j désigne le numéro du bullet point associé dans le chunk i.\n",
    "        self.bullet_point_location = number\n",
    "        self.text = text\n",
    "\n",
    "# Prompting elements\n",
    "instruction_templates = {\"text_en\":\"Provide a list of key points for the following text:\\n\\n{text}\", \"text_fr\":\"Résume le texte suivant:\\n\\n{text}\"} # Prompt à améliorer peut-être avec une liste de independent key points\n",
    "text_keys = {'text_en'} # Cet ensemble donne les clés pour le texte français et anglais. On retire 'text_fr' pour les modèles anglais\n",
    "key_to_language = {'text_fr':'0', 'text_en':'1'}\n",
    "language_to_key = {'0':'text_fr', '1':'text_en'}\n",
    "number_to_code = {'0':'text_fr', '1':'text_en', '2':'text_en'}\n",
    "\n",
    "# Miscellaneous useful functions and constants\n",
    "def count_words(s):\n",
    "    words = re.findall(r'\\b\\w+\\b', s)  # Find all word-like sequences using regular expression\n",
    "    return len(words)  # Return the number of words\n",
    "\n",
    "def mkdir(folder_path):\n",
    "    try:\n",
    "        os.mkdir(folder_path)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "def classify(text): # Classificateur déterministe basique, qui accumule des indices de langue et renvoie le langage avec le plsu haut score\n",
    "    score_en = 0\n",
    "    score_fr = 0\n",
    "    score_en += len(find_all_occurrences_regex(text, \" and \" ))\n",
    "    score_en += len(find_all_occurrences_regex(text, \" of \" ))\n",
    "    score_en += len(find_all_occurrences_regex(text, \" the \" ))\n",
    "    score_en += len(find_all_occurrences_regex(text, \" in \" ))\n",
    "    score_en += len(find_all_occurrences_regex(text, \" is \" ))\n",
    "    score_en += len(find_all_occurrences_regex(text, \" for \" ))\n",
    "    score_en += len(find_all_occurrences_regex(text, \" how \" ))\n",
    "    score_en += len(find_all_occurrences_regex(text, \" with \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" le \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" la \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" de \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" un \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" une \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" et \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" à \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" avec \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" il \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" pour \" ))\n",
    "    if score_fr + score_en <= 3: return 2\n",
    "    if score_fr > score_en: return 0\n",
    "    return 1\n",
    "\n",
    "def find_all_occurrences_regex(text, pattern):\n",
    "    occurrences = [match.start() for match in re.finditer(pattern, text)]\n",
    "    return occurrences\n",
    "\n",
    "def maxRouge(summaries_data, language_code, generated_summary, golds): # Pour un dictionnaire de résumés de référence, pour un langage donné, renvoie le résumé de référence le plus proche du résumé prédit au sens de rouge et les scores associés\n",
    "    max_rouge2 = -1\n",
    "    max_rougel = -1\n",
    "    nb_words_closest_gold = 0\n",
    "\n",
    "    for i in range(len(summaries_data)):\n",
    "        summary_data = summaries_data[i]\n",
    "        summary_text = summary_data[language_code]\n",
    "        result_rouge = rouge.compute(predictions=[generated_summary], references=[summary_text], use_aggregator=False)\n",
    "        golds.append(summary_text)\n",
    "        if result_rouge['rouge2'][0] > max_rouge2:\n",
    "            max_rouge2 = result_rouge['rouge2'][0]\n",
    "            nb_words_closest_gold = summary_data['nb_words_' + language_code[-2:]]\n",
    "        if result_rouge['rougeL'][0] > max_rougel:\n",
    "            max_rougel = result_rouge['rougeL'][0]\n",
    "\n",
    "    return max_rouge2, max_rougel, nb_words_closest_gold\n",
    "\n",
    "def load_json_into_dict(path):\n",
    "    file = open(path, 'r', encoding='utf-8')\n",
    "    dict = json.load(file)\n",
    "    file.close()\n",
    "    return dict\n",
    "\n",
    "def save_dict_into_json(dict, path):\n",
    "    file = open(path, 'r', encoding='utf-8')\n",
    "    json.dump(dict, file, indent=4, ensure_ascii=False)\n",
    "    file.close()\n",
    "\n",
    "def token_len(text, tokenizer):\n",
    "    return len(tokenizer(text, return_tensors=\"pt\")['input_ids'][0])\n",
    "\n",
    "def append_to_chunk(current_chunk, utterance):\n",
    "    if len(current_chunk) > 0:\n",
    "        current_chunk += '\\n'\n",
    "    current_chunk += utterance\n",
    "    return current_chunk\n",
    "\n",
    "def append_to_split(current_split, sentence): # Les ? . ! ... sont remplacés par des \". \"\n",
    "    if len(current_split) > 0:\n",
    "        current_split += '. '\n",
    "    current_split += sentence\n",
    "    return current_split\n",
    "\n",
    "def chunkize(text, tokenizer):\n",
    "    \"\"\"\n",
    "    Greedy implementation of a dialogue transcript chunking algorithm. This method returns a list of transcript chunks.\n",
    "    - It priorities stability over performance. There is a set maximum chunk size for LLM inference stability. Really long utterances bypass this limit.\n",
    "    - It guarantees the cuts are made at utterance ends (\\n).\n",
    "    - It counts everything in MODEL TOKENS and not characters for more exact experiments.\n",
    "    \"\"\"\n",
    "    \n",
    "    chunks = [] # Final list of transcript chunks. This makes up the loop invariant\n",
    "    utterances = text.split('\\n') # Transcript is split into sentences\n",
    "    utterances.reverse() # Reverse everything!!\n",
    "    current_chunk = ''\n",
    "\n",
    "    # While there is still an utterance to process\n",
    "    while len(utterances) > 0:\n",
    "        utterance = utterances.pop()\n",
    "        new_current_chunk = append_to_chunk(current_chunk, utterance)\n",
    "        if token_len(utterance, tokenizer) > MAX_TOKEN_CHUNK_SIZE:\n",
    "            if current_chunk != '': chunks.append(current_chunk)\n",
    "            splits = split_utterance(utterance, tokenizer) # Découpe une utterance en s'assurant que chaque coupe soit plus petite que la taille max\n",
    "            for split in splits[:-1]: chunks.append(split) # Except the last split, that will be used as the next current_chunk\n",
    "            current_chunk = splits[-1]\n",
    "\n",
    "        # Add to current chunk and proceed to next\n",
    "        elif token_len(new_current_chunk, tokenizer) <= MAX_TOKEN_CHUNK_SIZE:\n",
    "            current_chunk = new_current_chunk\n",
    "        \n",
    "        # Current chunk is big enough, append to list and create new one\n",
    "        else:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = utterance\n",
    "\n",
    "    if len(current_chunk) > 0:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def split_utterance(text, tokenizer):  \n",
    "    splits = [] # Final list of transcript chunks. This makes up the loop invariant\n",
    "    sentence_pattern = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
    "    sentences = re.split(sentence_pattern, text) # Transcript is split into sentences\n",
    "    sentences.reverse() # Reverse everything!!\n",
    "    current_split = ''\n",
    "    # While there is still an utterance to process\n",
    "    while len(sentences) > 0:\n",
    "        sentence = sentences.pop()\n",
    "        new_current_split = append_to_split(current_split, sentence)\n",
    "        if token_len(new_current_split, tokenizer) > MAX_TOKEN_CHUNK_SIZE:\n",
    "            if current_split =='': print(\" /!\\ Split issue : too long sentence.\")\n",
    "            splits.append(current_split)\n",
    "            current_split = sentence\n",
    "\n",
    "        # Add to current chunk and proceed to next\n",
    "        else:\n",
    "            current_split = new_current_split\n",
    "    \n",
    "    if len(current_split) > 0:\n",
    "        splits.append(current_split)\n",
    "\n",
    "    return splits\n",
    "\n",
    "def identify(line): # Renvoie -1 si la ligne de texte envoyée n'a pas le format classique d'un bullet point, renvoie le bullet point \"nettoyé\" de son format sinon\n",
    "    line_spe = line.split() # Permet de retirer les espaces\n",
    "    if len(line_spe)==0: return -1\n",
    "    if line_spe[0][0].isalpha() or line_spe[-1][-1]==\":\":\n",
    "        return -1\n",
    "    if line_spe[0][0].isdigit():\n",
    "        return \" \".join(line_spe[1:])\n",
    "    if line_spe[0][0]==\"-\" or line_spe[0][0]==\"*\":\n",
    "        if len(line_spe[0])>1:\n",
    "            line_spe[0] = line_spe[0][1:]\n",
    "            return \" \".join(line_spe)\n",
    "        else:\n",
    "            return \" \".join(line_spe[1:])\n",
    "    return \" \".join(line_spe)\n",
    "\n",
    "def get_bullet_points(bullet_points_unfiltered, nb_chunk):\n",
    "    index = 0\n",
    "    bullet_points = []\n",
    "    for i in range(len(bullet_points_unfiltered)):\n",
    "        line = bullet_points_unfiltered[i]\n",
    "        filtered_line = identify(line)\n",
    "        if filtered_line != -1:\n",
    "            bullet_points.append(BulletPoint((nb_chunk, i), filtered_line))\n",
    "        \n",
    "    return bullet_points\n",
    "\n",
    "def compute_metrics(bullet_points):\n",
    "    n = len(bullet_points)\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            predictions.append(bullet_points[i].text)\n",
    "            references.append(bullet_points[j].text)\n",
    "    result_rouge = rouge_score.compute(predictions=predictions, references=references, use_aggregator=False)\n",
    "    result_bertscore = bertscore.compute(predictions=predictions, references=references, lang='en', rescale_with_baseline=True, verbose=True)['f1']\n",
    "\n",
    "    return result_rouge, result_bertscore\n",
    "\n",
    "def twoD_array_to_oneD(result_rouge, result_bertscore, n):\n",
    "    k=0\n",
    "    matrices = {score:np.zeros((n,n)) for score in scores}\n",
    "    values = {score:[] for score in scores}\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            result = result_bertscore[k]\n",
    "            matrices[\"bertscore\"][i][j] = result\n",
    "            matrices[\"bertscore\"][j][i] = result\n",
    "            values[\"bertscore\"].append((i,j,result))\n",
    "            for rouge in rouges:\n",
    "                result = result_rouge[rouge][k]\n",
    "                matrices[rouge][i][j] = result\n",
    "                matrices[rouge][j][i] = result\n",
    "                values[rouge].append((i,j,result))\n",
    "            k+=1\n",
    "\n",
    "    for score in scores:\n",
    "        values[score] = sorted(values[score], key=lambda x:x[2])\n",
    "\n",
    "    return matrices, values\n",
    "\n",
    "def pairs_above_threshold(matrices, n):\n",
    "    pairs = {score:[] for score in scores}\n",
    "    for score in scores:\n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                if matrices[score][i][j] > thresholds[score]:\n",
    "                    pairs[score].append((i,j))\n",
    "    s = 'Nombre de paires au-dessus du seuil: '                \n",
    "    for score in scores:\n",
    "        s+=score + \" : \" + str(len(pairs[score])) + \" --- \"\n",
    "    print(s)\n",
    "\n",
    "\n",
    "    kept_points = {i for i  in range(n)}\n",
    "    for score in scores:\n",
    "        for k in range(len(pairs[score])):\n",
    "            (i, j) = pairs[score][k]\n",
    "            if i in kept_points and j in kept_points:\n",
    "                if random.randint(0,1)==0: # A AFFINER: COMMENT DISCRIMINER LE POINT QUI DOIT DISPARAITRE\n",
    "                    kept_points.remove(i)\n",
    "                else:\n",
    "                    kept_points.remove(j)\n",
    "    return kept_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_preparation_code = \"\"\"dataset_folder_url = \"datasets/KeyPointsExtractionTest/\"\n",
    "dataset_dict = {\"fredsum\":\"0\", \"ami\":\"6\", \"mediasum\":\"3\", \"summre\":5, \"icsi\":7}\n",
    "\n",
    "virgin_files = os.listdir(dataset_folder_url)\n",
    "data = {}\n",
    "for file_name in virgin_files:\n",
    "    if os.path.isdir(file_name): # Shouldn't happen normally\n",
    "        continue\n",
    "    file_code = file_name.split('.')[0].split(\"_\")\n",
    "    data_key = file_code[0] + \"_\" + file_code[1]\n",
    "    dataset = file_code[0]\n",
    "    if data_key not in data:\n",
    "        data[data_key] = [dataset, {}]\n",
    "\n",
    "    if file_code[2][0:3] == \"txt\":\n",
    "        dataset = file_code[0]\n",
    "        original_file_name = file_code[1]\n",
    "        data[data_key][1][\"file_name\"] = original_file_name\n",
    "        data[data_key][1][\"source_dataset\"] = dataset_dict[dataset]\n",
    "\n",
    "        file = open(dataset_folder_url + \"/\" + file_name, 'r', encoding='utf-8')\n",
    "        text = file.read()\n",
    "        file.close()\n",
    "        language_number = str(classify(text))\n",
    "        data[data_key][1][\"language\"] = language_number\n",
    "        language_code = number_to_code[language_number]\n",
    "        data[data_key][1][language_code] = text\n",
    "\n",
    "    elif file_code[2][0:3] == \"sum\":\n",
    "        if \"summaries\" not in data[data_key][1]:\n",
    "            data[data_key][1][\"summaries\"] = []\n",
    "\n",
    "        if len(file_code)==4: # Plusieurs résumés\n",
    "            summary_data = {}\n",
    "            summary_data[\"number\"] = file_code[3]\n",
    "            file = open(dataset_folder_url + \"/\" + file_name, 'r', encoding='utf-8')\n",
    "            text = file.read()\n",
    "            file.close()\n",
    "            summary_data[number_to_code[str(classify(text))]] = text\n",
    "\n",
    "            data[data_key][1][\"summaries\"].append(summary_data)\n",
    "\n",
    "        else: # Un seul résumé\n",
    "            summary_data = {}\n",
    "            summary_data[\"number\"] = \"1\"\n",
    "            file = open(dataset_folder_url + \"/\" + file_name, 'r', encoding='utf-8')\n",
    "            text = file.read()\n",
    "            file.close()\n",
    "            summary_data[number_to_code[str(classify(text))]] = text\n",
    "\n",
    "            data[data_key][1][\"summaries\"].append(summary_data)\n",
    "            \n",
    "\n",
    "\n",
    "for key in data:\n",
    "    dataset = data[key][0]\n",
    "    file_data = data[key][1]\n",
    "    mkdir(dataset_folder_url + dataset)\n",
    "    file_name = str(len(os.listdir(dataset_folder_url + dataset)))\n",
    "    file = open(dataset_folder_url + dataset + \"/\" + file_name, 'w', encoding='utf-8')\n",
    "    json.dump(file_data, file, indent=4, ensure_ascii=False)\n",
    "    file.close()\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These dictionaries have as a key the model name and as the value the function that load the prompt, the tokenizer, the model, give the context length or treat the output.\n",
    "\n",
    "prompt_templates = {} # Dictionnaire qui associe les templates de prompts, il faut insérer l'instruction et le texte à résumer.\n",
    "context_lengths = {} # Dictionnaire qui donne l'entier correspondant à la longueur de contexte\n",
    "tokenizers = {} # Dictionnaire qui associe la méthode pour obtenir le tokenizer du modèle\n",
    "models = {} # Dictionnaire qui associe la méthode pour obtenir le modèle\n",
    "treat_output = {} # Dictionnaire qui associe la méthode pour traiter l'output et ne conserver que la génération du modèle\n",
    "infer = {} # Dictionnaire qui associe la méthode pour l'inférence du modèle. Prend en paramètres l'input et le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model specific functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models available\n",
    "models_name = {\n",
    "    \"legendhasit/xgen-7b-8k-inst-8bit/4bit\", # 1 GPU \n",
    "    \"legendhasit/xgen-7b-8k-inst-8bit\", # 1 GPU\n",
    "    \"Salesforce/xgen-7b-8k-inst\", # 2GPU (3 shards de 10Go peut-être la dernière moins) de préférence, mais fonctionne avec 1... Sauf pour les textes longs, genre les plus petits de Fredsum + probablement mauvaise suppression automatique de la mémoire -> On a des OOM à 38GB\n",
    "    \"mosaicml/mpt-7b-instruct\", # 1 GPU (2 shards de 10 Go)- To quantize\n",
    "    # \"Trelis/mpt-7b-instruct-hosted-inference-8bit\", #  - Potentially unreliable\n",
    "    \"mosaicml/mpt-7b-8k-instruct\", # To quantize\n",
    "    \"mosaicml/mpt-30b-instruct\", # To quantize\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"tiiuae/falcon-7b\"\n",
    "}\n",
    "\n",
    "simplified_names = {\n",
    "    \"legendhasit/xgen-7b-8k-inst-8bit/4bit\":\"XGen7b8k4bit\", # 1 GPU \n",
    "    \"legendhasit/xgen-7b-8k-inst-8bit\":\"XGen7b8k8bit\", # 1 GPU\n",
    "    \"Salesforce/xgen-7b-8k-inst\":\"XGen7b8k\", # 2GPU (3 shards de 10Go peut-être la dernière moins) de préférence, mais fonctionne avec 1... Sauf pour les textes longs, genre les plus petits de Fredsum + probablement mauvaise suppression automatique de la mémoire -> On a des OOM à 38GB\n",
    "    \"mosaicml/mpt-7b-instruct\":\"MPT7b\", # 1 GPU (2 shards de 10 Go)- To quantize\n",
    "    # \"Trelis/mpt-7b-instruct-hosted-inference-8bit\", #  - Potentially unreliable\n",
    "    \"mosaicml/mpt-7b-8k-instruct\":\"MPT7b8k\", # To quantize\n",
    "    \"mosaicml/mpt-30b-instruct\":\"MPT30b\", # To quantize\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\":\"llama27b\",\n",
    "    \"tiiuae/falcon-7b\":\"falcon7b\"\n",
    "}\n",
    "\n",
    "reversed_names = {simplified_names[name]:name for name in simplified_names}\n",
    "# RMQ: Mieux que les dicos de fonction, comme on utilise toujours les mêmes fonctions, mais avec des paramètres différents, il suffit d'avoir le dico des paramètres de la fonction, avec un dico par défaut modifié pour chaque modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt templates\n",
    "prompt_template_XGen = \"A chat between a curious human and an artificial intelligence assistant.\\nThe assistant gives helpful, detailed, and polite answers to the human's questions.\\n\\n### Human: {instruction}\\n\\n### Assistant: \"\n",
    "\n",
    "prompt_templates[\"legendhasit/xgen-7b-8k-inst-8bit\"] = prompt_template_XGen\n",
    "prompt_templates[\"legendhasit/xgen-7b-8k-inst-8bit/4bit\"] = prompt_template_XGen\n",
    "prompt_templates[\"Salesforce/xgen-7b-8k-inst\"] = prompt_template_XGen\n",
    "\n",
    "# Context lengths # Allow to know whether the model generated out of his context window\n",
    "context_length_XGen = 8000 \n",
    "\n",
    "context_lengths[\"legendhasit/xgen-7b-8k-inst-8bit\"] = context_length_XGen\n",
    "context_lengths[\"legendhasit/xgen-7b-8k-inst-8bit/4bit\"] = context_length_XGen\n",
    "context_lengths[\"Salesforce/xgen-7b-8k-inst\"] = context_length_XGen\n",
    "\n",
    "# Tokenizers\n",
    "def get_tokenizer_XGen8bit(): # Le même pour le 4bit\n",
    "    return AutoTokenizer.from_pretrained('legendhasit/xgen-7b-8k-inst-8bit', trust_remote_code=True)\n",
    "\n",
    "def get_tokenizer_XGen():\n",
    "    return AutoTokenizer.from_pretrained('Salesforce/xgen-7b-8k-inst', trust_remote_code=True)\n",
    "\n",
    "tokenizers[\"legendhasit/xgen-7b-8k-inst-8bit\"] = get_tokenizer_XGen8bit\n",
    "tokenizers[\"legendhasit/xgen-7b-8k-inst-8bit/4bit\"] = get_tokenizer_XGen8bit\n",
    "tokenizers[\"Salesforce/xgen-7b-8k-inst\"] = get_tokenizer_XGen\n",
    "\n",
    "# Models\n",
    "\n",
    "def get_model_XGen8bit():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    'legendhasit/xgen-7b-8k-inst-8bit',\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_model_XGen4bit():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    'legendhasit/xgen-7b-8k-inst-8bit',\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_model_XGen():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    'Salesforce/xgen-7b-8k-inst',\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "    return model\n",
    "\n",
    "models[\"legendhasit/xgen-7b-8k-inst-8bit\"] = get_model_XGen8bit\n",
    "models[\"legendhasit/xgen-7b-8k-inst-8bit/4bit\"] = get_model_XGen4bit\n",
    "models[\"Salesforce/xgen-7b-8k-inst\"] = get_model_XGen\n",
    "\n",
    "# Tokenize (if specificities in the way models call the tokenizer)\n",
    "\n",
    "# Inference (if specificities in the way models call the generate function)\n",
    "\n",
    "def infer_XGen(tokenized_input, model):\n",
    "    return model.generate(**tokenized_input, do_sample=True, max_new_tokens=max_new_tokens, top_k=top_k, eos_token_id=50256, temperature=temperature) # eos_token_id : id du token de fin de réponse d'XGen dans son vocabulaire.\n",
    "\n",
    "infer['legendhasit/xgen-7b-8k-inst-8bit'] = infer_XGen\n",
    "infer['legendhasit/xgen-7b-8k-inst-8bit/4bit'] = infer_XGen\n",
    "infer['Salesforce/xgen-7b-8k-inst'] = infer_XGen\n",
    "\n",
    "# Treating the output (to remove the input if present in the output as well as the end of text token for example)\n",
    "\n",
    "def treat_output_XGen(output): # Differs if it is N-shot. Here, it is 0-shot\n",
    "    occ_1 = output.find(\"### Assistant: \")\n",
    "    output = output[occ_1+15:]\n",
    "    if output.find('<|endoftext|>')!=-1:\n",
    "        output = output[:-14]\n",
    "    return output\n",
    "\n",
    "treat_output['legendhasit/xgen-7b-8k-inst-8bit'] = treat_output_XGen\n",
    "treat_output['legendhasit/xgen-7b-8k-inst-8bit/4bit'] = treat_output_XGen\n",
    "treat_output['Salesforce/xgen-7b-8k-inst'] = treat_output_XGen\n",
    "\n",
    "# /workspace/.miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1411: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
    "# Avec XGen de salesforce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt templates\n",
    "prompt_template_MPT7B = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{instruction}\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template_MPT30B = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n###Instruction\\n{instruction}\\n\\n### Response\\n\" # On considère que c'est le même template que pour MPT 7B 8k car pour ce dernier, le template utilisé n'est pas précisé\n",
    "\n",
    "prompt_templates[\"mosaicml/mpt-7b-instruct\"] = prompt_template_MPT7B\n",
    "prompt_templates[\"Trelis/mpt-7b-instruct-hosted-inference-8bit\"] = prompt_template_MPT7B\n",
    "prompt_templates[\"mosaicml/mpt-7b-8k-instruct\"] = prompt_template_MPT30B\n",
    "prompt_templates[\"mosaicml/mpt-30b-instruct\"] = prompt_template_MPT30B\n",
    "\n",
    "# Context lengths\n",
    "\n",
    "context_lengths[\"mosaicml/mpt-7b-instruct\"] = 2048 # 4096 d'après mosaicml\n",
    "\n",
    "# Models\n",
    "def get_model_MPT7B():\n",
    "    config = AutoConfig.from_pretrained(\"mosaicml/mpt-7b-instruct\", trust_remote_code=True)\n",
    "    config.max_seq_len = 4096 # (input + output) tokens can now be up to 4096\n",
    "    return AutoModelForCausalLM.from_pretrained(\"mosaicml/mpt-7b-instruct\",config=config,trust_remote_code=True,device_map = \"auto\")\n",
    "\n",
    "models[\"mosaicml/mpt-7b-instruct\"] = get_model_MPT7B\n",
    "\n",
    "# Tokenize\n",
    "def get_tokenizer_MPT7B():\n",
    "    return AutoTokenizer.from_pretrained(\"mosaicml/mpt-7b-instruct\", trust_remote_code=True)\n",
    "\n",
    "tokenizers[\"mosaicml/mpt-7b-instruct\"] = get_tokenizer_MPT7B\n",
    "# Inference ######################################\n",
    "\n",
    "def infer_MPT7B(tokenized_input, model):\n",
    "    return model.generate(**tokenized_input, do_sample=True, max_new_tokens=max_new_tokens, top_k=top_k, temperature=temperature, eos_token_id=0) # eos_token_id : id du token de fin de réponse d'XGen dans son vocabulaire. repetition_penalty=1.2 : évitd la répéttioon\n",
    "\n",
    "infer[\"mosaicml/mpt-7b-instruct\"] = infer_MPT7B\n",
    "\n",
    "# Treating the output ######################################\n",
    "\n",
    "def treat_output_MPT(output): # Differs if it is N-shot. Here, it is 0-shot\n",
    "    occ_1 = output.find(\"### Response:\\n\")\n",
    "    output = output[occ_1+14:]\n",
    "    if output.find('<|endoftext|>')!=-1:\n",
    "        output = output[:-14]\n",
    "    return output\n",
    "\n",
    "treat_output[\"mosaicml/mpt-7b-instruct\"] = treat_output_MPT\n",
    "treat_output[\"Trelis/mpt-7b-instruct-hosted-inference-8bit\"] = treat_output_MPT\n",
    "treat_output[\"mosaicml/mpt-7b-8k-instruct\"] = treat_output_MPT\n",
    "treat_output[\"mosaicml/mpt-30b-instruct\"] = treat_output_MPT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Llama 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt templates\n",
    "prompt_templates[\"meta-llama/Llama-2-7b-chat-hf\"] = \"\"\"<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "\n",
    "{instruction} [/INST]\"\"\"\n",
    "\n",
    "context_lengths[\"meta-llama/Llama-2-7b-chat-hf\"] = 4096\n",
    "\n",
    "def get_tokenizer_llama2_7b():\n",
    "    return AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", trust_remote_code=True)\n",
    "\n",
    "tokenizers[\"meta-llama/Llama-2-7b-chat-hf\"] = get_tokenizer_llama2_7b\n",
    "\n",
    "def get_model_llama2_7b():\n",
    "    return AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",trust_remote_code=True,device_map = \"auto\")\n",
    "\n",
    "models[\"meta-llama/Llama-2-7b-chat-hf\"] = get_model_llama2_7b\n",
    "\n",
    "def infer_llama2_7b(tokenized_input, model):\n",
    "    return model.generate(**tokenized_input, do_sample=True, max_new_tokens=max_new_tokens, top_k=top_k, temperature=temperature, eos_token_id=2) # tokenizer.eos_token_id)\n",
    "\n",
    "infer[\"meta-llama/Llama-2-7b-chat-hf\"] = infer_llama2_7b\n",
    "\n",
    "def treat_output_llama2_7b(output): # Differs if it is N-shot. Here, it is 0-shot\n",
    "    occ_1 = output.find(\"[/INST]\")\n",
    "    output = output[occ_1+9:] # +7 + les 2 espaces avant que le modèle ne parle\n",
    "    if output.find('</s>')!=-1:\n",
    "        output = output[:-4]\n",
    "    return output\n",
    "\n",
    "treat_output[\"meta-llama/Llama-2-7b-chat-hf\"] = treat_output_llama2_7b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Falcon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_templates[\"tiiuae/falcon-7b\"] = \"\"\"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\n",
    ">>QUESTION<<{instruction}\n",
    ">>ANSWER<<\"\"\"\n",
    "\n",
    "context_lengths[\"tiiuae/falcon-7b\"] = 2048\n",
    "\n",
    "def get_tokenizer_falcon7b():\n",
    "    return AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\", trust_remote_code=True)\n",
    "\n",
    "tokenizers[\"tiiuae/falcon-7b\"] = get_tokenizer_falcon7b\n",
    "\n",
    "def get_model_falcon7b():\n",
    "    return AutoModelForCausalLM.from_pretrained(\n",
    "    \"tiiuae/falcon-7b\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "models[\"tiiuae/falcon-7b\"] = get_model_falcon7b\n",
    "\n",
    "def infer_falcon7b(tokenized_input, model):\n",
    "    return model.generate(**tokenized_input, do_sample=True, max_new_tokens=max_new_tokens, top_k=top_k, temperature=temperature, eos_token_id=11)\n",
    "\n",
    "infer[\"tiiuae/falcon-7b\"] = infer_falcon7b\n",
    "\n",
    "def treat_output_falcon7b(output):\n",
    "    occ_1 = output.find('>>ANSWER<<')\n",
    "    output = output[occ_1 + len('>>ANSWER<<'):]\n",
    "    if output.find('<|endoftext|>')!=-1:\n",
    "        output = output[:-len('<|endoftext|>')]\n",
    "    return output\n",
    "\n",
    "treat_output[\"tiiuae/falcon-7b\"] = treat_output_falcon7b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing the input dataset\n",
    "Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mosaicml/mpt-7b-instruct --- ami --- 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m new_file_name \u001b[39m=\u001b[39m corpus \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name_file\n\u001b[0;32m     22\u001b[0m new_data[\u001b[39m\"\u001b[39m\u001b[39msummaries\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39m\u001b[39msummaries\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m---> 23\u001b[0m en_chunks \u001b[39m=\u001b[39m chunkize(data[\u001b[39m\"\u001b[39;49m\u001b[39mtext_en\u001b[39;49m\u001b[39m\"\u001b[39;49m], tokenizer)\n\u001b[0;32m     24\u001b[0m \u001b[39m#fr_chunks = chunkize(data[\"text_fr\"], tokenizer)\u001b[39;00m\n\u001b[0;32m     25\u001b[0m new_data[\u001b[39m\"\u001b[39m\u001b[39men_chunks\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[26], line 121\u001b[0m, in \u001b[0;36mchunkize\u001b[1;34m(text, tokenizer)\u001b[0m\n\u001b[0;32m    118\u001b[0m     current_chunk \u001b[39m=\u001b[39m splits[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    120\u001b[0m \u001b[39m# Add to current chunk and proceed to next\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m \u001b[39melif\u001b[39;00m token_len(new_current_chunk, tokenizer) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m MAX_TOKEN_CHUNK_SIZE:\n\u001b[0;32m    122\u001b[0m     current_chunk \u001b[39m=\u001b[39m new_current_chunk\n\u001b[0;32m    124\u001b[0m \u001b[39m# Current chunk is big enough, append to list and create new one\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[26], line 83\u001b[0m, in \u001b[0;36mtoken_len\u001b[1;34m(text, tokenizer)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtoken_len\u001b[39m(text, tokenizer):\n\u001b[1;32m---> 83\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(tokenizer(text, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\rayci\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2602\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2600\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2601\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2602\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_one(text\u001b[39m=\u001b[39mtext, text_pair\u001b[39m=\u001b[39mtext_pair, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mall_kwargs)\n\u001b[0;32m   2603\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2604\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Users\\rayci\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2708\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2688\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   2689\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2690\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2705\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2706\u001b[0m     )\n\u001b[0;32m   2707\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2708\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[0;32m   2709\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m   2710\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   2711\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2712\u001b[0m         padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2713\u001b[0m         truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   2714\u001b[0m         max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   2715\u001b[0m         stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   2716\u001b[0m         is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2717\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2718\u001b[0m         return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2719\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2720\u001b[0m         return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2721\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2722\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2723\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2724\u001b[0m         return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   2725\u001b[0m         verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   2726\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2727\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\rayci\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2781\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2771\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   2772\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   2773\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2774\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2778\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2779\u001b[0m )\n\u001b[1;32m-> 2781\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encode_plus(\n\u001b[0;32m   2782\u001b[0m     text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m   2783\u001b[0m     text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   2784\u001b[0m     add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2785\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[0;32m   2786\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   2787\u001b[0m     max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   2788\u001b[0m     stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   2789\u001b[0m     is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2790\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2791\u001b[0m     return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2792\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2793\u001b[0m     return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2794\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2795\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2796\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2797\u001b[0m     return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   2798\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   2799\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2800\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\rayci\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:517\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_encode_plus\u001b[39m(\n\u001b[0;32m    496\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    497\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    514\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    515\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchEncoding:\n\u001b[0;32m    516\u001b[0m     batched_input \u001b[39m=\u001b[39m [(text, text_pair)] \u001b[39mif\u001b[39;00m text_pair \u001b[39melse\u001b[39;00m [text]\n\u001b[1;32m--> 517\u001b[0m     batched_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_encode_plus(\n\u001b[0;32m    518\u001b[0m         batched_input,\n\u001b[0;32m    519\u001b[0m         is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m    520\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m    521\u001b[0m         padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[0;32m    522\u001b[0m         truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m    523\u001b[0m         max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m    524\u001b[0m         stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m    525\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m    526\u001b[0m         return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m    527\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m    528\u001b[0m         return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m    529\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m    530\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m    531\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m    532\u001b[0m         return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m    533\u001b[0m         verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m    534\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    535\u001b[0m     )\n\u001b[0;32m    537\u001b[0m     \u001b[39m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[0;32m    538\u001b[0m     \u001b[39m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[0;32m    539\u001b[0m     \u001b[39mif\u001b[39;00m return_tensors \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[1;32mc:\\Users\\rayci\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:445\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[39m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_truncation_and_padding(\n\u001b[0;32m    438\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[0;32m    439\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    442\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m    443\u001b[0m )\n\u001b[1;32m--> 445\u001b[0m encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\u001b[39m.\u001b[39;49mencode_batch(\n\u001b[0;32m    446\u001b[0m     batch_text_or_text_pairs,\n\u001b[0;32m    447\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[0;32m    448\u001b[0m     is_pretokenized\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[0;32m    449\u001b[0m )\n\u001b[0;32m    451\u001b[0m \u001b[39m# Convert encoding to dict\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[39m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[39m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[39m#                       List[EncodingFast]\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[39m#                    ]\u001b[39;00m\n\u001b[0;32m    456\u001b[0m \u001b[39m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[0;32m    457\u001b[0m tokens_and_encodings \u001b[39m=\u001b[39m [\n\u001b[0;32m    458\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_encoding(\n\u001b[0;32m    459\u001b[0m         encoding\u001b[39m=\u001b[39mencoding,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    468\u001b[0m     \u001b[39mfor\u001b[39;00m encoding \u001b[39min\u001b[39;00m encodings\n\u001b[0;32m    469\u001b[0m ]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "inputs_folder = \"0\"\n",
    "nb_to_tokenizer = {}\n",
    "\n",
    "# On peut imaginer une première passe qui découpe les inputs et forme les instructions, rangés dans des fichiers .json, dans un dossier instructions. Il faut utiliser le bon tokenizer -> Process à répéter pour chaque modèle. instructions/<tokenizer_name>/...\n",
    "\n",
    "dataset_folder = \"datasets/KeyPointsExtractionTest\"\n",
    "mkdir('inputs')\n",
    "for model_name in used_models_name:\n",
    "    tokenizer = tokenizers[model_name]()\n",
    "\n",
    "    for corpus in os.listdir(dataset_folder):\n",
    "        for name_file in os.listdir(dataset_folder + \"/\" + corpus):\n",
    "            print(model_name + \" --- \" + corpus + \" --- \" + name_file)\n",
    "            file = open(dataset_folder + \"/\" + corpus + \"/\" + name_file, 'r', encoding='utf-8')\n",
    "            data = json.load(file)\n",
    "            file.close()\n",
    "        \n",
    "            new_data = {}\n",
    "            new_data[\"tokenizer\"] = model_name\n",
    "            new_data[\"original_path\"] = corpus + \"/\" + name_file\n",
    "            new_file_name = corpus + \"_\" + name_file\n",
    "            new_data[\"summaries\"] = data[\"summaries\"]\n",
    "            en_chunks = chunkize(data[\"text_en\"], tokenizer)\n",
    "            #fr_chunks = chunkize(data[\"text_fr\"], tokenizer)\n",
    "            new_data[\"en_chunks\"] = []\n",
    "            #new_data[\"fr_chunks\"] = []\n",
    "            new_data[\"MAX_TOKEN_CHUNK_SIZE\"] = str(MAX_TOKEN_CHUNK_SIZE)\n",
    "\n",
    "            for i in range(len(en_chunks)):\n",
    "                chunk = {\"text_en\":en_chunks[i], \"nb_words_en\":str(count_words(en_chunks[i])), \"nb_characters_en\":str(len(en_chunks[i]))}\n",
    "                new_data[\"en_chunks\"].append(chunk)\n",
    "\n",
    "            #for i in range(len(fr_chunks)):\n",
    "            #    chunk = {\"text_fr\":fr_chunks[i], \"nb_words_fr\":str(count_words(fr_chunks[i])), \"nb_characters_fr\":str(len(fr_chunks[i]))}\n",
    "            #    new_data[\"fr_chunks\"].append(chunk)\n",
    "\n",
    "            mkdir(\"inputs/\" + simplified_names[model_name] + \"_\" + str(MAX_TOKEN_CHUNK_SIZE))\n",
    "            mkdir(\"inputs/\" + simplified_names[model_name] + \"_\" + str(MAX_TOKEN_CHUNK_SIZE) + \"/\" + corpus)\n",
    "            new_file = open(\"inputs/\" + simplified_names[model_name] + \"_\" + str(MAX_TOKEN_CHUNK_SIZE) + \"/\" + corpus + \"/\" + new_file_name, 'w', encoding='utf-8')\n",
    "            json.dump(new_data, new_file, ensure_ascii=False, indent=4)\n",
    "            new_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'legendhasit/xgen-7b-8k-inst-8bit': <function get_tokenizer_XGen8bit at 0x000001799D8FA5F0>, 'legendhasit/xgen-7b-8k-inst-8bit/4bit': <function get_tokenizer_XGen8bit at 0x000001799D8FA5F0>, 'Salesforce/xgen-7b-8k-inst': <function get_tokenizer_XGen at 0x000001799D8F91B0>, 'mosaicml/mpt-7b-instruct': <function get_tokenizer_MPT7B at 0x000001799C35C9D0>, 'meta-llama/Llama-2-7b-chat-hf': <function get_tokenizer_llama2_7b at 0x000001799D8FAD40>, 'tiiuae/falcon-7b': <function get_tokenizer_falcon7b at 0x000001799D8FAA70>}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every time the script is casted, it must register the results in the \"results\" folder, without smashing the existing results\n",
    "# Then, it is stored in a file whose name is the number of the result, and this file contains a little .txt note describing what was the experiment.\n",
    "# In such a folder, all the generations are json files with additional information like the prompt used, the number of samples, the path of the input text\n",
    "\n",
    "# Now, what is specific to the model ? The prompt (different headers potentially), the tokenizer, the model itself, the way the output is displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir(\"results\")\n",
    "\n",
    "archives = os.listdir(\"results\")\n",
    "output_folder_path = \"results/\"+str(len(archives))\n",
    "mkdir(output_folder_path) # Create a folder for the last results.\n",
    "desc_file = open(output_folder_path + \"/desc.txt\", 'w', encoding='utf-8')\n",
    "desc_file.write(desc)\n",
    "desc_file.close()\n",
    "\n",
    "initial_time = time.time()\n",
    "\n",
    "model_total = len(used_models_name)\n",
    "model_index = 0\n",
    "for model_name in used_models_name: # For each model\n",
    "    model_index += 1\n",
    "    print(\"---- Model : \" + model_name + \" (\" + str(model_index) + \"/\" + str(model_total) +\")----              (loading tokenizer and model...)\")\n",
    "    load_model_time = time.time()\n",
    "    prompt_template = prompt_templates[model_name]\n",
    "    tokenizer = tokenizers[model_name]()\n",
    "    model = models[model_name]()\n",
    "    load_model_time = time.time() - load_model_time\n",
    "    !nvidia-smi\n",
    "    print(\"Tokenizer and model loaded in\", datetime.timedelta(seconds=int(load_model_time)), 'seconds')\n",
    "    inputs_folder_path = \"inputs/\" + simplified_names[model_name] + \"_\" + str(MAX_TOKEN_CHUNK_SIZE)\n",
    "    corpora = os.listdir(inputs_folder_path)\n",
    "    for corpus in corpora:\n",
    "        !nvidia-smi\n",
    "        file_index = 0\n",
    "        files_name = os.listdir(inputs_folder_path + \"/\" + corpus)\n",
    "        file_total = len(files_name)\n",
    "        for file_name in files_name:\n",
    "            file_path = inputs_folder_path + \"/\" + corpus + \"/\" + file_name\n",
    "            if file_name==\"ipynb_checkpoints\" or os.path.isdir(file_path):\n",
    "                continue\n",
    "            file_time = time.time()\n",
    "            file_index+=1\n",
    "            print(\"Starting inference for text \" + str(file_index) + \"/\" + str(file_total) + \" in the \" + corpus + \" corpus.\") # Possible errors in the display: file_total also counts the .ipynb checkpoints potentially in the directory.\n",
    "            file = open(file_path, 'r', encoding='utf-8')\n",
    "            input_data = json.load(file)\n",
    "            file.close()\n",
    "            for text_key in text_keys:\n",
    "                en_or_fr = text_key[-2:]\n",
    "                chunks = input_data[en_or_fr + \"_chunks\"]\n",
    "                output_data = {\"chunks\":[]}\n",
    "                output_data[\"input_path\"] = file_path\n",
    "                output_data[\"model\"] = model_name\n",
    "                output_data[\"input_language\"] = text_key # NE GERE PAS ENCORE BIEN FRANCAIS\n",
    "                output_data[\"MAX_TOKEN_CHUNK_SIZE\"] = input_data[\"MAX_TOKEN_CHUNK_SIZE\"]\n",
    "                # RAJOUTER UN AFFICHAGE pour toutes les chunks\n",
    "                precedent_text = \"\"\n",
    "                for i in range(len(chunks) + 1): # 1 fois le 1er chunk, puis une fois 1er et 2eme, puis une fois 2eme et 3eme etc. puis une fois avant dernier dernier puis une fois dernier\n",
    "                    print(\"Chunk \" + str(i) + \"/\" + str(len(chunks)) + \" ...\")\n",
    "                    if i==len(chunks):\n",
    "                        chunk = chunks[-1]\n",
    "                        precedent_text = \"\"\n",
    "                    else: chunk = chunks[i]\n",
    "                    chunk_data = {}\n",
    "                    input_text = precedent_text + chunk[text_key]\n",
    "                    precedent_text = chunk[text_key]\n",
    "                    instruction_template = instruction_templates[text_key]\n",
    "                    full_instruction = instruction_template.format(text=input_text)\n",
    "                    prompt = prompt_template.format(instruction=full_instruction)\n",
    "\n",
    "                    # Probably specific to the model\n",
    "                    input=0\n",
    "                    if model_name==\"tiiuae/falcon-7b\": input = tokenizer(prompt, return_token_type_ids=False, return_tensors=\"pt\").to('cuda')# A AMELIORER: Il faut un nouveau dico\n",
    "                    else: input = tokenizer(prompt, return_tensors=\"pt\").to('cuda') # Le renvoie sur le GPU car au départ, c'est généré en CPU le tensuer des tokens\n",
    "\n",
    "                    input_length = len(input['input_ids'][0])\n",
    "                    output_name = file_name + \"_\" + text_key + \".json\"\n",
    "                    output_path = output_folder_path + \"/\" + output_name\n",
    "                    if \"instruction\" not in output_data: output_data[\"instruction\"] = instruction_template.format(text=\"\")\n",
    "                    chunk_data[\"success\"] = \"0\"\n",
    "                    chunk_data[\"input_length\"] = str(input_length)\n",
    "\n",
    "                    if input_length > token_limit: # In this case, a file is created but no inference is made\n",
    "                        print(\"/!\\ With model \" + model_name + \", the input length is above the token limit for \" + text_key + \" input in \" + file_path + \" (\" + str(input_length) + \" > \" + str(token_limit) + \")\")\n",
    "\n",
    "                    else: # In this case, the output is inferred, treated to keep only the generation and the length information\n",
    "                        # PROBABLY SPECIFIC TO THE MODEL\n",
    "                        sample = infer[model_name](input, model)\n",
    "                        sample_length = len(sample[0])\n",
    "                        ### SPECIFIC TO THE MODEL ----------------------------------------\n",
    "                        if sample_length > context_lengths[model_name]: # There was a context window overflow\n",
    "                            output_data[\"over_context\"] = \"1\"\n",
    "                        else:\n",
    "                            output_data[\"over_context\"] = \"0\"\n",
    "                        output_length = sample_length - input_length\n",
    "                        output_data[\"output_length\"] = output_length\n",
    "                        full_output = tokenizer.decode(sample[0]).strip() # prompt + output generation\n",
    "                        # Ideally, treat_output is a dictionnary that associates model_name to a function taking full_output as an argument\n",
    "                        output = treat_output[model_name](full_output)\n",
    "                        chunk_data[\"text\"] = output\n",
    "                        chunk_data[\"nb_characters\"] = len(output)\n",
    "                        chunk_data[\"nb_words\"] = count_words(output)\n",
    "                        chunk_data[\"success\"] = \"1\"\n",
    "                        chunk_data[\"language_output\"] = str(classify(output))\n",
    "                    output_data[\"chunks\"].append(chunk_data)\n",
    "                    \n",
    "                # SAVING THE OUTPUT\n",
    "                output_file = open(output_path, 'w', encoding='utf-8')\n",
    "                json.dump(output_data, output_file, indent=4, ensure_ascii=False)\n",
    "                output_file.close()\n",
    "            file_time = time.time() - file_time\n",
    "            print(\"Inference done for French and English in\", datetime.timedelta(seconds=int(file_time)), \"seconds.\")\n",
    "    # HERE, clear memory... remove model, tokenizer from memory\n",
    "                \n",
    "delta = time.time() - initial_time\n",
    "print('Done! Took', datetime.timedelta(seconds=int(delta)), 'seconds')\n",
    "# Write treatOutput function / dictionnary\n",
    "\n",
    "\n",
    "# RETIRER le fichier template dans samsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bullet points extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier ami_ami_0_text_en.json :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 62.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 412868.91 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 10 --- rouge1 : 24 --- bertscore : 19 --- rouge2 : 2 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 59.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 412888.76 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 3 --- rouge1 : 9 --- bertscore : 11 --- rouge2 : 0 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 63.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 412878.91 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 3 --- rouge1 : 8 --- bertscore : 4 --- rouge2 : 0 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 81.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 412868.06 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 4 --- rouge1 : 11 --- bertscore : 5 --- rouge2 : 2 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 88.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 412856.54 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 7 --- rouge1 : 13 --- bertscore : 16 --- rouge2 : 3 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 85.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 412879.47 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 5 --- rouge1 : 14 --- bertscore : 15 --- rouge2 : 0 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 73.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 412929.33 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 1 --- rouge1 : 7 --- bertscore : 3 --- rouge2 : 0 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 76.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 412920.19 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 2 --- rouge1 : 7 --- bertscore : 5 --- rouge2 : 2 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 83.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 412925.32 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 6 --- rouge1 : 12 --- bertscore : 8 --- rouge2 : 1 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 79.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 412899.48 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 12 --- rouge1 : 21 --- bertscore : 12 --- rouge2 : 3 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 85.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 412905.66 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 4 --- rouge1 : 11 --- bertscore : 5 --- rouge2 : 1 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 57.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 412927.75 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 5 --- rouge1 : 12 --- bertscore : 6 --- rouge2 : 1 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 74.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 412917.27 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 5 --- rouge1 : 9 --- bertscore : 9 --- rouge2 : 1 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 66.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 412953.56 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 1 --- rouge1 : 5 --- bertscore : 4 --- rouge2 : 0 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 72.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 412911.30 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 5 --- rouge1 : 10 --- bertscore : 7 --- rouge2 : 2 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 82.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 412859.56 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 6 --- rouge1 : 15 --- bertscore : 7 --- rouge2 : 3 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 95.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 413027.37 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 3 --- rouge1 : 6 --- bertscore : 3 --- rouge2 : 1 --- \n",
      "Fichier ami_ami_1_text_en.json :\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 49.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 412909.16 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 3 --- rouge1 : 9 --- bertscore : 3 --- rouge2 : 1 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 42.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 412966.70 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 5 --- rouge1 : 14 --- bertscore : 10 --- rouge2 : 1 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 52.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 412902.80 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 5 --- rouge1 : 19 --- bertscore : 19 --- rouge2 : 1 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 45.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 412909.41 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 8 --- rouge1 : 18 --- bertscore : 13 --- rouge2 : 2 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 50.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 412934.67 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 10 --- rouge1 : 30 --- bertscore : 12 --- rouge2 : 3 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 60.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 412939.94 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 11 --- rouge1 : 23 --- bertscore : 15 --- rouge2 : 2 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 62.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 412925.90 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 28 --- rouge1 : 39 --- bertscore : 27 --- rouge2 : 8 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 52.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 412930.99 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 38 --- rouge1 : 56 --- bertscore : 40 --- rouge2 : 12 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 72.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 412990.27 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 9 --- rouge1 : 17 --- bertscore : 21 --- rouge2 : 2 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 99.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 413052.39 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 1 --- rouge1 : 6 --- bertscore : 3 --- rouge2 : 0 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 101.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 413056.53 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 4 --- rouge1 : 10 --- bertscore : 3 --- rouge2 : 1 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 86.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 413047.92 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 8 --- rouge1 : 13 --- bertscore : 6 --- rouge2 : 2 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 85.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 413052.17 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 5 --- rouge1 : 9 --- bertscore : 7 --- rouge2 : 1 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 67.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 413058.03 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 5 --- rouge1 : 13 --- bertscore : 5 --- rouge2 : 0 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 60.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 413019.67 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 6 --- rouge1 : 16 --- bertscore : 5 --- rouge2 : 1 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 80.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 413025.63 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 8 --- rouge1 : 17 --- bertscore : 10 --- rouge2 : 2 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 107.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 413143.04 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 2 --- rouge1 : 3 --- bertscore : 2 --- rouge2 : 1 --- \n",
      "Fichier ami_ami_2_text_en.json :\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 65.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 413017.21 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 3 --- rouge1 : 6 --- bertscore : 3 --- rouge2 : 0 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 62.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 413005.68 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 7 --- rouge1 : 12 --- bertscore : 6 --- rouge2 : 2 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 85.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 412971.77 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 17 --- rouge1 : 30 --- bertscore : 18 --- rouge2 : 4 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 61.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 412753.98 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 8 --- rouge1 : 18 --- bertscore : 4 --- rouge2 : 0 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 62.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 412818.46 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 4 --- rouge1 : 7 --- bertscore : 4 --- rouge2 : 1 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  4.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 73.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 413048.70 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 11 --- rouge1 : 21 --- bertscore : 19 --- rouge2 : 2 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 69.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 413017.32 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 15 --- rouge1 : 37 --- bertscore : 33 --- rouge2 : 0 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 73.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 413022.89 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 7 --- rouge1 : 19 --- bertscore : 18 --- rouge2 : 2 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 76.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 413027.77 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 15 --- rouge1 : 33 --- bertscore : 23 --- rouge2 : 6 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 67.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 413033.86 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 17 --- rouge1 : 42 --- bertscore : 29 --- rouge2 : 6 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 64.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 413058.68 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 7 --- rouge1 : 17 --- bertscore : 4 --- rouge2 : 2 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 73.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 413114.35 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 3 --- rouge1 : 3 --- bertscore : 3 --- rouge2 : 1 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 81.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 413103.24 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 1 --- rouge1 : 5 --- bertscore : 2 --- rouge2 : 0 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 80.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 413073.17 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 5 --- rouge1 : 17 --- bertscore : 7 --- rouge2 : 1 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 78.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 413128.51 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 2 --- rouge1 : 7 --- bertscore : 3 --- rouge2 : 0 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 76.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 413117.88 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 3 --- rouge1 : 11 --- bertscore : 10 --- rouge2 : 1 --- \n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 81.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 413123.37 seconds, 0.00 sentences/sec\n",
      "Nombre de paires au-dessus du seuil: rougeL : 5 --- rouge1 : 10 --- bertscore : 7 --- rouge2 : 0 --- \n",
      "Fichier desc.txt :\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m key_points \u001b[39m=\u001b[39m []\n\u001b[0;32m     26\u001b[0m file \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mresults/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m nb_batch \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name_file, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mload(file)\n\u001b[0;32m     28\u001b[0m file\u001b[39m.\u001b[39mclose()\n\u001b[0;32m     29\u001b[0m chunks \u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39m\u001b[39mchunks\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\rayci\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(fp, \u001b[39m*\u001b[39m, \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_float\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_constant\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_pairs_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[39m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[39m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[39mreturn\u001b[39;00m loads(fp\u001b[39m.\u001b[39mread(),\n\u001b[0;32m    294\u001b[0m         \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mcls\u001b[39m, object_hook\u001b[39m=\u001b[39mobject_hook,\n\u001b[0;32m    295\u001b[0m         parse_float\u001b[39m=\u001b[39mparse_float, parse_int\u001b[39m=\u001b[39mparse_int,\n\u001b[0;32m    296\u001b[0m         parse_constant\u001b[39m=\u001b[39mparse_constant, object_pairs_hook\u001b[39m=\u001b[39mobject_pairs_hook, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\rayci\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\Users\\rayci\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[0;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[1;32mc:\\Users\\rayci\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "nb_batch = \"0\"\n",
    "\n",
    "nb_to_dataset = {\"0\":\"fredsum\", \"3\":\"mediasum\", \"5\":\"summre\", \"6\":\"ami\", \"7\":\"icsi\"}\n",
    "scores = {\"rouge1\", \"rouge2\", \"rougeL\", \"bertscore\"}\n",
    "rouges = {\"rouge1\", \"rouge2\", \"rougeL\"}\n",
    "# Input: [text_output for text_output in texts_output] -> [[point1, point2, point3, point4], []] -> matrice distance -> Virer les doublons\n",
    "\n",
    "# [1 0 1 1 0 1 1 0 0 0 0 2 1 0 0]\n",
    "\n",
    "def treat_bullet_points(bullet_points):\n",
    "    n = len(bullet_points)\n",
    "    result_rouge, result_bertscore = compute_metrics(bullet_points)\n",
    "    matrices, values = twoD_array_to_oneD(result_rouge, result_bertscore, n)\n",
    "    return pairs_above_threshold(matrices, n)\n",
    "\n",
    "rouge_score = evaluate.load('rouge') # Warning ! When comparing many references to a prediction, an average is performed, not a max...\n",
    "bertscore = evaluate.load('bertscore') # The BERTScore calculation will take a lot of time... (if not performed in only one call)\n",
    "\n",
    "# Extraction des résultats ==================================\n",
    "results = {} # ce dictionnaire conserve les résultats tout en gardant INUTILE\n",
    "\n",
    "mkdir(\"treated\")\n",
    "for name_file in os.listdir(\"results/\" + nb_batch):\n",
    "    print(\"Fichier \" + name_file + \" :\")\n",
    "    key_points = []\n",
    "    file = open(\"results/\" + nb_batch + \"/\" + name_file, 'r', encoding='utf-8')\n",
    "    data = json.load(file)\n",
    "    file.close()\n",
    "    chunks = data[\"chunks\"]\n",
    "    string_output = \"\"\n",
    "    for i in range(len(chunks)-1): # Traitement en mettant en commun deux listes de key points pour des chunks qui se suivent\n",
    "        chunk1 = chunks[i]\n",
    "        chunk2 = chunks[i+1]\n",
    "        raw_text1 = chunk1[\"text\"]\n",
    "        raw_text2 = chunk2[\"text\"]\n",
    "        bullet_points_unfiltered1 = raw_text1.split(\"\\n\")\n",
    "        bullet_points_unfiltered2 = raw_text2.split(\"\\n\")\n",
    "        index = 0\n",
    "        bullet_points = get_bullet_points(bullet_points_unfiltered1, i) + get_bullet_points(bullet_points_unfiltered2, i)\n",
    "        kept_points = treat_bullet_points(bullet_points)\n",
    "        \n",
    "        kept_bullets = [bullet_points[i].text for i in kept_points]\n",
    "        string_output += '\\n'.join(kept_bullets) + \"\\n\\n\"\n",
    "    file_output = open(\"treated/\" + str(len(os.listdir(\"treated\"))), 'w', encoding='utf-8')\n",
    "    file_output.write(string_output)\n",
    "    file_output.close()\n",
    "        # PAS OUBLIER IL FAUT UNE VARIABLE POUR SAUVEGARDER CES DONNEES PAR DATASET PAR FICHIER\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# Dépassements de seuil =================================\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['desc.txt', 'dialogsum_1_text_en.json', 'dialogsum_2_text_en.json', 'dialogsum_3_text_en.json', 'dialogsum_4_text_en.json', 'dialogsum_5_text_en.json', 'fredsum_1_text_en.json', 'fredsum_2_text_en.json', 'fredsum_3_text_en.json', 'fredsum_4_text_en.json', 'fredsum_5_text_en.json', 'mediasum_1_text_en.json', 'mediasum_2_text_en.json', 'mediasum_3_text_en.json', 'mediasum_4_text_en.json', 'mediasum_5_text_en.json', 'samsum_1_text_en.json', 'samsum_2_text_en.json', 'samsum_3_text_en.json', 'samsum_4_text_en.json', 'samsum_5_text_en.json', 'scores.csv', 'xsum_1_text_en.json', 'xsum_2_text_en.json', 'xsum_3_text_en.json', 'xsum_4_text_en.json', 'xsum_5_text_en.json']\n",
      "0\n",
      "25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:42<00:00, 51.45s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1559143.69 seconds, 0.00 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# A FAIRE : GERER LE BERT MULTILINGUE\n",
    "# Anglais vers français : taux de foisonnement de 20%\n",
    "\n",
    "rouge = evaluate.load('rouge') # Warning ! When comparing many references to a prediction, an average is performed, not a max...\n",
    "bertscore = evaluate.load('bertscore') # The BERTScore calculation will take a lot of time... (if not performed in only one call)\n",
    "\n",
    "mkdir('results')\n",
    "\n",
    "nb_batch = len(os.listdir(\"results\")) # Mettre le numéro du \"batch d'inférence\"\n",
    "\n",
    "# Define the place of storage : \"results/x/scores.csv\"\n",
    "output_names = os.listdir(\"results/\" + nb_batch)\n",
    "# Lists used for the BERT score computation\n",
    "references = {\"0\":[], \"1\":[]}\n",
    "predictions = {\"0\":[], \"1\":[]}\n",
    "# Columns of the csv file\n",
    "output_ids, rouges, input_nb_words_list, nb_words_generated_summaries, nb_words_gold_summaries, outputs_success, outputs_over_context, languages_output, languages_input = [], [], [], [], [], [], [], [], []\n",
    "print(output_names)\n",
    "for output_name in output_names:\n",
    "    # Makes sure the file is an output file, and opens it\n",
    "    output_path = \"results/\" + nb_batch + \"/\" + output_name\n",
    "    if output_name==\"scores.csv\" or output_name==\"desc.txt\" or os.path.isdir(output_path):\n",
    "        continue\n",
    "    output_data = load_json_into_dict(output_path)\n",
    "    generated_summary = output_data[\"text\"]\n",
    "    output_language = str(classify(generated_summary))\n",
    "    language_code = \"text_en\" \n",
    "    if output_language in number_to_code:\n",
    "        language_code = number_to_code[output_language] # Le language code est soit text_fr si output fr, soit text_en si output en ou toute autre langue ! Juste pour choisir un résumé de référence...\n",
    "    \n",
    "    find_path = output_name.split('_') # 0: corpus, 1: name json input file, 2: text (quite useless) 3: fr.json or en.json\n",
    "    input_path = \"datasets/\" + name_dataset + \"/\" + find_path[0] + \"/\" + find_path[1]\n",
    "    input_data = load_json_into_dict(input_path)\n",
    "    summaries_data = input_data[\"summaries\"]\n",
    "    # Computation of Rouge\n",
    "    golds = [] # Sert uniquement pour le calcul du BertScore avec plusieurs références\n",
    "    max_rouge2, max_rougel, nb_words_closest_gold = maxRouge(summaries_data, language_code, generated_summary, golds)\n",
    "    # For BertScore\n",
    "    if output_language in {\"0\", \"1\"}:\n",
    "        predictions[output_language].append(generated_summary)\n",
    "        references[output_language].append(golds)\n",
    "    # Fill the columns\n",
    "    rouges.append([max_rouge2, max_rougel])\n",
    "    output_ids.append(output_name)\n",
    "    nb_words_gold_summaries.append(nb_words_closest_gold)\n",
    "    nb_words_generated_summaries.append(output_data['nb_words'])\n",
    "    outputs_success.append(output_data['success'])\n",
    "    outputs_over_context.append(output_data['over_context'])\n",
    "    languages_input.append(output_data['input_language'])\n",
    "    input_nb_words_list.append(int(input_data['nb_words_' + language_code[-2:]]))\n",
    "    languages_output.append(output_language)\n",
    "\n",
    "print(len(predictions[\"0\"]))\n",
    "print(len(predictions[\"1\"]))\n",
    "\n",
    "\n",
    "#max_bertscores_fr = bertscore.compute(predictions=predictions[\"0\"], references=references[\"0\"], lang='fr', rescale_with_baseline=True, verbose=True)['f1']\n",
    "max_bertscores_en = bertscore.compute(predictions=predictions[\"1\"], references=references[\"1\"], lang='en', rescale_with_baseline=True, verbose=True)['f1']\n",
    "\n",
    "index_fr = 0\n",
    "index_en = 0\n",
    "rows = []\n",
    "for i in range(len(output_ids)):\n",
    "    max_bertscore = 0\n",
    "    if languages_output[i]==\"0\":\n",
    "        #max_bertscore = max_bertscores_fr[index_fr]\n",
    "        index_fr += 1\n",
    "    elif languages_output[i]==\"1\":\n",
    "        max_bertscore = max_bertscores_en[index_en]\n",
    "        index_en += 1\n",
    "    rows.append([output_ids[i], rouges[i][0], rouges[i][1], max_bertscore, input_nb_words_list[i], nb_words_gold_summaries[i], nb_words_generated_summaries[i], outputs_success[i], outputs_over_context[i], languages_output[i], languages_input[i]])\n",
    "\n",
    "header = [[\"input_path\", \"rouge2\", \"rougel\", \"bertscore\", \"nb_words_input\", \"nb_words_gold\", \"nb_words_generated\", \"success\", \"over_context\", \"output_language\", \"input_language\"]]\n",
    "\n",
    "storage_file = open('results/' + nb_batch + '/scores1.csv', mode='w', newline='', encoding='utf-8')# MODIFIER LE SCORE1 ----------------------------------------------------------\n",
    "csv_writer = csv.writer(storage_file)\n",
    "csv_writer.writerows(header + rows)\n",
    "storage_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n"
     ]
    }
   ],
   "source": [
    "a = \"text_en\"\n",
    "print(a[-2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hardware use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvidia-smi' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "# model.hf_device_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "name = 'mosaicml/mpt-7b-instruct'\n",
    "name2 = \"Trelis/mpt-7b-instruct-hosted-inference-8bit\"\n",
    "\n",
    "config = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\n",
    "config.max_seq_len = 4096 # (input + output) tokens can now be up to 4096\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('legendhasit/xgen-7b-8k-inst-8bit', trust_remote_code=True)\n",
    "\n",
    "!nvidia-smi\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "  name,\n",
    "  config=config,\n",
    "  trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "input_test = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "sample = model.generate(**input_test, do_sample=True, max_new_tokens=100, top_k=20, temperature=0.3)\n",
    "print(tokenizer.decode(sample[0]).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [04:06<00:00, 123.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1107629.12 seconds, 0.00 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# A FAIRE : GERER LE BERT MULTILINGUE\n",
    "# COnserver ce 2eme programme adapté aux anciennes générations\n",
    "# Anglais vers français : taux de foisonnement de 20%\n",
    "\n",
    "rouge = evaluate.load('rouge') # Warning ! When comparing many references to a prediction, an average is performed, not a max...\n",
    "bertscore = evaluate.load('bertscore') # The BERTScore calculation will take a lot of time... (if not performed in only one call)\n",
    "\n",
    "mkdir('results')\n",
    "\n",
    "nb_batch = \"2\" # Mettre le numéro du \"batch d'inférence\"\n",
    "\n",
    "# Define the place of storage : \"results/x/scores.csv\"\n",
    "output_names = os.listdir(\"results/\" + nb_batch)\n",
    "# The texts for bertscore\n",
    "references, predictions = [], []\n",
    "# The columns of the csv file\n",
    "output_ids, rouges, input_nb_words_list, nb_words_generated_summaries, nb_words_gold_summaries, outputs_success, outputs_over_context, languages_output, languages_input = [], [], [], [], [], [], [], [], []\n",
    "\n",
    "for output_name in output_names:\n",
    "    # To ensure the read file is an output\n",
    "    output_path = \"results/\" + nb_batch + \"/\" + output_name\n",
    "    if output_name==\"scores.csv\" or output_name==\"desc.txt\" or os.path.isdir(output_path):\n",
    "        continue\n",
    "    # Extraire le dictionnaire de données de l'ouptut\n",
    "    output_data = load_json_into_dict(output_path)\n",
    "    # Obtenir l'input, le langage de l'output, les résumés de référence\n",
    "    language_output = str(classify(generated_summary))\n",
    "    language_code = number_to_code[language_output]\n",
    "    find_path = output_name.split('_') # 0: corpus, 1: name json input file, 2: text (quite useless) 3: fr.json or en.json\n",
    "    input_path = \"datasets/\" + name_dataset + \"/\" + find_path[0] + \"/\" + find_path[1]\n",
    "    input_data = load_json_into_dict(input_path)\n",
    "    summaries_data = input_data[\"summaries\"]\n",
    "    # Preparation pour Max Rouge\n",
    "    golds = [] # Sert uniquement pour le calcul du BertScore avec plusieurs références\n",
    "    max_rouge2, max_rougel, nb_words_closest_gold = maxRouge(summaries_data, language_code, generated_summary, golds)\n",
    "    # Fill in the row\n",
    "    output_ids.append(output_name)\n",
    "    languages_output.append(language_output)\n",
    "    rouges.append([max_rouge2, max_rougel])\n",
    "    predictions.append(generated_summary)\n",
    "    references.append(golds)\n",
    "    nb_words_gold_summaries.append(nb_words_closest_gold)\n",
    "    nb_words_generated_summaries.append(output_data['nb_words'])\n",
    "    outputs_success.append(output_data['success'])\n",
    "    outputs_over_context.append(output_data['over_context'])\n",
    "    input_nb_words_list.append(int(input_data['nb_words_' + language_code]))\n",
    "    languages_input.append(key_to_language[output_data['input_language']])\n",
    "\n",
    "max_bertscores = bertscore.compute(predictions=predictions, references=references, lang='en', rescale_with_baseline=True, verbose=True)['f1'] # PROBLEM WITH LANGUAGE USED\n",
    "\n",
    "storage_file = open('results/' + nb_batch + '/scores4.csv', mode='w', newline='', encoding='utf-8')\n",
    "csv_writer = csv.writer(storage_file)\n",
    "\n",
    "rows = [[output_ids[i], rouges[i][0], rouges[i][1], max_bertscores[i], input_nb_words_list[i], nb_words_gold_summaries[i], nb_words_generated_summaries[i], outputs_success[i], outputs_over_context[i], languages_output[i], languages_input[i]] for i in range(len(output_ids))]\n",
    "\n",
    "header = [[\"input_path\", \"rouge2\", \"rougel\", \"bertscore\", \"nb_words_input\", \"nb_words_gold\", \"nb_words_generated\", \"success\", \"over_context\", \"output_language\", \"input_language\"]]\n",
    "\n",
    "csv_writer.writerows(header + rows)\n",
    "\n",
    "storage_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 776/776 [00:00<?, ?B/s] \n",
      "c:\\Users\\rayci\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rayci\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 13.9MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 6.20MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 161kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", trust_remote_code=True)\n",
    "print(tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizers[\"tiiuae/falcon-7b\"]()\n",
    "model = models[\"tiiuae/falcon-7b\"]()\n",
    "\n",
    "prompt = prompt_templates[\"tiiuae/falcon-7b\"].format(\"Hey, how are you my dear Falcon ?\")\n",
    "tokenized_input = tokenizer(prompt, return_token_type_ids=False, return_tensors=\"pt\").to('cuda')\n",
    "output = infer[\"tiiuae/falcon-7b\"](tokenized_input, model)\n",
    "print(tokenizer.decode(output[0]).strip())\n",
    "print()\n",
    "print(\"Treated ================================\")\n",
    "print()\n",
    "print(treat_output[\"tiiuae/falcon-7b\"](tokenizer.decode(output[0]).strip()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
