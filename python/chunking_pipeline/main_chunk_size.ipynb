{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dependences, tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-api-python-client in /home/linagora/anaconda3/lib/python3.10/site-packages (2.97.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /home/linagora/.local/lib/python3.10/site-packages (from google-api-python-client) (2.11.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /home/linagora/.local/lib/python3.10/site-packages (from google-api-python-client) (4.1.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /home/linagora/.local/lib/python3.10/site-packages (from google-api-python-client) (0.1.0)\n",
      "Requirement already satisfied: google-auth<3.0.0.dev0,>=1.19.0 in /home/linagora/.local/lib/python3.10/site-packages (from google-api-python-client) (2.21.0)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.15.0 in /home/linagora/anaconda3/lib/python3.10/site-packages (from google-api-python-client) (0.22.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /home/linagora/.local/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.59.1)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /home/linagora/anaconda3/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.28.1)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /home/linagora/.local/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (4.23.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/linagora/.local/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.3.0)\n",
      "Requirement already satisfied: urllib3<2.0 in /home/linagora/anaconda3/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (1.26.14)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/linagora/.local/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (4.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/linagora/anaconda3/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (1.16.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/linagora/.local/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (5.3.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /home/linagora/anaconda3/lib/python3.10/site-packages (from httplib2<1.dev0,>=0.15.0->google-api-python-client) (3.0.9)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/linagora/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/linagora/anaconda3/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/linagora/anaconda3/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/linagora/anaconda3/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.0.4)\n",
      "Requirement already satisfied: bitsandbytes in /home/linagora/.local/lib/python3.10/site-packages (0.39.1)\n",
      "Collecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-jfht7cax\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-jfht7cax\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit f92cc7034a49959b247a46a210b912e56a6f977d\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /home/linagora/anaconda3/lib/python3.10/site-packages (from transformers==4.32.0.dev0) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/linagora/anaconda3/lib/python3.10/site-packages (from transformers==4.32.0.dev0) (0.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/linagora/anaconda3/lib/python3.10/site-packages (from transformers==4.32.0.dev0) (22.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/linagora/anaconda3/lib/python3.10/site-packages (from transformers==4.32.0.dev0) (0.11.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/linagora/anaconda3/lib/python3.10/site-packages (from transformers==4.32.0.dev0) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/linagora/anaconda3/lib/python3.10/site-packages (from transformers==4.32.0.dev0) (1.23.5)\n",
      "Requirement already satisfied: requests in /home/linagora/anaconda3/lib/python3.10/site-packages (from transformers==4.32.0.dev0) (2.28.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/linagora/anaconda3/lib/python3.10/site-packages (from transformers==4.32.0.dev0) (2022.7.9)\n",
      "Requirement already satisfied: filelock in /home/linagora/.local/lib/python3.10/site-packages (from transformers==4.32.0.dev0) (3.12.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/linagora/anaconda3/lib/python3.10/site-packages (from transformers==4.32.0.dev0) (4.64.1)\n",
      "Requirement already satisfied: fsspec in /home/linagora/anaconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.32.0.dev0) (2022.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/linagora/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.32.0.dev0) (4.7.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/linagora/anaconda3/lib/python3.10/site-packages (from requests->transformers==4.32.0.dev0) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/linagora/anaconda3/lib/python3.10/site-packages (from requests->transformers==4.32.0.dev0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/linagora/anaconda3/lib/python3.10/site-packages (from requests->transformers==4.32.0.dev0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/linagora/anaconda3/lib/python3.10/site-packages (from requests->transformers==4.32.0.dev0) (1.26.14)\n",
      "Collecting git+https://github.com/huggingface/accelerate.git\n",
      "  Cloning https://github.com/huggingface/accelerate.git to /tmp/pip-req-build-tdflwk46\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate.git /tmp/pip-req-build-tdflwk46\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade google-api-python-client\n",
    "!pip install bitsandbytes\n",
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "!pip install git+https://github.com/huggingface/accelerate.git\n",
    "!pip install tiktoken\n",
    "!pip install torch\n",
    "!pip install scipy\n",
    "!pip install einops # Falcon dependency\n",
    "!pip install huggingface_hub[\"cli\"] # To empty model cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug 25 08:08:05 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100S-PCI...  Off  | 00000000:00:08.0 Off |                    0 |\n",
      "| N/A   42C    P0    39W / 250W |   7502MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100S-PCI...  Off  | 00000000:00:0B.0 Off |                    0 |\n",
      "| N/A   42C    P0    40W / 250W |   7502MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:      181108932     7669360    57199608     1305232   116239964   170494836\n",
      "Swap:             0           0           0\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub[cli] in /workspace/.miniconda3/lib/python3.10/site-packages (0.16.4)\n",
      "Requirement already satisfied: filelock in /workspace/.miniconda3/lib/python3.10/site-packages (from huggingface_hub[cli]) (3.9.0)\n",
      "Requirement already satisfied: fsspec in /workspace/.miniconda3/lib/python3.10/site-packages (from huggingface_hub[cli]) (2023.6.0)\n",
      "Requirement already satisfied: requests in /workspace/.miniconda3/lib/python3.10/site-packages (from huggingface_hub[cli]) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /workspace/.miniconda3/lib/python3.10/site-packages (from huggingface_hub[cli]) (4.51.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /workspace/.miniconda3/lib/python3.10/site-packages (from huggingface_hub[cli]) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /workspace/.miniconda3/lib/python3.10/site-packages (from huggingface_hub[cli]) (4.4.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /workspace/.miniconda3/lib/python3.10/site-packages (from huggingface_hub[cli]) (23.1)\n",
      "Collecting InquirerPy==0.3.4 (from huggingface_hub[cli])\n",
      "  Downloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pfzy<0.4.0,>=0.3.1 (from InquirerPy==0.3.4->huggingface_hub[cli])\n",
      "  Downloading pfzy-0.3.4-py3-none-any.whl (8.5 kB)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /workspace/.miniconda3/lib/python3.10/site-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (3.0.38)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests->huggingface_hub[cli]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests->huggingface_hub[cli]) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests->huggingface_hub[cli]) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests->huggingface_hub[cli]) (2022.12.7)\n",
      "Requirement already satisfied: wcwidth in /workspace/.miniconda3/lib/python3.10/site-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]) (0.2.6)\n",
      "Installing collected packages: pfzy, InquirerPy\n",
      "Successfully installed InquirerPy-0.3.4 pfzy-0.3.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[6n\u001b[?2004h\u001b[?1l\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;180m?\u001b[0m Select revisions to delete:\u001b[0;38;5;249m 0 revisions selected counting for 0.0. \u001b[0m\n",
      "\u001b[0;38;5;75m❯\u001b[0m \u001b[0;38;5;108m○\u001b[0m \u001b[0;38;5;75mNone of the following (if selected, nothing will be deleted).\u001b[0m\n",
      "\u001b[0m \u001b[0m\n",
      "\u001b[0mModel bigscience/mt0-xxl (55.8G, used 6 days ago)\u001b[0m\n",
      "\u001b[0m  \u001b[0;38;5;108m○\u001b[0m b5461b49: main # modified 6 days ago\u001b[0m\n",
      "\u001b[0m \u001b[0m\n",
      "\u001b[0mModel tiiuae/falcon-40b-instruct (50.3K, used 6 days ago)\u001b[0m\n",
      "\u001b[0m  \u001b[0;38;5;108m○\u001b[0m ca78eac0: main # modified 6 days ago\u001b[0m\n",
      "\u001b[0m \u001b[0m\n",
      "\u001b[0mModel ichitaka/falcon-40b-instruct-8bit (41.9G, used 6 days ago)\u001b[0m\n",
      "\u001b[0m  \u001b[0;38;5;108m○\u001b[0m 03dd12af: main # modified 6 days ago\u001b[0m\n",
      "\u001b[0m \u001b[0m\n",
      "\u001b[0mModel Salesforce/xgen-7b-8k-inst (27.6G, used 2 days ago)\u001b[0m\n",
      "\u001b[0m  \u001b[0;38;5;108m○\u001b[0m 68f77dec: (detached) # modified 6 days ago\u001b[0m\n",
      "\u001b[0m  \u001b[0;38;5;108m○\u001b[0m d008fe87: main # modified 2 days ago\u001b[0m\n",
      "\u001b[0m \u001b[0m\n",
      "\u001b[0mModel tiiuae/falcon-7b-instruct (14.4G, used 5 hours ago)\u001b[0m\n",
      "\u001b[0m  \u001b[0;38;5;108m○\u001b[0m eb410fb6: main # modified 5 hours ago\u001b[0m\n",
      "\u001b[0m \u001b[0m\n",
      "\u001b[0mModel mosaicml/mpt-7b-instruct (13.3G, used 19 minutes ago)\u001b[0m\n",
      "\u001b[0m  \u001b[0;38;5;108m○\u001b[0m 925e0d80: main # modified 19 minutes ago\u001b[0m\n",
      "\u001b[79Ci\u001b[0m49mPress <space> to select, <enter> to validate and <ctrl+c> to quit without modif\n",
      "\u001b[0;38;5;249mcation.\u001b[7D\u001b[22A\u001b[69C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[69D\u001b[J\u001b[0m\u001b[?7h\u001b[?2004lWARNING: your terminal doesn't support cursor position requests (CPR).\n",
      "\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;180m?\u001b[0m Select revisions to delete:\u001b[0;38;5;249m 0 revisions selected counting for 0.0. \u001b[0m\n",
      "\u001b[0;38;5;75m❯\u001b[0m \u001b[0;38;5;108m○\u001b[0m \u001b[0;38;5;75mNone of the following (if selected, nothing will be deleted).\u001b[0m\n",
      "\u001b[0m \u001b[0m\n",
      "\u001b[0mModel bigscience/mt0-xxl (55.8G, used 6 days ago)\u001b[0m\n",
      "\u001b[0m  \u001b[0;38;5;108m○\u001b[0m b5461b49: main # modified 6 days ago\u001b[0m\n",
      "\u001b[0m \u001b[0m\n",
      "\u001b[0mModel tiiuae/falcon-40b-instruct (50.3K, used 6 days ago)\u001b[0m\n",
      "\u001b[0m  \u001b[0;38;5;108m○\u001b[0m ca78eac0: main # modified 6 days ago\u001b[0m\n",
      "\u001b[0m \u001b[0m\n",
      "\u001b[0mModel ichitaka/falcon-40b-instruct-8bit (41.9G, used 6 days ago)\u001b[0m\n",
      "\u001b[0m  \u001b[0;38;5;108m○\u001b[0m 03dd12af: main # modified 6 days ago\u001b[0m\n",
      "\u001b[0m \u001b[0m\n",
      "\u001b[0mModel Salesforce/xgen-7b-8k-inst (27.6G, used 2 days ago)\u001b[0m\n",
      "\u001b[0m  \u001b[0;38;5;108m○\u001b[0m 68f77dec: (detached) # modified 6 days ago\u001b[0m\n",
      "\u001b[0m  \u001b[0;38;5;108m○\u001b[0m d008fe87: main # modified 2 days ago\u001b[0m\n",
      "\u001b[0m \u001b[0m\n",
      "\u001b[0mModel tiiuae/falcon-7b-instruct (14.4G, used 5 hours ago)\u001b[0m\n",
      "\u001b[0m  \u001b[0;38;5;108m○\u001b[0m eb410fb6: main # modified 5 hours ago\u001b[0m\n",
      "\u001b[0m \u001b[0m\n",
      "\u001b[0mModel mosaicml/mpt-7b-instruct (13.3G, used 19 minutes ago)\u001b[0m\n",
      "\u001b[0m  \u001b[0;38;5;108m○\u001b[0m 925e0d80: main # modified 19 minutes ago\u001b[0m\n",
      "\u001b[79Ci\u001b[0m49mPress <space> to select, <enter> to validate and <ctrl+c> to quit without modif\n",
      "\u001b[0;38;5;249mcation.\u001b[7D\u001b[22A\u001b[69C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Empty model cache\n",
    "!huggingface-cli delete-cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os, traceback, datetime, torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does not work.\n",
    "#model.generation_config.do_sample = False\n",
    "#model.generation_config.max_new_tokens = 2048\n",
    "#model.generation_config.temperature = .3\n",
    "#model.generation_config.top_k = 100\n",
    "\n",
    "DO_SAMPLE = True\n",
    "MAX_NEW_TOKENS = 2048\n",
    "TEMPERATURE = .3\n",
    "TOP_K = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d97e0f22074d0fa25cd9869f290cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(51200, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=51200, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'Salesforce/xgen-7b-8k-inst'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\n",
    "\n",
    "### Human: Summarize the following text.\n",
    "\n",
    "P1: You're finally here! What took so long?\n",
    "P2: I got stuck in traffic again. There was a terrible traffic jam near the Carrefour intersection.\n",
    "P1: It's always rather congested down there during rush hour. Maybe you should try to find a different route to get home.\n",
    "P2: I don't think it can be avoided, to be honest.\n",
    "P1: perhaps it would be better if you started taking public transport system to work.\n",
    "P2: I think it's something that I'll have to consider. The public transport system is pretty good.\n",
    "P1: It would be better for the environment, too.\n",
    "P2: I know. I feel bad about how much my car is adding to the pollution problem in this city.\n",
    "P1: Taking the subway would be a lot less stressful than driving as well.\n",
    "P2: The only problem is that I'm going to really miss having the freedom that you have with a car.\n",
    "P1: Well, when it's nicer outside, you can start biking to work. That will give you just as much freedom as your car usually provides.\n",
    "P2: That's true. I could certainly use the exercise!\n",
    "P1: So, are you going to quit driving to work then?\n",
    "P2: Yes, it's not good for me or for the environment.\n",
    "\n",
    "### Assistant: Two individuals discuss the delay caused by traffic congestion near Carrefour intersection. One suggests finding an alternate route or using public transport. They discuss the benefits of public transport for the environment, stress reduction, and consider biking as an option in good weather. Eventually, one person decides to stop driving to work due to health and environmental concerns.\n",
    "\n",
    "### Human: Summarize the following text.\n",
    "\n",
    "AC: Once search-and-rescue efforts are under control in China, building experts are going to begin looking at the extent of structural damage. Stanford University's Ann Kiremidjian is a structural engineer. She helped assess damage years after China's 1976 huge earthquake that killed about 250,000 people. Ann Kiremidjian, when you look at the pictures and video of the collapsed buildings around Chengdu, I bet you see things that I don't notice.\n",
    "Pr. AK: Alex, what I have seen from the video clips is first of all a lot of concrete buildings, what we call concrete frames, and I would classify them as a non-ductile concrete frame. What that means is that there is very little steel reinforcement in the concrete and as a result, the concrete cannot resist displacement in the horizontal direction.\n",
    "AC: It needs to have steel bars, I think that is called rebar. Yeah.\n",
    "Pr. AK: That is correct. Not enough rebar both in the vertical direction and in the horizontal direction. And in addition, you have to provide rebar going from the column to the beam at the joint to provide continuity and a way to transfer the forces from one member to the other.\n",
    "AC: You inspected buildings after this last, huge, terrible earthquake more than 30 years ago. Did you make recommendations to the Chinese government then?\n",
    "Pr. AK: At the time, they didn't have earthquake-resistant codes in place. And since then, we've had numerous discussions, and the Chinese have had very strong programs to develop seismic codes, to develop seismic hazard maps. The seismic hazard maps similar to the ones that we have in the United States, are developed with the purpose to identify regions of frequent large earthquakes that can cause damage to buildings, the very first step in any earthquake-resistant design.\n",
    "AC: And what about this region around Chengdu, the Sichuan province, is this prone to earthquakes?\n",
    "Pr. AK: Yes, all of China is in a highly seismic area. You would notice that there is a fault that runs along the plateau between the Himalayas and where Chengdu is. If you trace all the aftershocks, you can probably trace the fault line that has ruptured. To cause a magnitude 7.8 earthquake, you will need to rupture more than 100 kilometers of a fault, and it looks it is almost 200 kilometers of locations of aftershock which is likely to be the length of the rupture of the fault.\n",
    "AC: Is there some way to measure the size of the earthquake and then overlay that on a map of the region and figure out, we have to look at every single building within a particular radius from the epicenter? I mean do you have to do that?\n",
    "Pr. AK: Oh, we do that all the time. In fact, what typically you would do is you would focus within the first 15 or 20 kilometers from the rupture zone. It's not just the epicenter, but it is the rupture zone, and the rupture can run about a couple of hundred kilometers, as it is in the case of China. So you are talking a lot of buildings. A single inspector or 10 or 20 inspectors cannot perform that task in a reasonable amount of time. What you have to do is recruit all the professional structural engineers to help with the inspection process.\n",
    "AC: Ann, you're speaking to us from your home in Los Altos Hills, that's south of San Francisco. And it's two miles from the most notorious earthquake fault in North America, the San Andreas Fault.\n",
    "Pr. AK: Correct.\n",
    "AC: Can you explain to me why a structural engineer would build a house there?\n",
    "Pr. AK: Yes, indeed. You have to take certain precautions. We have taken into consideration the fact that we are close to the fault. Can we do more? Yes, we can. There's always a balancing act between the economics, how much can you afford, and what you are willing to live with. It's a beautiful area, the weather is lovely, it's close to my work, that's where I live.\n",
    "AC: Ann Kirmidjian teaches structural engineering at Stanford University. Professor, thank you.\n",
    "Pr. AK: You're welcome.\n",
    "AC: NPR's DAY TO DAY continues.\n",
    "\n",
    "### Assistant: After the recent earthquake in China, structural engineer Ann Kiremidjian from Stanford University discusses the extent of structural damage. She identifies that many collapsed buildings had non-ductile concrete frames lacking sufficient steel reinforcement, both vertically and horizontally. She emphasizes the importance of earthquake-resistant design, mentioning how China has developed seismic codes and hazard maps over the years. All of China, including the Sichuan province where the earthquake occurred, is in a highly seismic area due to fault lines. Kiremidjian explains that inspecting buildings after earthquakes involves focusing on the rupture zone within the first 15-20 kilometers, which requires the involvement of numerous professional structural engineers. The interview also touches on building near earthquake faults, highlighting the trade-off between economic considerations and safety precautions.\n",
    "\n",
    "### Human: Summarize the following text.\n",
    "\n",
    "{text}\n",
    "\n",
    "### Assistant:\"\"\"\n",
    "\n",
    "INTERMEDIATE_FOLDER = 'xgen_7b_2sl_6kt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_from_output_sequence(output, n_shot = 2):\n",
    "    \n",
    "    # Un-prompt output\n",
    "    delimiter = '### Assistant:'\n",
    "    output = delimiter.join(output.split(delimiter)[1 + n_shot:])\n",
    "    \n",
    "    # Remove white spaces in front of summary\n",
    "    while len(output) > 0 and output[0] == ' ':\n",
    "        output = output[1:]\n",
    "    \n",
    "    # Remove <|endoftext|>\n",
    "    eos_token = '\\n<|endoftext|>'\n",
    "    output = output[:-len(eos_token)]\n",
    "    \n",
    "    return output\n",
    "\n",
    "def generate_output(prompt, counter, chunks):\n",
    "    with torch.no_grad(): # Do not store forward activations\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "        print('---- Computing chunk', counter, 'of', len(chunks), 'total, size:', len(input_ids['input_ids'][0]), 'tokens')\n",
    "        output_ids = model.generate(**input_ids, do_sample=DO_SAMPLE, max_new_tokens=MAX_NEW_TOKENS, temperature=TEMPERATURE, eos_token_id=50256, top_k=TOP_K)\n",
    "        output = tokenizer.decode(output_ids[0]).strip()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 samples: ['sample_10_9852.txt', 'sample_11_10498.txt', 'sample_12_11166.txt', 'sample_13_4562.txt', 'sample_14_6858.txt', 'sample_15_9989.txt', 'sample_16_6057.txt', 'sample_17_4135.txt', 'sample_18_9825.txt', 'sample_19_9648.txt', 'sample_1_4693.txt', 'sample_20_13365.txt', 'sample_21_7283.txt', 'sample_22_6522.txt', 'sample_23_9169.txt', 'sample_24_10638.txt', 'sample_25_2751.txt', 'sample_26_7860.txt', 'sample_27_9004.txt', 'sample_28_5705.txt']\n"
     ]
    }
   ],
   "source": [
    "def mkdir(folder_path):\n",
    "    try:\n",
    "        os.mkdir(folder_path)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "def get_intermediate_file_path(sample):\n",
    "    global INTERMEDIATE_FOLDER\n",
    "\n",
    "    return \"intermediate/\" + INTERMEDIATE_FOLDER + '/' + sample\n",
    "\n",
    "# Load samples from dataset\n",
    "samples = os.listdir('input/texts')\n",
    "samples = [sample for sample in samples if sample.endswith('.txt')]\n",
    "samples.sort()\n",
    "samples = samples[:20]\n",
    "n_samples = len(samples)\n",
    "print('Found', n_samples, 'samples:', samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Chunking algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CHARACTER_CHUNK_SIZE = 6000\n",
    "\n",
    "def token_len(text):\n",
    "    return len(tokenizer(text, return_tensors=\"pt\")['input_ids'][0])\n",
    "\n",
    "def append_to_chunk(current_chunk, utterance):\n",
    "    if len(current_chunk) > 0:\n",
    "        current_chunk += '\\n'\n",
    "    current_chunk += utterance\n",
    "    return current_chunk\n",
    "\n",
    "def chunkize(text):\n",
    "    \"\"\"\n",
    "    Greedy implementation of a dialogue transcript chunking algorithm. This method returns a list of transcript chunks.\n",
    "    - It priorities stability over performance. There is a set maximum chunk size for LLM inference stability. Really long utterances bypass this limit.\n",
    "    - It guarantees the cuts are made at utterance ends (\\n).\n",
    "    - It counts everything in MODEL TOKENS and not characters for more exact experiments.\n",
    "    \"\"\"\n",
    "    \n",
    "    chunks = [] # Final list of transcript chunks. This makes up the loop invariant\n",
    "    utterances = text.split('\\n') # Transcript is split into sentences\n",
    "    utterances.reverse() # Reverse everything!!\n",
    "    current_chunk = ''\n",
    "\n",
    "    # While there is still an utterance to process\n",
    "    while len(utterances) > 0:\n",
    "        utterance = utterances.pop()\n",
    "\n",
    "        # Add to current chunk and proceed to next\n",
    "        new_current_chunk = append_to_chunk(current_chunk, utterance)\n",
    "        if token_len(new_current_chunk) <= MAX_CHARACTER_CHUNK_SIZE:\n",
    "            current_chunk = new_current_chunk\n",
    "        \n",
    "        # Utterance is larger than maximum chunk size\n",
    "        elif len(current_chunk) == 0:\n",
    "            chunks.append(current_chunk)\n",
    "            chunks.append(utterance)\n",
    "            current_chunk = ''\n",
    "\n",
    "        # Current chunk is big enough, append to list and create new one\n",
    "        else:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = utterance\n",
    "\n",
    "    if len(current_chunk) > 0:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Merging algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_sample(sample):\n",
    "    \n",
    "    # Current summary is the concatenation of all the chunk sub-summaries \n",
    "    current_summary = ''\n",
    "    #previous_summary = ''\n",
    "    chunks = chunkize(sample)\n",
    "\n",
    "    # Split into chunks\n",
    "    counter = 0\n",
    "    for chunk in chunks:\n",
    "        counter += 1\n",
    "\n",
    "        # Create prompt for this chunk\n",
    "        #if len(previous_summary) == 0:\n",
    "        #    prompt = PROMPT_TEMPLATE_1.format(text=chunk)\n",
    "        #    n_shot = 2\n",
    "        #else:\n",
    "        #    prompt = PROMPT_TEMPLATE_2.format(summary=previous_summary, text=chunk)\n",
    "        #    n_shot = 0\n",
    "        prompt = PROMPT_TEMPLATE.format(text=chunk)\n",
    "        #n_shot = 0\n",
    "            \n",
    "        # Sample one sub-input\n",
    "        output = generate_output(prompt, counter, chunks)\n",
    "        subsummary = result_from_output_sequence(output)\n",
    "        print(subsummary)\n",
    "\n",
    "        # Add to summary\n",
    "        if len(current_summary) > 0:\n",
    "            current_summary += '\\n\\n'\n",
    "        current_summary += subsummary\n",
    "        #previous_summary = subsummary\n",
    "    \n",
    "    return current_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 5947 tokens\n",
      "- 3904 tokens\n"
     ]
    }
   ],
   "source": [
    "# Tests for chunking algorithm\n",
    "import os\n",
    "\n",
    "sample_file_path = 'input/texts/' + samples[0]\n",
    "sample_file = open(sample_file_path, 'r', encoding='utf-8')\n",
    "sample = sample_file.read()\n",
    "sample_file.close()\n",
    "\n",
    "chunks = chunkize(sample)\n",
    "for chunk in chunks:\n",
    "    print('->', token_len(chunk), 'tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting computation...\n",
      "Prompting on sample N1/20\n",
      "-- Completion: 0.0%\n",
      "---- Computing chunk 1 of 2 total, size: 7431 tokens\n",
      "Error while generating sample\n",
      "Done! Took 0:00:03 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_5700/3447844630.py\", line 44, in <module>\n",
      "    output = inference_sample(sample)\n",
      "  File \"/tmp/ipykernel_5700/1118729588.py\", line 24, in inference_sample\n",
      "    output = generate_output(prompt, counter, chunks)\n",
      "  File \"/tmp/ipykernel_5700/2000570703.py\", line 21, in generate_output\n",
      "    output_ids = model.generate(**input_ids, do_sample=DO_SAMPLE, max_new_tokens=MAX_NEW_TOKENS, temperature=TEMPERATURE, eos_token_id=50256, top_k=TOP_K)\n",
      "  File \"/workspace/.miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/workspace/.miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1642, in generate\n",
      "    return self.sample(\n",
      "  File \"/workspace/.miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2724, in sample\n",
      "    outputs = self(\n",
      "  File \"/workspace/.miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/workspace/.miniconda3/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
      "    output = old_forward(*args, **kwargs)\n",
      "  File \"/workspace/.miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 810, in forward\n",
      "    outputs = self.model(\n",
      "  File \"/workspace/.miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/workspace/.miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 698, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "  File \"/workspace/.miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/workspace/.miniconda3/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
      "    output = old_forward(*args, **kwargs)\n",
      "  File \"/workspace/.miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 413, in forward\n",
      "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "  File \"/workspace/.miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/workspace/.miniconda3/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
      "    output = old_forward(*args, **kwargs)\n",
      "  File \"/workspace/.miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 335, in forward\n",
      "    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.58 GiB (GPU 0; 31.74 GiB total capacity; 20.33 GiB already allocated; 4.07 GiB free; 26.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "initial_time = time.time()\n",
    "skipped_samples = 0\n",
    "\n",
    "mkdir('intermediate')\n",
    "mkdir('intermediate/' + INTERMEDIATE_FOLDER)\n",
    "\n",
    "print('Starting computation...')\n",
    "\n",
    "# For each sample in dataset\n",
    "check = True\n",
    "for sample_n in range(n_samples):\n",
    "    if not(check):\n",
    "        break\n",
    "\n",
    "    # Estimate completion and time.\n",
    "    cur_samples = sample_n - skipped_samples\n",
    "    tot_samples = n_samples - skipped_samples\n",
    "    progress = cur_samples / tot_samples\n",
    "    pct = round(progress * 100, 1)\n",
    "    print('Prompting on sample N' + str(sample_n + 1) + '/' + str(n_samples))\n",
    "    print('-- Completion: ' + str(pct) + '%')\n",
    "    if cur_samples > 0:\n",
    "        approx_total = (time.time() - initial_time) / cur_samples * tot_samples\n",
    "        approx_remaining = approx_total * (1 - progress)\n",
    "        print('-- Estimated Remaining Time: ' + str(datetime.timedelta(seconds=int(approx_remaining))) + ' (total ' + str(datetime.timedelta(seconds=int(approx_total))) + ')')\n",
    "    \n",
    "    # Read sample and generate prompt\n",
    "    sample_file_path = 'input/texts/' + samples[sample_n]\n",
    "    sample_file = open(sample_file_path, 'r', encoding='utf-8')\n",
    "    sample = sample_file.read()\n",
    "    sample_file.close()\n",
    "    \n",
    "    # Find target file\n",
    "    target_file_path = get_intermediate_file_path(samples[sample_n])\n",
    "    if os.path.isfile(target_file_path):\n",
    "        print('-- Found intermediate result file \\'' + target_file_path + '\\', skipped.')\n",
    "        skipped_samples += 1\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        check = False\n",
    "\n",
    "        # Inference\n",
    "        output = inference_sample(sample)\n",
    "\n",
    "        # Save answer in file\n",
    "        target_file = open(target_file_path, 'w', encoding='utf-8')\n",
    "        target_file.write(output)\n",
    "        target_file.close()\n",
    "        \n",
    "        check = True\n",
    "    except:\n",
    "        print('Error while generating sample')\n",
    "        traceback.print_exc()\n",
    "\n",
    "delta = time.time() - initial_time\n",
    "print('Done! Took', datetime.timedelta(seconds=int(delta)), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch 2.0.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
