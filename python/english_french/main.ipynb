{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rayci\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import time\n",
    "import datetime\n",
    "import evaluate\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoConfig\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Working conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "name_dataset = \"LanguageTestDataSet\" # The dataset must be in a certain format\n",
    "\n",
    "# Models used\n",
    "used_models_name = {'legendhasit/xgen-7b-8k-inst-8bit'} # Models taken for computation \n",
    "\n",
    "# Input limits\n",
    "token_limit = 3000 # To be determined with the context length of the used models\n",
    "\n",
    "# Description of the inference wanted\n",
    "desc = \"This inferenee batch is aimed at testing XGen 8bit on summarization. We shall compare his performance on French and on English\"\n",
    "\n",
    "# Caracteristics of the inference wanted\n",
    "max_new_tokens=700\n",
    "top_k=20\n",
    "temperature=0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompting elements\n",
    "instruction_templates = {\"text_en\":\"Summarize the following text:\\n\\n{text}\", \"text_fr\":\"Résume le texte suivant:\\n\\n{text}\"}\n",
    "text_keys = {'text_fr', 'text_en'} # Cet ensemble donne les clés pour le texte français et anglais.\n",
    "key_to_language = {'text_fr':'0', 'text_en':'1'}\n",
    "\n",
    "# Miscellaneous useful functions and constants\n",
    "def count_words(s):\n",
    "    words = re.findall(r'\\b\\w+\\b', s)  # Find all word-like sequences using regular expression\n",
    "    return len(words)  # Return the number of words\n",
    "\n",
    "def mkdir(folder_path):\n",
    "    try:\n",
    "        os.mkdir(folder_path)\n",
    "    except FileExistsError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These dictionaries have as a key the model name and as the value the function that load the prompt, the tokenizer, the model, give the context length or treat the output.\n",
    "\n",
    "prompt_templates = {}\n",
    "context_lengths = {}\n",
    "tokenizers = {}\n",
    "models = {}\n",
    "treat_output = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model specific functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models available\n",
    "models_name = {\n",
    "    \"legendhasit/xgen-7b-8k-inst-8bit/4bit\", # 1 GPU \n",
    "    \"legendhasit/xgen-7b-8k-inst-8bit\", # 1 GPU\n",
    "    \"Salesforce/xgen-7b-8k-inst\", # 2GPU (3 shards de 10Go peut-être la dernière moins) de préférence, mais fonctionne avec 1... Sauf pour les textes longs, genre les plus petits de Fredsum + probablement mauvaise suppression automatique de la mémoire -> On a des OOM à 38GB\n",
    "    \"mosaicml/mpt-7b-instruct\", # 1 GPU (2 shards de 10 Go)- To quantize\n",
    "    \"Trelis/mpt-7b-instruct-hosted-inference-8bit\", #  - Potentially unreliable\n",
    "    \"mosaicml/mpt-7b-8k-instruct\", # To quantize\n",
    "    \"mosaicml/mpt-30b-instruct\" # To quantize\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt templates\n",
    "prompt_template_XGen = \"A chat between a curious human and an artificial intelligence assistant.\\nThe assistant gives helpful, detailed, and polite answers to the human's questions.\\n\\n### Human: {instruction}\\n\\n### Assistant: \"\n",
    "\n",
    "prompt_templates[\"legendhasit/xgen-7b-8k-inst-8bit\"] = prompt_template_XGen\n",
    "prompt_templates[\"legendhasit/xgen-7b-8k-inst-8bit/4bit\"] = prompt_template_XGen\n",
    "prompt_templates[\"Salesforce/xgen-7b-8k-inst\"] = prompt_template_XGen\n",
    "\n",
    "# Context lengths # Allow to know whether the model generated out of his context window\n",
    "context_length_XGen = 8000 \n",
    "\n",
    "context_lengths[\"legendhasit/xgen-7b-8k-inst-8bit\"] = context_length_XGen\n",
    "context_lengths[\"legendhasit/xgen-7b-8k-inst-8bit/4bit\"] = context_length_XGen\n",
    "context_lengths[\"Salesforce/xgen-7b-8k-inst\"] = context_length_XGen\n",
    "\n",
    "# Tokenizers\n",
    "def get_tokenizer_XGen8bit(): # Le même pour le 4bit\n",
    "    return AutoTokenizer.from_pretrained('legendhasit/xgen-7b-8k-inst-8bit', trust_remote_code=True)\n",
    "\n",
    "def get_tokenizer_XGen():\n",
    "    return AutoTokenizer.from_pretrained('Salesforce/xgen-7b-8k-inst', trust_remote_code=True)\n",
    "\n",
    "tokenizers[\"legendhasit/xgen-7b-8k-inst-8bit\"] = get_tokenizer_XGen8bit\n",
    "tokenizers[\"legendhasit/xgen-7b-8k-inst-8bit/4bit\"] = get_tokenizer_XGen8bit\n",
    "tokenizers[\"Salesforce/xgen-7b-8k-inst\"] = get_tokenizer_XGen\n",
    "\n",
    "# Models\n",
    "\n",
    "def get_model_XGen8bit():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    'legendhasit/xgen-7b-8k-inst-8bit',\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_model_XGen4bit():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    'legendhasit/xgen-7b-8k-inst-8bit',\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_model_XGen():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    'Salesforce/xgen-7b-8k-inst',\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "    return model\n",
    "\n",
    "models[\"legendhasit/xgen-7b-8k-inst-8bit\"] = get_model_XGen8bit\n",
    "models[\"legendhasit/xgen-7b-8k-inst-8bit/4bit\"] = get_model_XGen4bit\n",
    "models[\"Salesforce/xgen-7b-8k-inst\"] = get_model_XGen\n",
    "\n",
    "# Tokenize (if specificities in the way models call the tokenizer)\n",
    "\n",
    "# Inference (if specificities in the way models call the generate function)\n",
    "\n",
    "def infer_XGen(tokenized_input):\n",
    "    return model.generate(**tokenized_input, do_sample=True, max_new_tokens=max_new_tokens, top_k=top_k, eos_token_id=50256, temperature=temperature) # eos_token_id : id du token de fin de réponse d'XGen dans son vocabulaire.\n",
    "\n",
    "infer['legendhasit/xgen-7b-8k-inst-8bit'] = infer_XGen\n",
    "infer['legendhasit/xgen-7b-8k-inst-8bit/4bit'] = infer_XGen\n",
    "infer['Salesforce/xgen-7b-8k-inst'] = infer_XGen\n",
    "\n",
    "# Treating the output (to remove the input if present in the output as well as the end of text token for example)\n",
    "\n",
    "def treat_output_XGen(output): # Differs if it is N-shot. Here, it is 0-shot\n",
    "    occ_1 = output.find(\"### Assistant: \")\n",
    "    output = output[occ_1+15:]\n",
    "    if output.find('<|endoftext|>')!=-1:\n",
    "        output = output[:-14]\n",
    "    return output\n",
    "\n",
    "treat_output['legendhasit/xgen-7b-8k-inst-8bit'] = treat_output_XGen\n",
    "treat_output['legendhasit/xgen-7b-8k-inst-8bit/4bit'] = treat_output_XGen\n",
    "treat_output['Salesforce/xgen-7b-8k-inst'] = treat_output_XGen\n",
    "\n",
    "# /workspace/.miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1411: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
    "# Avec XGen de salesforce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt templates\n",
    "prompt_template_MPT7B = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{instruction}\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template_MPT30B = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n###Instruction\\n{instruction}\\n\\n### Response\\n\" # On considère que c'est le même template que pour MPT 7B 8k car pour ce dernier, le template utilisé n'est pas précisé\n",
    "\n",
    "prompt_templates[\"mosaicml/mpt-7b-instruct\"] = prompt_template_MPT7B\n",
    "prompt_templates[\"Trelis/mpt-7b-instruct-hosted-inference-8bit\"] = prompt_template_MPT7B\n",
    "prompt_templates[\"mosaicml/mpt-7b-8k-instruct\"] = prompt_template_MPT30B\n",
    "prompt_templates[\"mosaicml/mpt-30b-instruct\"] = prompt_template_MPT30B\n",
    "\n",
    "import transformers\n",
    "\n",
    "name = 'mosaicml/mpt-7b-instruct'\n",
    "\n",
    "config = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\n",
    "config.max_seq_len = 4096 # (input + output) tokens can now be up to 4096\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "  name,\n",
    "  config=config,\n",
    "  trust_remote_code=True\n",
    ")\n",
    "\n",
    "\n",
    "# Treating the output\n",
    "\n",
    "def treat_output_MPT(output): # Differs if it is N-shot. Here, it is 0-shot\n",
    "    occ_1 = output.find(\"### Response:\\n\")\n",
    "    output = output[occ_1+14:]\n",
    "    if output.find('<|endoftext|>')!=-1:\n",
    "        output = output[:-14]\n",
    "    return output\n",
    "\n",
    "treat_output[\"mosaicml/mpt-7b-instruct\"] = treat_output_MPT\n",
    "treat_output[\"Trelis/mpt-7b-instruct-hosted-inference-8bit\"] = treat_output_MPT\n",
    "treat_output[\"mosaicml/mpt-7b-8k-instruct\"] = treat_output_MPT\n",
    "treat_output[\"mosaicml/mpt-30b-instruct\"] = treat_output_MPT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing the input dataset\n",
    "Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for corpus in os.listdir(\"datasets/\" + name_dataset): # This script fills in the number of words and number of characters of the input files\n",
    "    for file_name in os.listdir(\"datasets/\" + name_dataset + \"/\" + corpus):\n",
    "        if file_name==\".ipynb_checkpoints\":\n",
    "            continue\n",
    "        file_path = \"datasets/\" + name_dataset + \"/\" + corpus + \"/\" + file_name\n",
    "        file = open(file_path, 'r', encoding='utf-8')\n",
    "        data = json.load(file)\n",
    "        file.close()\n",
    "        text = data[\"text_en\"]\n",
    "        nb_words = count_words(text)\n",
    "        nb_characters = len(text)\n",
    "        data[\"nb_characters\"] = str(nb_characters)\n",
    "        data[\"nb_words\"] = str(nb_words)\n",
    "        for i in range(len(data[\"summaries\"])):\n",
    "            summary = data[\"summaries\"][i]\n",
    "            text = summary[\"text_en\"]\n",
    "            nb_words = count_words(text)\n",
    "            nb_characters = len(text)\n",
    "            summary[\"nb_characters\"] = str(nb_characters)\n",
    "            summary[\"nb_words\"] = str(nb_words)\n",
    "        file = open(file_path, 'w', encoding='utf-8')\n",
    "        json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "        file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every time the script is casted, it must register the results in the \"results\" folder, without smashing the existing results\n",
    "# Then, it is stored in a file whose name is the number of the result, and this file contains a little .txt note describing what was the experiment.\n",
    "# In such a folder, all the generations are json files with additional information like the prompt used, the number of samples, the path of the input text\n",
    "\n",
    "# Now, what is specific to the model ? The prompt (different headers potentially), the tokenizer, the model itself, the way the output is displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir(\"results\")\n",
    "\n",
    "archives = os.listdir(\"results\")\n",
    "output_folder_path = \"results/\"+str(len(archives))\n",
    "mkdir(output_folder_path) # Create a folder for the last results.\n",
    "desc_file = open(output_folder_path + \"/desc.txt\", 'w', encoding='utf-8')\n",
    "desc_file.write(desc)\n",
    "desc_file.close()\n",
    "\n",
    "initial_time = time.time()\n",
    "\n",
    "model_total = len(used_models_name)\n",
    "model_index = 0\n",
    "for model_name in used_models_name: # For each model\n",
    "    model_index += 1\n",
    "    print(\"---- Model : \" + model_name + \" (\" + str(model_index) + \"/\" + str(model_total) +\")----              (loading tokenizer and model...)\")\n",
    "    load_model_time = time.time()\n",
    "    prompt_template = prompt_templates[model_name]\n",
    "    tokenizer = tokenizers[model_name]()\n",
    "    model = models[model_name]()\n",
    "    load_model_time = time.time() - load_model_time\n",
    "    print(\"Tokenizer and model loaded in\", datetime.timedelta(seconds=int(load_model_time)), 'seconds')\n",
    "    corpora = os.listdir(\"datasets/\" + name_dataset)\n",
    "    for corpus in corpora:\n",
    "        file_index = 0\n",
    "        files_name = os.listdir(\"datasets/\" + name_dataset + \"/\" + corpus)\n",
    "        file_total = len(files_name)\n",
    "        for file_name in files_name:\n",
    "            if file_name==\"ipynb_checkpoints\":\n",
    "                continue\n",
    "            file_time = time.time()\n",
    "            file_index+=1\n",
    "            print(\"Starting inference for text \" + str(file_index) + \"/\" + str(file_total) + \" in the \" + corpus + \"corpus.\")\n",
    "            file_path = \"datasets/\" + name_dataset + \"/\" + corpus + \"/\" + file_name\n",
    "            file = open(file_path, 'r', encoding='utf-8')\n",
    "            input_data = json.load(file)\n",
    "            file.close()\n",
    "            for text_key in text_keys:\n",
    "                input_text = input_data[text_key]\n",
    "                instruction_template = instruction_templates[text_key]\n",
    "                full_instruction = instruction_template.format(text=input_text)\n",
    "                prompt = prompt_template.format(instruction=full_instruction)\n",
    "                # Probably specific to the model\n",
    "                input = tokenizer(prompt, return_tensors=\"pt\").to('cuda') # Le renvoie sur le GPU car au départ, c'est généré en CPU le tensuer des tokens\n",
    "                #\n",
    "                input_length = len(input['input_ids'][0])\n",
    "                output_name = corpus + \"_\" + file_name + \"_\" + text_key + \".json\"\n",
    "                output_path = output_folder_path + \"/\" + output_name\n",
    "                output_data = {\"input_path\":file_path, \"model\":model_name, \"instruction\":instruction_template.format(text=\"\"), \"input_language\":text_key, \"success\":\"0\", \"over_context\":\"\", \"input_length\":str(input_length), \"text\":\"\", \"output_length\":\"\", \"nb_words\":\"\", \"nb_characters\":\"\"}\n",
    "                if input_length > token_limit: # In this case, a file is created but no inference is made\n",
    "                    print(\"/!\\ With model \" + model_name + \", the input length is above the token limit for \" + text_key + \" input in \" + file_path + \" (\" + str(input_length) + \" > \" + str(token_limit) + \")\")\n",
    "\n",
    "                else: # In this case, the output is inferred, treated to keep only the generation and the length information\n",
    "                    # PROBABLY SPECIFIC TO THE MODEL\n",
    "                    sample = infer[model_name](input)\n",
    "                    sample_length = len(sample[0])\n",
    "                    ### SPECIFIC TO THE MODEL ----------------------------------------\n",
    "                    if sample_length > context_lengths[model_name]: # There was a context window overflow\n",
    "                        output_data[\"over_context\"] = \"1\"\n",
    "                    else:\n",
    "                        output_data[\"over_context\"] = \"0\"\n",
    "                    output_length = sample_length - input_length\n",
    "                    output_data[\"output_length\"] = output_length\n",
    "                    full_output = tokenizer.decode(sample[0]).strip() # prompt + output generation\n",
    "                    # Ideally, treat_output is a dictionnary that associates model_name to a function taking full_output as an argument\n",
    "                    output = treat_output[model_name](full_output)\n",
    "                    output_data[\"text\"] = output\n",
    "                    output_data[\"nb_characters\"] = len(output)\n",
    "                    output_data[\"nb_words\"] = count_words(output)\n",
    "                    output_data[\"success\"] = \"1\"\n",
    "                # SAVING THE OUTPUT\n",
    "                output_file = open(output_path, 'w', encoding='utf-8')\n",
    "                json.dump(output_data, output_file, indent=4, ensure_ascii=False)\n",
    "                output_file.close()\n",
    "            file_time = time.time() - file_time\n",
    "            print(\"Inference done for French and English in\", datetime.timedelta(seconds=int(file_time)), \"seconds.\")\n",
    "    # HERE, clear memory... remove model, tokenizer from memory\n",
    "                \n",
    "delta = time.time() - initial_time\n",
    "print('Done! Took', datetime.timedelta(seconds=int(delta)), 'seconds')\n",
    "# Write treatOutput function / dictionnary\n",
    "\n",
    "\n",
    "# RETIRER le fichier template dans samsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:10<00:00, 35.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 869995.94 seconds, 0.00 sentences/sec\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge') # Warning ! When comparing many references to a prediction, an average is performed, not a max...\n",
    "bertscore = evaluate.load('bertscore') # The BERTScore calculation will take a lot of time... (if not performed in only one call)\n",
    "\n",
    "mkdir('results')\n",
    "\n",
    "nb_batch = \"2\" # Mettre le numéro du \"batch d'inférence\"\n",
    "\n",
    "# Define the place of storage : \"results/x/scores.csv\"\n",
    "output_names = os.listdir(\"results/\" + nb_batch)\n",
    "references = []\n",
    "predictions = []\n",
    "rouges = []\n",
    "output_ids = []\n",
    "input_nb_words_list = []\n",
    "nb_words_generated_summaries = []\n",
    "nb_words_gold_summaries = []\n",
    "outputs_success = []\n",
    "outputs_over_context = []\n",
    "\n",
    "for output_name in output_names:\n",
    "    if output_name==\"scores.csv\" or output_name==\"desc.txt\" or os.path.isdir(\"results/\" + nb_batch + \"/\" + output_name):\n",
    "        continue\n",
    "    output_ids.append(output_name)\n",
    "    output_file = open(\"results/\" + nb_batch + \"/\" + output_name, 'r', encoding='utf-8')\n",
    "    output_data = json.load(output_file)\n",
    "    output_file.close()\n",
    "    generated_summary = output_data[\"text\"]\n",
    "    nb_words_generated_summaries.append(output_data['nb_words'])\n",
    "    outputs_success.append(output_data['success'])\n",
    "    outputs_over_context.append(output_data['over_context'])\n",
    "    find_path = output_name.split('_') # 0: corpus, 1: name json input file, 2: text (quite useless) 3: fr.json or en.json\n",
    "    input_path = \"datasets/\" + name_dataset + \"/\" + find_path[0] + \"/\" + find_path[1]\n",
    "    input_file = open(input_path, 'r', encoding='utf-8')\n",
    "    input_data = json.load(input_file)\n",
    "    input_nb_words_list.append(int(input_data['nb_words']))\n",
    "    input_file.close()\n",
    "    summaries_data = input_data[\"summaries\"]\n",
    "    lang = output_data['input_language'][-2:] # en or fr, for bertscore lang parameter\n",
    "    max_rouge2 = -1\n",
    "    max_rougel = -1\n",
    "    golds = []\n",
    "    nb_words_closest_gold = 0\n",
    "    for i in range(len(summaries_data)):\n",
    "        summary_data = summaries_data[i]\n",
    "        summary_text = summary_data[find_path[2] + \"_\" + find_path[3][:2]]\n",
    "        result_rouge = rouge.compute(predictions=[generated_summary], references=[summary_text], use_aggregator=False)\n",
    "        golds.append(summary_text)\n",
    "        if result_rouge['rouge2'][0] > max_rouge2:\n",
    "            max_rouge2 = result_rouge['rouge2'][0]\n",
    "            nb_words_closest_gold = summary_data['nb_words']\n",
    "        if result_rouge['rougeL'][0] > max_rougel:\n",
    "            max_rougel = result_rouge['rougeL'][0]\n",
    "    rouges.append([max_rouge2, max_rougel])\n",
    "    predictions.append(generated_summary)\n",
    "    references.append(golds)\n",
    "    nb_words_gold_summaries.append(nb_words_closest_gold)\n",
    "max_bertscores = bertscore.compute(predictions=predictions, references=references, lang=lang, rescale_with_baseline=True, verbose=True)['f1']\n",
    "\n",
    "storage_file = open('results/' + nb_batch + '/scores.csv', mode='w', newline='', encoding='utf-8')\n",
    "csv_writer = csv.writer(storage_file)\n",
    "\n",
    "rows = [[output_ids[i], rouges[i][0], rouges[i][1], max_bertscores[i], input_nb_words_list[i], nb_words_gold_summaries[i], nb_words_generated_summaries[i], outputs_success[i], outputs_over_context[i]] for i in range(len(output_ids))]\n",
    "\n",
    "header = [[\"input_path\", \"rouge2\", \"rougel\", \"bertscore\", \"nb_words_input\", \"nb_words_gold\", \"nb_words_generated\", \"success\", \"over_context\"]]\n",
    "\n",
    "csv_writer.writerows(header + rows)\n",
    "\n",
    "storage_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hardware use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvidia-smi' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
