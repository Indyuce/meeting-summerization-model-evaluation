{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "import traceback\n",
    "import gc\n",
    "import evaluate\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoConfig\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Working conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "name_dataset = \"LanguageTestDataSet\" # The dataset must be in a certain format\n",
    "\n",
    "# Models used\n",
    "models_name = {'legendhasit/xgen-7b-8k-inst-8bit', 'legendhasit/xgen-7b-dolly-15k-4bit', 'mosaicml/mpt-7b-instruct', 'Trelis/mpt-7b-instruct-hosted-inference-8bit'} # Model names list\n",
    "\n",
    "# Input limits\n",
    "token_limit = 1800 # To be determined with the context length of the used models\n",
    "\n",
    "# Description of the inference wanted\n",
    "desc = \"This inference batch is aimed at testing a lot of models on a simple, preliminary, text for summarization. The 4bit 8bit 16bit and 32 bit models will be compared.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompting elements\n",
    "instruction_templates = {\"Summarize the following text:\\n\\n{text}\", \"Résume le texte suivant:\\n\\n{text}\"}\n",
    "text_keys = {'text_fr', 'text_en'} # Cet ensemble donne les clés pour le texte français et anglais.\n",
    "key_to_language = {'text_fr':'0', 'text_en':'1'}\n",
    "\n",
    "# Miscellaneous useful functions and constants\n",
    "def count_words(s):\n",
    "    words = re.findall(r'\\b\\w+\\b', s)  # Find all word-like sequences using regular expression\n",
    "    return len(words)  # Return the number of words\n",
    "\n",
    "def mkdir(folder_path):\n",
    "    try:\n",
    "        os.mkdir(folder_path)\n",
    "    except FileExistsError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model specific functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt templates\n",
    "prompt_template_XGen = \"A chat between a curious human and an artificial intelligence assistant.\\nThe assistant gives helpful, detailed, and polite answers to the human's questions.\\n\\n### Human: {instruction}\\n\\n### Assistant: \"\n",
    "\n",
    "prompt_templates = {\n",
    "    \"legendhasit/xgen-7b-8k-inst-8bit\":prompt_template_XGen\n",
    "                    }\n",
    "\n",
    "# Tokenizers\n",
    "\n",
    "def get_tokenizer_XGen8bit():\n",
    "    return AutoTokenizer.from_pretrained('legendhasit/xgen-7b-8k-inst-8bit', trust_remote_code=True)\n",
    "\n",
    "def get_tokenizer_XGen():\n",
    "    return AutoTokenizer.from_pretrained(\"Salesforce/xgen-7b-8k-inst\", trust_remote_code=True)\n",
    "\n",
    "def get_tokenizer_MPT7B():\n",
    "    return AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "\n",
    "def get_tokenizer_MPT7Bbit():\n",
    "    return AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')\n",
    "\n",
    "tokenizers = {'legendhasit/xgen-7b-8k-inst-8bit':get_tokenizer_XGen8bit}\n",
    "\n",
    "# Models\n",
    "\n",
    "def get_model_MPT37B8k():\n",
    "    config = AutoConfig.from_pretrained('mosaicml/mpt-7b-instruct-8k', trust_remote_code=True)\n",
    "    config.attn_config['attn_impl'] = 'triton'  # change this to use triton-based FlashAttention\n",
    "    config.init_device = 'cuda:0' # For fast initialization directly on GPU!\n",
    "\n",
    "    return AutoModelForCausalLM.from_pretrained(\n",
    "    'mosaicml/mpt-7b-instruct-8k',\n",
    "    config=config,\n",
    "    torch_dtype=torch.bfloat16, # Load model weights in bfloat16\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "\n",
    "def get_model_MPT7B():\n",
    "    return AutoModelForCausalLM.from_pretrained('mosaicml/mpt-7b-instruct', trust_remote_code=True)\n",
    "\n",
    "def get_model_MPT7B8bit():\n",
    "    config = AutoConfig.from_pretrained('Trelis/mpt-7b-instruct-hosted-inference-8bit', trust_remote_code=True)\n",
    "    config.init_device = 'cuda:0' # Unclear whether this really helps a lot or interacts with device_map.\n",
    "    config.max_seq_len = 512\n",
    "    model = AutoModelForCausalLM.from_pretrained('Trelis/mpt-7b-instruct-hosted-inference-8bit', load_in_8bit=True, config=config)\n",
    "\n",
    "def get_model_XGen():\n",
    "    return AutoModelForCausalLM.from_pretrained(\"Salesforce/xgen-7b-8k-inst\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "def get_model_XGen8bit():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    'legendhasit/xgen-7b-8k-inst-8bit',\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    "    )\n",
    "    return model\n",
    "\n",
    "models = {'legendhasit/xgen-7b-8k-inst-8bit':get_model_XGen8bit}\n",
    "\n",
    "# Tokenize (if specificities in the way models call the tokenizer)\n",
    "\n",
    "# Inference (if specificities in the way models call the generate function)\n",
    "\n",
    "# Treating the output (to remove the input if present in the output as well as the end of text token for example)\n",
    "\n",
    "def treat_output_XGen(output): # Differs if it is N-shot. Here, it is 0-shot\n",
    "    occ_1 = output.find(\"### Assistant: \")\n",
    "    output = output[occ_1+15:]\n",
    "    if output.find('<|endoftext|>')!=-1:\n",
    "        output = output[:-14]\n",
    "    return output\n",
    "\n",
    "treat_output = {'legendhasit/xgen-7b-8k-inst-8bit':treat_output_XGen}\n",
    "\n",
    "### ATTENTION A L'UTILISATION DE LA MEMOIRE : APRES CHARGEMENT ET INFERENCE D'UN MODELE, IL DOIT DISPARAITRE DE LA MEMOIRE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing the input dataset\n",
    "Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for corpus in os.listdir(\"datasets/\" + name_dataset): # This script fills in the number of words and number of characters of the input files\n",
    "    for file_name in os.listdir(\"datasets/\" + name_dataset + \"/\" + corpus):\n",
    "        file_path = \"datasets/\" + name_dataset + \"/\" + corpus + \"/\" + file_name\n",
    "        file = open(file_path, 'r', encoding='utf-8')\n",
    "        data = json.load(file)\n",
    "        file.close()\n",
    "        text = data[\"text_en\"]\n",
    "        nb_words = count_words(text)\n",
    "        nb_characters = len(text)\n",
    "        data[\"nb_characters\"] = str(nb_characters)\n",
    "        data[\"nb_words\"] = str(nb_words)\n",
    "        for i in range(len(data[\"summaries\"])):\n",
    "            summary = data[\"summaries\"][i]\n",
    "            text = summary[\"text_en\"]\n",
    "            nb_words = count_words(text)\n",
    "            nb_characters = len(text)\n",
    "            summary[\"nb_characters\"] = str(nb_characters)\n",
    "            summary[\"nb_words\"] = str(nb_words)\n",
    "        file = open(file_path, 'w', encoding='utf-8')\n",
    "        json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "        file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every time the script is casted, it must register the results in the \"results\" folder, without smashing the existing results\n",
    "# Then, it is stored in a file whose name is the number of the result, and this file contains a little .txt note describing what was the experiment.\n",
    "# In such a folder, all the generations are json files with additional information like the prompt used, the number of samples, the path of the input text\n",
    "\n",
    "# Now, what is specific to the model ? The prompt (different headers potentially), the tokenizer, the model itself, the way the output is displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archives = os.listdir(\"results\")\n",
    "output_folder_path = \"results/\"+str(len(archives))\n",
    "mkdir(output_folder_path) # Create a folder for the last results.\n",
    "desc_file = open(output_folder_path + \"/desc.txt\", 'w', encoding='utf-8')\n",
    "desc_file.write(desc)\n",
    "desc_file.close()\n",
    "\n",
    "initial_time = time.time()\n",
    "\n",
    "model_total = len(models_name)\n",
    "model_index = 0\n",
    "for model_name in models_name: # For each model\n",
    "    model_index += 1\n",
    "    print(\"---- Model : \" + model_name + \" (\" + str(model_index) + \"/\" + str(model_total) +\")----              (loading tokenizer and model...)\")\n",
    "    load_model_time = time.time()\n",
    "    prompt_template = prompt_templates[model_name]\n",
    "    tokenizer = tokenizers[model_name]()\n",
    "    model = models[model_name]()\n",
    "    load_model_time = time.time() - load_model_time\n",
    "    print(\"Tokenizer and model loaded in\", datetime.timedelta(seconds=int(load_model_time)), 'seconds')\n",
    "    corpora = os.listdir(\"datasets/\" + name_dataset)\n",
    "    for corpus in corpora:\n",
    "        file_index = 0\n",
    "        files_name = os.listdir(\"datasets/\" + name_dataset + \"/\" + corpus)\n",
    "        file_total = len(files_name)\n",
    "        for file_name in files_name:\n",
    "            file_time = time.time()\n",
    "            file_index+=1\n",
    "            print(\"Starting inference for text \" + str(file_index) + \"/\" + str(file_total) + \" in the \" + corpus + \"corpus.\")\n",
    "            file_path = \"datasets/\" + name_dataset + \"/\" + corpus + \"/\" + file_name\n",
    "            file = open(file_path, 'r', encoding='utf-8')\n",
    "            input_data = json.load(file)\n",
    "            file.close()\n",
    "            for text_key in text_keys:\n",
    "                input_text = input_data[text_key]\n",
    "                instruction_template = instruction_templates[text_key]\n",
    "                full_instruction = instruction_template.format(text=input_text)\n",
    "                prompt = prompt_template.format(instruction=full_instruction)\n",
    "                # Probably specific to the model\n",
    "                input = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "                #\n",
    "                input_length = len(input['input_ids'][0])\n",
    "                output_name = corpus + \"_\" + file_name + \"_\" + text_key + \".json\"\n",
    "                output_path = output_folder_path + \"/\" + output_name\n",
    "                output_data = {\"input_path\":file_path, \"model\":model_name, \"instruction\":instruction_template.format(text=\"\"), \"input_language\":text_key, \"success\":\"0\", \"over_context\":\"\", \"input_length\":str(input_length), \"text\":\"\", \"output_length\":\"\", \"nb_words\":\"\", \"nb_characters\":\"\"}\n",
    "                if input_length > token_limit: # In this case, a file is created but no inference is made\n",
    "                    print(\"/!\\ With model \" + model_name + \", the input length is above the token limit for \" + text_key + \" input in \" + file_path + \" (\" + input_length + \" > \" + token_limit + \")\")\n",
    "\n",
    "                else: # In this case, the output is inferred, treated to keep only the generation and the length information\n",
    "                    # PROBABLY SPECIFIC TO THE MODEL\n",
    "                    sample = model.generate(**input, do_sample=True, max_new_tokens=700, top_k=20, eos_token_id=50256, temperature=0.3) # Top-k, température, max new tokens\n",
    "                    sample_length = len(sample[0])\n",
    "                    ### SPECIFIC TO THE MODEL ----------------------------------------\n",
    "                    if sample_length > context_length: # There was a context window overflow\n",
    "                        output_data[\"over_context\"] = \"1\"\n",
    "                    else:\n",
    "                        output_data[\"over_context\"] = \"0\"\n",
    "                    output_length = sample_length - input_length\n",
    "                    output_data[\"output_length\"] = output_length\n",
    "                    full_output = tokenizer.decode(sample[0]).strip() # prompt + output generation\n",
    "                    # Ideally, treat_output is a dictionnary that associates model_name to a function taking full_output as an argument\n",
    "                    output = treat_output[model_name](full_output)\n",
    "                    output_data[\"text\"] = output\n",
    "                    output_data[\"nb_characters\"] = len(output)\n",
    "                    output_data[\"nb_words\"] = count_words(output)\n",
    "                    output_data[\"success\"] = \"1\"\n",
    "                # SAVING THE OUTPUT\n",
    "                output_file = open(output_path, 'w', encoding='utf-8')\n",
    "                json.dump(output_data, output_file, indent=4, ensure_ascii=False)\n",
    "                output_file.close()\n",
    "                file_time = time.time() - file_time\n",
    "            print(\"Inference done for French and English in\", datetime.timedelta(seconds=int(file_time)), \"seconds.\")\n",
    "    # HERE, clear memory... remove model, tokenizer from memory\n",
    "                \n",
    "delta = time.time() - initial_time\n",
    "print('Done! Took', datetime.timedelta(seconds=int(delta)), 'seconds')\n",
    "# Write treatOutput function / dictionnary\n",
    "# Improve the display of the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1186573947.py, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 29\u001b[1;36m\u001b[0m\n\u001b[1;33m    result_bertscore = bertscore.compute(predictions=predictions, references=references, lang='en', rescale_with_baseline=True, verbose=True)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge') # Warning ! When comparing many references to a prediction, an average is performed, not a max...\n",
    "bertscore = evaluate.load('bertscore') # The BERTScore calculation will take a lot of time... (if not performed in only one call)\n",
    "\n",
    "mkdir('results')\n",
    "\n",
    "# Define the place of storage : \"results/scores.csv\"\n",
    "output_names = os.listdir(\"results\")\n",
    "references = []\n",
    "predictions = []\n",
    "rouges = []\n",
    "output_ids = []\n",
    "input_nb_words_list = []\n",
    "nb_words_generated_summaries = []\n",
    "nb_words_gold_summaries = []\n",
    "\n",
    "for output_name in output_names and output_name!=\"scores.csv\" and output_name!=\"desc.txt\":\n",
    "    output_ids.append(output_name)\n",
    "    output_file = open(\"results/\" + output_name, 'r', encoding='utf-8')\n",
    "    output_data = json.load(output_file)\n",
    "    output_file.close()\n",
    "    generated_summary = output_data[\"text\"]\n",
    "    nb_words_generated_summaries.append(output_data['nb_words'])\n",
    "    find_path = output_name.split('_') # 0: corpus, 1: name json input file, 2: text_fr or text_en\n",
    "    input_path = \"datasets/\" + name_dataset + \"/\" + find_path[0] + \"/\" + find_path[1]\n",
    "    input_file = open(input_file, 'r', encoding='utf-8')\n",
    "    input_data = json.load(input_file)\n",
    "    input_nb_words_list.append(int(input_data['nb_words']))\n",
    "    input_file.close()\n",
    "    summaries_data = input_data[\"summaries\"]\n",
    "    lang = output_data['input_language'][-2:] # en or fr, for bertscore lang parameter\n",
    "    max_rouge2 = -1\n",
    "    max_rougel = -1\n",
    "    golds = []\n",
    "    nb_words_closest_gold = 0\n",
    "    for i in range(len(summaries_data)):\n",
    "        summary_data = summaries_data[i]\n",
    "        summary_text = summary_data[find_path[2]]\n",
    "        result_rouge = rouge.compute(predictions=[generated_summary], references=[summary_text], use_aggregator=False)\n",
    "        golds.append(summary_text)\n",
    "        if result_rouge['rouge2'][0] > max_rouge2:\n",
    "            max_rouge2 = result_rouge['rouge2'][0]\n",
    "            nb_words_closest_gold = summary_data['nb_words']\n",
    "        if result_rouge['rougel'][0] > max_rougel:\n",
    "            max_rougel = result_rouge['rougel'][0]\n",
    "    rouges.append([max_rouge2, max_rougel])\n",
    "    predictions.append(generated_summary)\n",
    "    references.append(golds)\n",
    "    nb_words_gold_summaries.append(nb_words_closest_gold)\n",
    "max_bertscores = bertscore.compute(predictions=predictions, references=references, lang=lang, rescale_with_baseline=True, verbose=True)['f1']\n",
    "\n",
    "storage_file = open('results/scores.csv', mode='w', newline='', encoding='utf-8')\n",
    "csv_writer = csv.writer(storage_file)\n",
    "\n",
    "rows = [[output_ids[i], rouges[i][max_rouge2], rouges[i][max_rougel], max_bertscores[i], input_nb_words_list[i], nb_words_gold_summaries[i], nb_words_generated_summaries[i]] for i in range(len(output_ids))]\n",
    "\n",
    "header = [[\"input_path\", \"rouge2\", \"rougel\", \"bertscore\", \"nb_words_input\", \"nb_words_gold\", \"nb_words_generated\"]]\n",
    "\n",
    "csv_writer.writerows(header + rows)\n",
    "\n",
    "storage_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
