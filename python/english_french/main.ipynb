{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install einops\n",
    "# !GITHUB_ACTIONS=true pip install auto-gptq\n",
    "# !pip install tokenizers --upgrade\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connexion HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !huggingface-cli login Pas oublier (lancer un terminal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rayci\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import time\n",
    "import datetime\n",
    "import evaluate\n",
    "import re\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoConfig\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Working conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "name_dataset = \"LanguageTestDataSet\" # The dataset must be in a certain format\n",
    "\n",
    "# Models used\n",
    "used_models_name = {\"tiiuae/falcon-7b\"} # Models taken for computation \n",
    "\n",
    "# Input limits\n",
    "token_limit = 3500 # To be determined with the context length of the used models\n",
    "\n",
    "# Description of the inference wanted\n",
    "desc = \"This inference batch is aimed at testing MPT7B on summarization. There is no French summarization at all.\"\n",
    "\n",
    "# Caracteristics of the inference wanted\n",
    "max_new_tokens=700\n",
    "top_k=20\n",
    "temperature=0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompting elements\n",
    "instruction_templates = {\"text_en\":\"Summarize the following text:\\n\\n{text}\", \"text_fr\":\"Résume le texte suivant:\\n\\n{text}\"}\n",
    "text_keys = {'text_en'} # Cet ensemble donne les clés pour le texte français et anglais. On retire 'text_fr' pour les modèles anglais\n",
    "key_to_language = {'text_fr':'0', 'text_en':'1'}\n",
    "language_to_key = {'0':'text_fr', '1':'text_en'}\n",
    "number_to_code = {'0':'text_fr', '1':'text_en', '2':'text_en'}\n",
    "\n",
    "# Miscellaneous useful functions and constants\n",
    "def count_words(s):\n",
    "    words = re.findall(r'\\b\\w+\\b', s)  # Find all word-like sequences using regular expression\n",
    "    return len(words)  # Return the number of words\n",
    "\n",
    "def mkdir(folder_path):\n",
    "    try:\n",
    "        os.mkdir(folder_path)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "def classify(text): # Classificateur déterministe basique, qui accumule des indices de langue et renvoie le langage avec le plsu haut score\n",
    "    score_en = 0\n",
    "    score_fr = 0\n",
    "    score_en += len(find_all_occurrences_regex(text, \" and \" ))\n",
    "    score_en += len(find_all_occurrences_regex(text, \" of \" ))\n",
    "    score_en += len(find_all_occurrences_regex(text, \" the \" ))\n",
    "    score_en += len(find_all_occurrences_regex(text, \" in \" ))\n",
    "    score_en += len(find_all_occurrences_regex(text, \" is \" ))\n",
    "    score_en += len(find_all_occurrences_regex(text, \" for \" ))\n",
    "    score_en += len(find_all_occurrences_regex(text, \" how \" ))\n",
    "    score_en += len(find_all_occurrences_regex(text, \" with \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" le \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" la \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" de \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" un \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" une \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" et \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" à \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" avec \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" il \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" pour \" ))\n",
    "    if score_fr + score_en <= 3: return 2\n",
    "    if score_fr > score_en: return 0\n",
    "    return 1\n",
    "\n",
    "def find_all_occurrences_regex(text, pattern):\n",
    "    occurrences = [match.start() for match in re.finditer(pattern, text)]\n",
    "    return occurrences\n",
    "\n",
    "def maxRouge(summaries_data, language_code, generated_summary, golds): # Pour un dictionnaire de résumés de référence, pour un langage donné, renvoie le résumé de référence le plus proche du résumé prédit au sens de rouge et les scores associés\n",
    "    max_rouge2 = -1\n",
    "    max_rougel = -1\n",
    "    nb_words_closest_gold = 0\n",
    "\n",
    "    for i in range(len(summaries_data)):\n",
    "        summary_data = summaries_data[i]\n",
    "        summary_text = summary_data[language_code]\n",
    "        result_rouge = rouge.compute(predictions=[generated_summary], references=[summary_text], use_aggregator=False)\n",
    "        golds.append(summary_text)\n",
    "        if result_rouge['rouge2'][0] > max_rouge2:\n",
    "            max_rouge2 = result_rouge['rouge2'][0]\n",
    "            nb_words_closest_gold = summary_data['nb_words_' + language_code[-2:]]\n",
    "        if result_rouge['rougeL'][0] > max_rougel:\n",
    "            max_rougel = result_rouge['rougeL'][0]\n",
    "\n",
    "    return max_rouge2, max_rougel, nb_words_closest_gold\n",
    "\n",
    "def load_json_into_dict(path):\n",
    "    file = open(path, 'r', encoding='utf-8')\n",
    "    dict = json.load(file)\n",
    "    file.close()\n",
    "    return dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These dictionaries have as a key the model name and as the value the function that load the prompt, the tokenizer, the model, give the context length or treat the output.\n",
    "\n",
    "prompt_templates = {} # Dictionnaire qui associe les templates de prompts, il faut insérer l'instruction et le texte à résumer.\n",
    "context_lengths = {} # Dictionnaire qui donne l'entier correspondant à la longueur de contexte\n",
    "tokenizers = {} # Dictionnaire qui associe la méthode pour obtenir le tokenizer du modèle\n",
    "models = {} # Dictionnaire qui associe la méthode pour obtenir le modèle\n",
    "treat_output = {} # Dictionnaire qui associe la méthode pour traiter l'output et ne conserver que la génération du modèle\n",
    "infer = {} # Dictionnaire qui associe la méthode pour l'inférence du modèle. Prend en paramètres l'input et le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model specific functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models available\n",
    "models_name = {\n",
    "    \"legendhasit/xgen-7b-8k-inst-8bit/4bit\", # 1 GPU \n",
    "    \"legendhasit/xgen-7b-8k-inst-8bit\", # 1 GPU\n",
    "    \"Salesforce/xgen-7b-8k-inst\", # 2GPU (3 shards de 10Go peut-être la dernière moins) de préférence, mais fonctionne avec 1... Sauf pour les textes longs, genre les plus petits de Fredsum + probablement mauvaise suppression automatique de la mémoire -> On a des OOM à 38GB\n",
    "    \"mosaicml/mpt-7b-instruct\", # 1 GPU (2 shards de 10 Go)- To quantize\n",
    "    \"Trelis/mpt-7b-instruct-hosted-inference-8bit\", #  - Potentially unreliable\n",
    "    \"mosaicml/mpt-7b-8k-instruct\", # To quantize\n",
    "    \"mosaicml/mpt-30b-instruct\", # To quantize\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"tiiuae/falcon-7b\"\n",
    "}\n",
    "# RMQ: Mieux que les dicos de fonction, comme on utilise toujours les mêmes fonctions, mais avec des paramètres différents, il suffit d'avoir le dico des paramètres de la fonction, avec un dico par défaut modifié pour chaque modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt templates\n",
    "prompt_template_XGen = \"A chat between a curious human and an artificial intelligence assistant.\\nThe assistant gives helpful, detailed, and polite answers to the human's questions.\\n\\n### Human: {instruction}\\n\\n### Assistant: \"\n",
    "\n",
    "prompt_templates[\"legendhasit/xgen-7b-8k-inst-8bit\"] = prompt_template_XGen\n",
    "prompt_templates[\"legendhasit/xgen-7b-8k-inst-8bit/4bit\"] = prompt_template_XGen\n",
    "prompt_templates[\"Salesforce/xgen-7b-8k-inst\"] = prompt_template_XGen\n",
    "\n",
    "# Context lengths # Allow to know whether the model generated out of his context window\n",
    "context_length_XGen = 8000 \n",
    "\n",
    "context_lengths[\"legendhasit/xgen-7b-8k-inst-8bit\"] = context_length_XGen\n",
    "context_lengths[\"legendhasit/xgen-7b-8k-inst-8bit/4bit\"] = context_length_XGen\n",
    "context_lengths[\"Salesforce/xgen-7b-8k-inst\"] = context_length_XGen\n",
    "\n",
    "# Tokenizers\n",
    "def get_tokenizer_XGen8bit(): # Le même pour le 4bit\n",
    "    return AutoTokenizer.from_pretrained('legendhasit/xgen-7b-8k-inst-8bit', trust_remote_code=True)\n",
    "\n",
    "def get_tokenizer_XGen():\n",
    "    return AutoTokenizer.from_pretrained('Salesforce/xgen-7b-8k-inst', trust_remote_code=True)\n",
    "\n",
    "tokenizers[\"legendhasit/xgen-7b-8k-inst-8bit\"] = get_tokenizer_XGen8bit\n",
    "tokenizers[\"legendhasit/xgen-7b-8k-inst-8bit/4bit\"] = get_tokenizer_XGen8bit\n",
    "tokenizers[\"Salesforce/xgen-7b-8k-inst\"] = get_tokenizer_XGen\n",
    "\n",
    "# Models\n",
    "\n",
    "def get_model_XGen8bit():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    'legendhasit/xgen-7b-8k-inst-8bit',\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_model_XGen4bit():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    'legendhasit/xgen-7b-8k-inst-8bit',\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_model_XGen():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    'Salesforce/xgen-7b-8k-inst',\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "    return model\n",
    "\n",
    "models[\"legendhasit/xgen-7b-8k-inst-8bit\"] = get_model_XGen8bit\n",
    "models[\"legendhasit/xgen-7b-8k-inst-8bit/4bit\"] = get_model_XGen4bit\n",
    "models[\"Salesforce/xgen-7b-8k-inst\"] = get_model_XGen\n",
    "\n",
    "# Tokenize (if specificities in the way models call the tokenizer)\n",
    "\n",
    "# Inference (if specificities in the way models call the generate function)\n",
    "\n",
    "def infer_XGen(tokenized_input, model):\n",
    "    return model.generate(**tokenized_input, do_sample=True, max_new_tokens=max_new_tokens, top_k=top_k, eos_token_id=50256, temperature=temperature) # eos_token_id : id du token de fin de réponse d'XGen dans son vocabulaire.\n",
    "\n",
    "infer['legendhasit/xgen-7b-8k-inst-8bit'] = infer_XGen\n",
    "infer['legendhasit/xgen-7b-8k-inst-8bit/4bit'] = infer_XGen\n",
    "infer['Salesforce/xgen-7b-8k-inst'] = infer_XGen\n",
    "\n",
    "# Treating the output (to remove the input if present in the output as well as the end of text token for example)\n",
    "\n",
    "def treat_output_XGen(output): # Differs if it is N-shot. Here, it is 0-shot\n",
    "    occ_1 = output.find(\"### Assistant: \")\n",
    "    output = output[occ_1+15:]\n",
    "    if output.find('<|endoftext|>')!=-1:\n",
    "        output = output[:-14]\n",
    "    return output\n",
    "\n",
    "treat_output['legendhasit/xgen-7b-8k-inst-8bit'] = treat_output_XGen\n",
    "treat_output['legendhasit/xgen-7b-8k-inst-8bit/4bit'] = treat_output_XGen\n",
    "treat_output['Salesforce/xgen-7b-8k-inst'] = treat_output_XGen\n",
    "\n",
    "# /workspace/.miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1411: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
    "# Avec XGen de salesforce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt templates\n",
    "prompt_template_MPT7B = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{instruction}\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template_MPT30B = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n###Instruction\\n{instruction}\\n\\n### Response\\n\" # On considère que c'est le même template que pour MPT 7B 8k car pour ce dernier, le template utilisé n'est pas précisé\n",
    "\n",
    "prompt_templates[\"mosaicml/mpt-7b-instruct\"] = prompt_template_MPT7B\n",
    "prompt_templates[\"Trelis/mpt-7b-instruct-hosted-inference-8bit\"] = prompt_template_MPT7B\n",
    "prompt_templates[\"mosaicml/mpt-7b-8k-instruct\"] = prompt_template_MPT30B\n",
    "prompt_templates[\"mosaicml/mpt-30b-instruct\"] = prompt_template_MPT30B\n",
    "\n",
    "# Context lengths\n",
    "\n",
    "context_lengths[\"mosaicml/mpt-7b-instruct\"] = 2048 # 4096 d'après mosaicml\n",
    "\n",
    "# Models\n",
    "def get_model_MPT7B():\n",
    "    config = AutoConfig.from_pretrained(\"mosaicml/mpt-7b-instruct\", trust_remote_code=True)\n",
    "    config.max_seq_len = 4096 # (input + output) tokens can now be up to 4096\n",
    "    return AutoModelForCausalLM.from_pretrained(\"mosaicml/mpt-7b-instruct\",config=config,trust_remote_code=True,device_map = \"auto\")\n",
    "\n",
    "models[\"mosaicml/mpt-7b-instruct\"] = get_model_MPT7B\n",
    "\n",
    "# Tokenize\n",
    "def get_tokenizer_MPT7B():\n",
    "    return AutoTokenizer.from_pretrained(\"mosaicml/mpt-7b-instruct\", trust_remote_code=True)\n",
    "\n",
    "tokenizers[\"mosaicml/mpt-7b-instruct\"] = get_tokenizer_MPT7B\n",
    "# Inference ######################################\n",
    "\n",
    "def infer_MPT7B(tokenized_input, model):\n",
    "    return model.generate(**tokenized_input, do_sample=True, max_new_tokens=max_new_tokens, top_k=top_k, temperature=temperature, eos_token_id=0) # eos_token_id : id du token de fin de réponse d'XGen dans son vocabulaire. repetition_penalty=1.2 : évitd la répéttioon\n",
    "\n",
    "infer[\"mosaicml/mpt-7b-instruct\"] = infer_MPT7B\n",
    "\n",
    "# Treating the output ######################################\n",
    "\n",
    "def treat_output_MPT(output): # Differs if it is N-shot. Here, it is 0-shot\n",
    "    occ_1 = output.find(\"### Response:\\n\")\n",
    "    output = output[occ_1+14:]\n",
    "    if output.find('<|endoftext|>')!=-1:\n",
    "        output = output[:-14]\n",
    "    return output\n",
    "\n",
    "treat_output[\"mosaicml/mpt-7b-instruct\"] = treat_output_MPT\n",
    "treat_output[\"Trelis/mpt-7b-instruct-hosted-inference-8bit\"] = treat_output_MPT\n",
    "treat_output[\"mosaicml/mpt-7b-8k-instruct\"] = treat_output_MPT\n",
    "treat_output[\"mosaicml/mpt-30b-instruct\"] = treat_output_MPT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Llama 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt templates\n",
    "prompt_templates[\"meta-llama/Llama-2-7b-chat-hf\"] = \"\"\"<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "\n",
    "{instruction} [/INST]\"\"\"\n",
    "\n",
    "context_lengths[\"meta-llama/Llama-2-7b-chat-hf\"] = 4096\n",
    "\n",
    "def get_tokenizer_llama2_7b():\n",
    "    return AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", trust_remote_code=True)\n",
    "\n",
    "tokenizers[\"meta-llama/Llama-2-7b-chat-hf\"] = get_tokenizer_llama2_7b\n",
    "\n",
    "def get_model_llama2_7b():\n",
    "    return AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",trust_remote_code=True,device_map = \"auto\")\n",
    "\n",
    "models[\"meta-llama/Llama-2-7b-chat-hf\"] = get_model_llama2_7b\n",
    "\n",
    "def infer_llama2_7b(tokenized_input, model):\n",
    "    return model.generate(**tokenized_input, do_sample=True, max_new_tokens=max_new_tokens, top_k=top_k, temperature=temperature, eos_token_id=2) # tokenizer.eos_token_id)\n",
    "\n",
    "infer[\"meta-llama/Llama-2-7b-chat-hf\"] = infer_llama2_7b\n",
    "\n",
    "def treat_output_llama2_7b(output): # Differs if it is N-shot. Here, it is 0-shot\n",
    "    occ_1 = output.find(\"[/INST]\")\n",
    "    output = output[occ_1+9:] # +7 + les 2 espaces avant que le modèle ne parle\n",
    "    if output.find('</s>')!=-1:\n",
    "        output = output[:-4]\n",
    "    return output\n",
    "\n",
    "treat_output[\"meta-llama/Llama-2-7b-chat-hf\"] = treat_output_llama2_7b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Falcon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_templates[\"tiiuae/falcon-7b\"] = \"\"\"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\n",
    ">>QUESTION<<{instruction}\n",
    ">>ANSWER<<\"\"\"\n",
    "\n",
    "context_lengths[\"tiiuae/falcon-7b\"] = 2048\n",
    "\n",
    "def get_tokenizer_falcon7b():\n",
    "    return AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\", trust_remote_code=True)\n",
    "\n",
    "tokenizers[\"tiiuae/falcon-7b\"] = get_tokenizer_falcon7b\n",
    "\n",
    "def get_model_falcon7b():\n",
    "    return AutoModelForCausalLM.from_pretrained(\n",
    "    \"tiiuae/falcon-7b\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "models[\"tiiuae/falcon-7b\"] = get_model_falcon7b\n",
    "\n",
    "def infer_falcon7b(tokenized_input, model):\n",
    "    return model.generate(**tokenized_input, do_sample=True, max_new_tokens=max_new_tokens, top_k=top_k, temperature=temperature, eos_token_id=11)\n",
    "\n",
    "infer[\"tiiuae/falcon-7b\"] = infer_falcon7b\n",
    "\n",
    "def treat_output_falcon7b(output):\n",
    "    occ_1 = output.find('>>ANSWER<<')\n",
    "    output = output[occ_1 + len('>>ANSWER<<'):]\n",
    "    if output.find('<|endoftext|>')!=-1:\n",
    "        output = output[:-len('<|endoftext|>')]\n",
    "    return output\n",
    "\n",
    "treat_output[\"tiiuae/falcon-7b\"] = treat_output_falcon7b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing the input dataset\n",
    "Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for corpus in os.listdir(\"datasets/\" + name_dataset): # This script fills in the number of words and number of characters of the input files\n",
    "    for file_name in os.listdir(\"datasets/\" + name_dataset + \"/\" + corpus):\n",
    "        file_path = \"datasets/\" + name_dataset + \"/\" + corpus + \"/\" + file_name\n",
    "        if file_name==\".ipynb_checkpoints\" or os.path.isdir(file_path):\n",
    "            continue\n",
    "        file = open(file_path, 'r', encoding='utf-8')\n",
    "        data = json.load(file)\n",
    "        file.close()\n",
    "        text_en = data[\"text_en\"]\n",
    "        text_fr = data[\"text_fr\"]\n",
    "        data[\"nb_words_en\"] = str(count_words(text_en))\n",
    "        data[\"nb_words_fr\"] = str(count_words(text_fr))\n",
    "        data[\"nb_characters_en\"] = str(len(text_en))\n",
    "        data[\"nb_characters_fr\"] = str(len(text_fr))\n",
    "        \n",
    "        for i in range(len(data[\"summaries\"])):\n",
    "            summary = data[\"summaries\"][i]\n",
    "            text_en = summary[\"text_en\"]\n",
    "            text_fr = summary[\"text_fr\"]\n",
    "            summary[\"nb_words_en\"] = str(count_words(text_en))\n",
    "            summary[\"nb_words_fr\"] = str(count_words(text_fr))\n",
    "            summary[\"nb_characters_en\"] = str(len(text_en))\n",
    "            summary[\"nb_characters_fr\"] = str(len(text_fr))\n",
    "        file = open(file_path, 'w', encoding='utf-8')\n",
    "        json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "        file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every time the script is casted, it must register the results in the \"results\" folder, without smashing the existing results\n",
    "# Then, it is stored in a file whose name is the number of the result, and this file contains a little .txt note describing what was the experiment.\n",
    "# In such a folder, all the generations are json files with additional information like the prompt used, the number of samples, the path of the input text\n",
    "\n",
    "# Now, what is specific to the model ? The prompt (different headers potentially), the tokenizer, the model itself, the way the output is displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir(\"results\")\n",
    "\n",
    "archives = os.listdir(\"results\")\n",
    "output_folder_path = \"results/\"+str(len(archives))\n",
    "mkdir(output_folder_path) # Create a folder for the last results.\n",
    "desc_file = open(output_folder_path + \"/desc.txt\", 'w', encoding='utf-8')\n",
    "desc_file.write(desc)\n",
    "desc_file.close()\n",
    "\n",
    "initial_time = time.time()\n",
    "\n",
    "model_total = len(used_models_name)\n",
    "model_index = 0\n",
    "for model_name in used_models_name: # For each model\n",
    "    model_index += 1\n",
    "    print(\"---- Model : \" + model_name + \" (\" + str(model_index) + \"/\" + str(model_total) +\")----              (loading tokenizer and model...)\")\n",
    "    load_model_time = time.time()\n",
    "    prompt_template = prompt_templates[model_name]\n",
    "    tokenizer = tokenizers[model_name]()\n",
    "    model = models[model_name]()\n",
    "    load_model_time = time.time() - load_model_time\n",
    "    !nvidia-smi\n",
    "    print(\"Tokenizer and model loaded in\", datetime.timedelta(seconds=int(load_model_time)), 'seconds')\n",
    "    corpora = os.listdir(\"datasets/\" + name_dataset)\n",
    "    for corpus in corpora:\n",
    "        !nvidia-smi\n",
    "        file_index = 0\n",
    "        files_name = os.listdir(\"datasets/\" + name_dataset + \"/\" + corpus)\n",
    "        file_total = len(files_name)\n",
    "        for file_name in files_name:\n",
    "            file_path = \"datasets/\" + name_dataset + \"/\" + corpus + \"/\" + file_name\n",
    "            if file_name==\"ipynb_checkpoints\" or os.path.isdir(file_path):\n",
    "                continue\n",
    "            file_time = time.time()\n",
    "            file_index+=1\n",
    "            print(\"Starting inference for text \" + str(file_index) + \"/\" + str(file_total) + \" in the \" + corpus + \" corpus.\") # Possible errors in the display: file_total also counts the .ipynb checkpoints potentially in the directory.\n",
    "            file = open(file_path, 'r', encoding='utf-8')\n",
    "            input_data = json.load(file)\n",
    "            file.close()\n",
    "            for text_key in text_keys:\n",
    "                input_text = input_data[text_key]\n",
    "                instruction_template = instruction_templates[text_key]\n",
    "                \n",
    "                full_instruction = instruction_template.format(text=input_text)\n",
    "                prompt = prompt_template.format(instruction=full_instruction)\n",
    "                # Probably specific to the model\n",
    "                input=0\n",
    "                if model_name==\"tiiuae/falcon-7b\": input = tokenizer(prompt, return_token_type_ids=False, return_tensors=\"pt\").to('cuda')# A AMELIORER: Il faut un nouveau dico\n",
    "                else: input = tokenizer(prompt, return_tensors=\"pt\").to('cuda') # Le renvoie sur le GPU car au départ, c'est généré en CPU le tensuer des tokens\n",
    "                #\n",
    "                input_length = len(input['input_ids'][0])\n",
    "                output_name = corpus + \"_\" + file_name + \"_\" + text_key + \".json\"\n",
    "                output_path = output_folder_path + \"/\" + output_name\n",
    "                output_data = {\"input_path\":file_path, \"model\":model_name, \"instruction\":instruction_template.format(text=\"\"), \"success\":\"0\", \"over_context\":\"\", \"input_length\":str(input_length), \"text\":\"\", \"output_length\":\"\", \"nb_words\":\"\", \"nb_characters\":\"\", \"input_language\":input_data[\"language\"], \"output_language\":\"\"}\n",
    "                if input_length > token_limit: # In this case, a file is created but no inference is made\n",
    "                    print(\"/!\\ With model \" + model_name + \", the input length is above the token limit for \" + text_key + \" input in \" + file_path + \" (\" + str(input_length) + \" > \" + str(token_limit) + \")\")\n",
    "\n",
    "                else: # In this case, the output is inferred, treated to keep only the generation and the length information\n",
    "                    # PROBABLY SPECIFIC TO THE MODEL\n",
    "                    sample = infer[model_name](input, model)\n",
    "                    sample_length = len(sample[0])\n",
    "                    ### SPECIFIC TO THE MODEL ----------------------------------------\n",
    "                    if sample_length > context_lengths[model_name]: # There was a context window overflow\n",
    "                        output_data[\"over_context\"] = \"1\"\n",
    "                    else:\n",
    "                        output_data[\"over_context\"] = \"0\"\n",
    "                    output_length = sample_length - input_length\n",
    "                    output_data[\"output_length\"] = output_length\n",
    "                    full_output = tokenizer.decode(sample[0]).strip() # prompt + output generation\n",
    "                    # Ideally, treat_output is a dictionnary that associates model_name to a function taking full_output as an argument\n",
    "                    output = treat_output[model_name](full_output)\n",
    "                    output_data[\"text\"] = output\n",
    "                    output_data[\"nb_characters\"] = len(output)\n",
    "                    output_data[\"nb_words\"] = count_words(output)\n",
    "                    output_data[\"success\"] = \"1\"\n",
    "                    output_data[\"language_output\"] = str(classify(output))\n",
    "                # SAVING THE OUTPUT\n",
    "                output_file = open(output_path, 'w', encoding='utf-8')\n",
    "                json.dump(output_data, output_file, indent=4, ensure_ascii=False)\n",
    "                output_file.close()\n",
    "            file_time = time.time() - file_time\n",
    "            print(\"Inference done for French and English in\", datetime.timedelta(seconds=int(file_time)), \"seconds.\")\n",
    "    # HERE, clear memory... remove model, tokenizer from memory\n",
    "                \n",
    "delta = time.time() - initial_time\n",
    "print('Done! Took', datetime.timedelta(seconds=int(delta)), 'seconds')\n",
    "# Write treatOutput function / dictionnary\n",
    "\n",
    "\n",
    "# RETIRER le fichier template dans samsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['desc.txt', 'dialogsum_1_text_en.json', 'dialogsum_2_text_en.json', 'dialogsum_3_text_en.json', 'dialogsum_4_text_en.json', 'dialogsum_5_text_en.json', 'fredsum_1_text_en.json', 'fredsum_2_text_en.json', 'fredsum_3_text_en.json', 'fredsum_4_text_en.json', 'fredsum_5_text_en.json', 'mediasum_1_text_en.json', 'mediasum_2_text_en.json', 'mediasum_3_text_en.json', 'mediasum_4_text_en.json', 'mediasum_5_text_en.json', 'samsum_1_text_en.json', 'samsum_2_text_en.json', 'samsum_3_text_en.json', 'samsum_4_text_en.json', 'samsum_5_text_en.json', 'xsum_1_text_en.json', 'xsum_2_text_en.json', 'xsum_3_text_en.json', 'xsum_4_text_en.json', 'xsum_5_text_en.json']\n",
      "0\n",
      "23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [03:29<00:00, 104.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1201412.56 seconds, 0.00 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# A FAIRE : GERER LE BERT MULTILINGUE\n",
    "# Anglais vers français : taux de foisonnement de 20%\n",
    "\n",
    "rouge = evaluate.load('rouge') # Warning ! When comparing many references to a prediction, an average is performed, not a max...\n",
    "bertscore = evaluate.load('bertscore') # The BERTScore calculation will take a lot of time... (if not performed in only one call)\n",
    "\n",
    "mkdir('results')\n",
    "\n",
    "nb_batch = \"3\" # Mettre le numéro du \"batch d'inférence\"\n",
    "\n",
    "# Define the place of storage : \"results/x/scores.csv\"\n",
    "output_names = os.listdir(\"results/\" + nb_batch)\n",
    "# Lists used for the BERT score computation\n",
    "references = {\"0\":[], \"1\":[]}\n",
    "predictions = {\"0\":[], \"1\":[]}\n",
    "# Columns of the csv file\n",
    "output_ids, rouges, input_nb_words_list, nb_words_generated_summaries, nb_words_gold_summaries, outputs_success, outputs_over_context, languages_output, languages_input = [], [], [], [], [], [], [], [], []\n",
    "print(output_names)\n",
    "for output_name in output_names:\n",
    "    # Makes sure the file is an output file, and opens it\n",
    "    output_path = \"results/\" + nb_batch + \"/\" + output_name\n",
    "    if output_name==\"scores.csv\" or output_name==\"desc.txt\" or os.path.isdir(output_path):\n",
    "        continue\n",
    "    output_data = load_json_into_dict(output_path)\n",
    "    generated_summary = output_data[\"text\"]\n",
    "    output_language = str(classify(generated_summary))\n",
    "    language_code = \"text_en\" \n",
    "    if output_language in number_to_code:\n",
    "        language_code = number_to_code[output_language] # Le language code est soit text_fr si output fr, soit text_en si output en ou toute autre langue ! Juste pour choisir un résumé de référence...\n",
    "    \n",
    "    find_path = output_name.split('_') # 0: corpus, 1: name json input file, 2: text (quite useless) 3: fr.json or en.json\n",
    "    input_path = \"datasets/\" + name_dataset + \"/\" + find_path[0] + \"/\" + find_path[1]\n",
    "    input_data = load_json_into_dict(input_path)\n",
    "    summaries_data = input_data[\"summaries\"]\n",
    "    # Computation of Rouge\n",
    "    golds = [] # Sert uniquement pour le calcul du BertScore avec plusieurs références\n",
    "    max_rouge2, max_rougel, nb_words_closest_gold = maxRouge(summaries_data, language_code, generated_summary, golds)\n",
    "    # For BertScore\n",
    "    if output_language in {\"0\", \"1\"}:\n",
    "        predictions[output_language].append(generated_summary)\n",
    "        references[output_language].append(golds)\n",
    "    # Fill the columns\n",
    "    rouges.append([max_rouge2, max_rougel])\n",
    "    output_ids.append(output_name)\n",
    "    nb_words_gold_summaries.append(nb_words_closest_gold)\n",
    "    nb_words_generated_summaries.append(output_data['nb_words'])\n",
    "    outputs_success.append(output_data['success'])\n",
    "    outputs_over_context.append(output_data['over_context'])\n",
    "    languages_input.append(output_data['input_language'])\n",
    "    input_nb_words_list.append(int(input_data['nb_words_' + language_code[-2:]]))\n",
    "    languages_output.append(output_language)\n",
    "\n",
    "print(len(predictions[\"0\"]))\n",
    "print(len(predictions[\"1\"]))\n",
    "\n",
    "\n",
    "#max_bertscores_fr = bertscore.compute(predictions=predictions[\"0\"], references=references[\"0\"], lang='fr', rescale_with_baseline=True, verbose=True)['f1']\n",
    "max_bertscores_en = bertscore.compute(predictions=predictions[\"1\"], references=references[\"1\"], lang='en', rescale_with_baseline=True, verbose=True)['f1']\n",
    "\n",
    "index_fr = 0\n",
    "index_en = 0\n",
    "rows = []\n",
    "for i in range(len(output_ids)):\n",
    "    max_bertscore = 0\n",
    "    if languages_output[i]==\"0\":\n",
    "        #max_bertscore = max_bertscores_fr[index_fr]\n",
    "        index_fr += 1\n",
    "    elif languages_output[i]==\"1\":\n",
    "        max_bertscore = max_bertscores_en[index_en]\n",
    "        index_en += 1\n",
    "    rows.append([output_ids[i], rouges[i][0], rouges[i][1], max_bertscore, input_nb_words_list[i], nb_words_gold_summaries[i], nb_words_generated_summaries[i], outputs_success[i], outputs_over_context[i], languages_output[i], languages_input[i]])\n",
    "\n",
    "header = [[\"input_path\", \"rouge2\", \"rougel\", \"bertscore\", \"nb_words_input\", \"nb_words_gold\", \"nb_words_generated\", \"success\", \"over_context\", \"output_language\", \"input_language\"]]\n",
    "\n",
    "storage_file = open('results/' + nb_batch + '/scores.csv', mode='w', newline='', encoding='utf-8')\n",
    "csv_writer = csv.writer(storage_file)\n",
    "csv_writer.writerows(header + rows)\n",
    "storage_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n"
     ]
    }
   ],
   "source": [
    "a = \"text_en\"\n",
    "print(a[-2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hardware use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvidia-smi' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "# model.hf_device_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "name = 'mosaicml/mpt-7b-instruct'\n",
    "name2 = \"Trelis/mpt-7b-instruct-hosted-inference-8bit\"\n",
    "\n",
    "config = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\n",
    "config.max_seq_len = 4096 # (input + output) tokens can now be up to 4096\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('legendhasit/xgen-7b-8k-inst-8bit', trust_remote_code=True)\n",
    "\n",
    "!nvidia-smi\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "  name,\n",
    "  config=config,\n",
    "  trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "input_test = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "sample = model.generate(**input_test, do_sample=True, max_new_tokens=100, top_k=20, temperature=0.3)\n",
    "print(tokenizer.decode(sample[0]).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [04:06<00:00, 123.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1107629.12 seconds, 0.00 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# A FAIRE : GERER LE BERT MULTILINGUE\n",
    "# COnserver ce 2eme programme adapté aux anciennes générations\n",
    "# Anglais vers français : taux de foisonnement de 20%\n",
    "\n",
    "rouge = evaluate.load('rouge') # Warning ! When comparing many references to a prediction, an average is performed, not a max...\n",
    "bertscore = evaluate.load('bertscore') # The BERTScore calculation will take a lot of time... (if not performed in only one call)\n",
    "\n",
    "mkdir('results')\n",
    "\n",
    "nb_batch = \"2\" # Mettre le numéro du \"batch d'inférence\"\n",
    "\n",
    "# Define the place of storage : \"results/x/scores.csv\"\n",
    "output_names = os.listdir(\"results/\" + nb_batch)\n",
    "# The texts for bertscore\n",
    "references, predictions = [], []\n",
    "# The columns of the csv file\n",
    "output_ids, rouges, input_nb_words_list, nb_words_generated_summaries, nb_words_gold_summaries, outputs_success, outputs_over_context, languages_output, languages_input = [], [], [], [], [], [], [], [], []\n",
    "\n",
    "for output_name in output_names:\n",
    "    # To ensure the read file is an output\n",
    "    output_path = \"results/\" + nb_batch + \"/\" + output_name\n",
    "    if output_name==\"scores.csv\" or output_name==\"desc.txt\" or os.path.isdir(output_path):\n",
    "        continue\n",
    "    # Extraire le dictionnaire de données de l'ouptut\n",
    "    output_data = load_json_into_dict(output_path)\n",
    "    # Obtenir l'input, le langage de l'output, les résumés de référence\n",
    "    language_output = str(classify(generated_summary))\n",
    "    language_code = number_to_code[language_output]\n",
    "    find_path = output_name.split('_') # 0: corpus, 1: name json input file, 2: text (quite useless) 3: fr.json or en.json\n",
    "    input_path = \"datasets/\" + name_dataset + \"/\" + find_path[0] + \"/\" + find_path[1]\n",
    "    input_data = load_json_into_dict(input_path)\n",
    "    summaries_data = input_data[\"summaries\"]\n",
    "    # Preparation pour Max Rouge\n",
    "    golds = [] # Sert uniquement pour le calcul du BertScore avec plusieurs références\n",
    "    max_rouge2, max_rougel, nb_words_closest_gold = maxRouge(summaries_data, language_code, generated_summary, golds)\n",
    "    # Fill in the row\n",
    "    output_ids.append(output_name)\n",
    "    languages_output.append(language_output)\n",
    "    rouges.append([max_rouge2, max_rougel])\n",
    "    predictions.append(generated_summary)\n",
    "    references.append(golds)\n",
    "    nb_words_gold_summaries.append(nb_words_closest_gold)\n",
    "    nb_words_generated_summaries.append(output_data['nb_words'])\n",
    "    outputs_success.append(output_data['success'])\n",
    "    outputs_over_context.append(output_data['over_context'])\n",
    "    input_nb_words_list.append(int(input_data['nb_words_' + language_code]))\n",
    "    languages_input.append(key_to_language[output_data['input_language']])\n",
    "\n",
    "max_bertscores = bertscore.compute(predictions=predictions, references=references, lang='en', rescale_with_baseline=True, verbose=True)['f1'] # PROBLEM WITH LANGUAGE USED\n",
    "\n",
    "storage_file = open('results/' + nb_batch + '/scores4.csv', mode='w', newline='', encoding='utf-8')\n",
    "csv_writer = csv.writer(storage_file)\n",
    "\n",
    "rows = [[output_ids[i], rouges[i][0], rouges[i][1], max_bertscores[i], input_nb_words_list[i], nb_words_gold_summaries[i], nb_words_generated_summaries[i], outputs_success[i], outputs_over_context[i], languages_output[i], languages_input[i]] for i in range(len(output_ids))]\n",
    "\n",
    "header = [[\"input_path\", \"rouge2\", \"rougel\", \"bertscore\", \"nb_words_input\", \"nb_words_gold\", \"nb_words_generated\", \"success\", \"over_context\", \"output_language\", \"input_language\"]]\n",
    "\n",
    "csv_writer.writerows(header + rows)\n",
    "\n",
    "storage_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 776/776 [00:00<?, ?B/s] \n",
      "c:\\Users\\rayci\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rayci\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 13.9MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 6.20MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 161kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", trust_remote_code=True)\n",
    "print(tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizers[\"tiiuae/falcon-7b\"]()\n",
    "model = models[\"tiiuae/falcon-7b\"]()\n",
    "\n",
    "prompt = prompt_templates[\"tiiuae/falcon-7b\"].format(\"Hey, how are you my dear Falcon ?\")\n",
    "tokenized_input = tokenizer(prompt, return_token_type_ids=False, return_tensors=\"pt\").to('cuda')\n",
    "output = infer[\"tiiuae/falcon-7b\"](tokenized_input, model)\n",
    "print(tokenizer.decode(output[0]).strip())\n",
    "print()\n",
    "print(\"Treated ================================\")\n",
    "print()\n",
    "print(treat_output[\"tiiuae/falcon-7b\"](tokenizer.decode(output[0]).strip()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
