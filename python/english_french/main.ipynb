{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rayci\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import time\n",
    "import datetime\n",
    "import evaluate\n",
    "import re\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoConfig\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Working conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "name_dataset = \"LanguageTestDataSet\" # The dataset must be in a certain format\n",
    "\n",
    "# Models used\n",
    "used_models_name = {\"mosaicml/mpt-7b-instruct\"} # Models taken for computation \n",
    "\n",
    "# Input limits\n",
    "token_limit = 3500 # To be determined with the context length of the used models\n",
    "\n",
    "# Description of the inference wanted\n",
    "desc = \"This inference batch is aimed at testing MPT7B on summarization. There is no French summarization at all.\"\n",
    "\n",
    "# Caracteristics of the inference wanted\n",
    "max_new_tokens=700\n",
    "top_k=20\n",
    "temperature=0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompting elements\n",
    "instruction_templates = {\"text_en\":\"Summarize the following text:\\n\\n{text}\", \"text_fr\":\"Résume le texte suivant:\\n\\n{text}\"}\n",
    "text_keys = {'text_en'} # Cet ensemble donne les clés pour le texte français et anglais. On retire 'text_fr' pour les modèles anglais\n",
    "key_to_language = {'text_fr':'0', 'text_en':'1'}\n",
    "language_to_key = {'0':'text_fr', '1':'text_en'}\n",
    "number_to_code = {'0':'fr', '1':'en', '2':'en'}\n",
    "\n",
    "# Miscellaneous useful functions and constants\n",
    "def count_words(s):\n",
    "    words = re.findall(r'\\b\\w+\\b', s)  # Find all word-like sequences using regular expression\n",
    "    return len(words)  # Return the number of words\n",
    "\n",
    "def mkdir(folder_path):\n",
    "    try:\n",
    "        os.mkdir(folder_path)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "def classify(text): # Classificateur déterministe basique, qui accumule des indices de langue et renvoie le langage avec le plsu haut score\n",
    "    score_en = 0\n",
    "    score_fr = 0\n",
    "    score_en += len(find_all_occurrences_regex(text, \" and \" ))\n",
    "    score_en += len(find_all_occurrences_regex(text, \" of \" ))\n",
    "    score_en += len(find_all_occurrences_regex(text, \" the \" ))\n",
    "    score_en += len(find_all_occurrences_regex(text, \" in \" ))\n",
    "    score_en += len(find_all_occurrences_regex(text, \" is \" ))\n",
    "    score_en += len(find_all_occurrences_regex(text, \" for \" ))\n",
    "    score_en += len(find_all_occurrences_regex(text, \" how \" ))\n",
    "    score_en += len(find_all_occurrences_regex(text, \" with \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" le \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" la \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" de \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" un \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" une \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" et \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" à \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" avec \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" il \" ))\n",
    "    score_fr += len(find_all_occurrences_regex(text, \" pour \" ))\n",
    "    if score_fr + score_en <= 3: return 2\n",
    "    if score_fr > score_en: return 0\n",
    "    return 1\n",
    "\n",
    "def find_all_occurrences_regex(text, pattern):\n",
    "    occurrences = [match.start() for match in re.finditer(pattern, text)]\n",
    "    return occurrences\n",
    "\n",
    "def maxRouge(summaries_data, language_code, generated_summary, golds): # Pour un dictionnaire de résumés de référence, pour un langage donné, renvoie le résumé de référence le plus proche du résumé prédit au sens de rouge et les scores associés\n",
    "    max_rouge2 = -1\n",
    "    max_rougel = -1\n",
    "    nb_words_closest_gold = 0\n",
    "\n",
    "    for i in range(len(summaries_data)):\n",
    "        summary_data = summaries_data[i]\n",
    "        summary_text = summary_data[\"text_\" + language_code]\n",
    "        result_rouge = rouge.compute(predictions=[generated_summary], references=[summary_text], use_aggregator=False)\n",
    "        golds.append(summary_text)\n",
    "        if result_rouge['rouge2'][0] > max_rouge2:\n",
    "            max_rouge2 = result_rouge['rouge2'][0]\n",
    "            nb_words_closest_gold = summary_data['nb_words_' + language_code]\n",
    "        if result_rouge['rougeL'][0] > max_rougel:\n",
    "            max_rougel = result_rouge['rougeL'][0]\n",
    "\n",
    "    return max_rouge2, max_rougel, nb_words_closest_gold\n",
    "\n",
    "def load_json_into_dict(path):\n",
    "    file = open(path, 'r', encoding='utf-8')\n",
    "    dict = json.load(file)\n",
    "    file.close()\n",
    "    return dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These dictionaries have as a key the model name and as the value the function that load the prompt, the tokenizer, the model, give the context length or treat the output.\n",
    "\n",
    "prompt_templates = {} # Dictionnaire qui associe les templates de prompts, il faut insérer l'instruction et le texte à résumer.\n",
    "context_lengths = {} # Dictionnaire qui donne l'entier correspondant à la longueur de contexte\n",
    "tokenizers = {} # Dictionnaire qui associe la méthode pour obtenir le tokenizer du modèle\n",
    "models = {} # Dictionnaire qui associe la méthode pour obtenir le modèle\n",
    "treat_output = {} # Dictionnaire qui associe la méthode pour traiter l'output et ne conserver que la génération du modèle\n",
    "infer = {} # Dictionnaire qui associe la méthode pour l'inférence du modèle. Prend en paramètres l'input et le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model specific functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models available\n",
    "models_name = {\n",
    "    \"legendhasit/xgen-7b-8k-inst-8bit/4bit\", # 1 GPU \n",
    "    \"legendhasit/xgen-7b-8k-inst-8bit\", # 1 GPU\n",
    "    \"Salesforce/xgen-7b-8k-inst\", # 2GPU (3 shards de 10Go peut-être la dernière moins) de préférence, mais fonctionne avec 1... Sauf pour les textes longs, genre les plus petits de Fredsum + probablement mauvaise suppression automatique de la mémoire -> On a des OOM à 38GB\n",
    "    \"mosaicml/mpt-7b-instruct\", # 1 GPU (2 shards de 10 Go)- To quantize\n",
    "    \"Trelis/mpt-7b-instruct-hosted-inference-8bit\", #  - Potentially unreliable\n",
    "    \"mosaicml/mpt-7b-8k-instruct\", # To quantize\n",
    "    \"mosaicml/mpt-30b-instruct\" # To quantize\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt templates\n",
    "prompt_template_XGen = \"A chat between a curious human and an artificial intelligence assistant.\\nThe assistant gives helpful, detailed, and polite answers to the human's questions.\\n\\n### Human: {instruction}\\n\\n### Assistant: \"\n",
    "\n",
    "prompt_templates[\"legendhasit/xgen-7b-8k-inst-8bit\"] = prompt_template_XGen\n",
    "prompt_templates[\"legendhasit/xgen-7b-8k-inst-8bit/4bit\"] = prompt_template_XGen\n",
    "prompt_templates[\"Salesforce/xgen-7b-8k-inst\"] = prompt_template_XGen\n",
    "\n",
    "# Context lengths # Allow to know whether the model generated out of his context window\n",
    "context_length_XGen = 8000 \n",
    "\n",
    "context_lengths[\"legendhasit/xgen-7b-8k-inst-8bit\"] = context_length_XGen\n",
    "context_lengths[\"legendhasit/xgen-7b-8k-inst-8bit/4bit\"] = context_length_XGen\n",
    "context_lengths[\"Salesforce/xgen-7b-8k-inst\"] = context_length_XGen\n",
    "\n",
    "# Tokenizers\n",
    "def get_tokenizer_XGen8bit(): # Le même pour le 4bit\n",
    "    return AutoTokenizer.from_pretrained('legendhasit/xgen-7b-8k-inst-8bit', trust_remote_code=True)\n",
    "\n",
    "def get_tokenizer_XGen():\n",
    "    return AutoTokenizer.from_pretrained('Salesforce/xgen-7b-8k-inst', trust_remote_code=True)\n",
    "\n",
    "tokenizers[\"legendhasit/xgen-7b-8k-inst-8bit\"] = get_tokenizer_XGen8bit\n",
    "tokenizers[\"legendhasit/xgen-7b-8k-inst-8bit/4bit\"] = get_tokenizer_XGen8bit\n",
    "tokenizers[\"Salesforce/xgen-7b-8k-inst\"] = get_tokenizer_XGen\n",
    "\n",
    "# Models\n",
    "\n",
    "def get_model_XGen8bit():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    'legendhasit/xgen-7b-8k-inst-8bit',\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_model_XGen4bit():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    'legendhasit/xgen-7b-8k-inst-8bit',\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_model_XGen():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    'Salesforce/xgen-7b-8k-inst',\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "    return model\n",
    "\n",
    "models[\"legendhasit/xgen-7b-8k-inst-8bit\"] = get_model_XGen8bit\n",
    "models[\"legendhasit/xgen-7b-8k-inst-8bit/4bit\"] = get_model_XGen4bit\n",
    "models[\"Salesforce/xgen-7b-8k-inst\"] = get_model_XGen\n",
    "\n",
    "# Tokenize (if specificities in the way models call the tokenizer)\n",
    "\n",
    "# Inference (if specificities in the way models call the generate function)\n",
    "\n",
    "def infer_XGen(tokenized_input, model):\n",
    "    return model.generate(**tokenized_input, do_sample=True, max_new_tokens=max_new_tokens, top_k=top_k, eos_token_id=50256, temperature=temperature) # eos_token_id : id du token de fin de réponse d'XGen dans son vocabulaire.\n",
    "\n",
    "infer['legendhasit/xgen-7b-8k-inst-8bit'] = infer_XGen\n",
    "infer['legendhasit/xgen-7b-8k-inst-8bit/4bit'] = infer_XGen\n",
    "infer['Salesforce/xgen-7b-8k-inst'] = infer_XGen\n",
    "\n",
    "# Treating the output (to remove the input if present in the output as well as the end of text token for example)\n",
    "\n",
    "def treat_output_XGen(output): # Differs if it is N-shot. Here, it is 0-shot\n",
    "    occ_1 = output.find(\"### Assistant: \")\n",
    "    output = output[occ_1+15:]\n",
    "    if output.find('<|endoftext|>')!=-1:\n",
    "        output = output[:-14]\n",
    "    return output\n",
    "\n",
    "treat_output['legendhasit/xgen-7b-8k-inst-8bit'] = treat_output_XGen\n",
    "treat_output['legendhasit/xgen-7b-8k-inst-8bit/4bit'] = treat_output_XGen\n",
    "treat_output['Salesforce/xgen-7b-8k-inst'] = treat_output_XGen\n",
    "\n",
    "# /workspace/.miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1411: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
    "# Avec XGen de salesforce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt templates\n",
    "prompt_template_MPT7B = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{instruction}\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template_MPT30B = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n###Instruction\\n{instruction}\\n\\n### Response\\n\" # On considère que c'est le même template que pour MPT 7B 8k car pour ce dernier, le template utilisé n'est pas précisé\n",
    "\n",
    "prompt_templates[\"mosaicml/mpt-7b-instruct\"] = prompt_template_MPT7B\n",
    "prompt_templates[\"Trelis/mpt-7b-instruct-hosted-inference-8bit\"] = prompt_template_MPT7B\n",
    "prompt_templates[\"mosaicml/mpt-7b-8k-instruct\"] = prompt_template_MPT30B\n",
    "prompt_templates[\"mosaicml/mpt-30b-instruct\"] = prompt_template_MPT30B\n",
    "\n",
    "# Context lengths\n",
    "\n",
    "context_lengths[\"mosaicml/mpt-7b-instruct\"] = 2048 # 4096 d'après mosaicml\n",
    "\n",
    "# Models\n",
    "def get_model_MPT7B():\n",
    "    config = AutoConfig.from_pretrained(\"mosaicml/mpt-7b-instruct\", trust_remote_code=True)\n",
    "    config.max_seq_len = 4096 # (input + output) tokens can now be up to 4096\n",
    "    return AutoModelForCausalLM.from_pretrained(\"mosaicml/mpt-7b-instruct\",config=config,trust_remote_code=True,device_map = \"auto\")\n",
    "\n",
    "models[\"mosaicml/mpt-7b-instruct\"] = get_model_MPT7B\n",
    "\n",
    "# Tokenize\n",
    "def get_tokenizer_MPT7B():\n",
    "    return AutoTokenizer.from_pretrained(\"mosaicml/mpt-7b-instruct\", trust_remote_code=True)\n",
    "\n",
    "tokenizers[\"mosaicml/mpt-7b-instruct\"] = get_tokenizer_MPT7B\n",
    "# Inference ######################################\n",
    "\n",
    "def infer_MPT7B(tokenized_input, model):\n",
    "    return model.generate(**tokenized_input, do_sample=True, max_new_tokens=max_new_tokens, top_k=top_k, temperature=temperature) # eos_token_id : id du token de fin de réponse d'XGen dans son vocabulaire.\n",
    "\n",
    "infer[\"mosaicml/mpt-7b-instruct\"] = infer_MPT7B\n",
    "\n",
    "# Treating the output ######################################\n",
    "\n",
    "def treat_output_MPT(output): # Differs if it is N-shot. Here, it is 0-shot\n",
    "    occ_1 = output.find(\"### Response:\\n\")\n",
    "    output = output[occ_1+14:]\n",
    "    if output.find('<|endoftext|>')!=-1:\n",
    "        output = output[:-14]\n",
    "    return output\n",
    "\n",
    "treat_output[\"mosaicml/mpt-7b-instruct\"] = treat_output_MPT\n",
    "treat_output[\"Trelis/mpt-7b-instruct-hosted-inference-8bit\"] = treat_output_MPT\n",
    "treat_output[\"mosaicml/mpt-7b-8k-instruct\"] = treat_output_MPT\n",
    "treat_output[\"mosaicml/mpt-30b-instruct\"] = treat_output_MPT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing the input dataset\n",
    "Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for corpus in os.listdir(\"datasets/\" + name_dataset): # This script fills in the number of words and number of characters of the input files\n",
    "    for file_name in os.listdir(\"datasets/\" + name_dataset + \"/\" + corpus):\n",
    "        file_path = \"datasets/\" + name_dataset + \"/\" + corpus + \"/\" + file_name\n",
    "        if file_name==\".ipynb_checkpoints\" or os.path.isdir(file_path):\n",
    "            continue\n",
    "        file = open(file_path, 'r', encoding='utf-8')\n",
    "        data = json.load(file)\n",
    "        file.close()\n",
    "        text_en = data[\"text_en\"]\n",
    "        text_fr = data[\"text_fr\"]\n",
    "        data[\"nb_words_en\"] = str(count_words(text_en))\n",
    "        data[\"nb_words_fr\"] = str(count_words(text_fr))\n",
    "        data[\"nb_characters_en\"] = str(len(text_en))\n",
    "        data[\"nb_characters_fr\"] = str(len(text_fr))\n",
    "        \n",
    "        for i in range(len(data[\"summaries\"])):\n",
    "            summary = data[\"summaries\"][i]\n",
    "            text_en = summary[\"text_en\"]\n",
    "            text_fr = summary[\"text_fr\"]\n",
    "            summary[\"nb_words_en\"] = str(count_words(text_en))\n",
    "            summary[\"nb_words_fr\"] = str(count_words(text_fr))\n",
    "            summary[\"nb_characters_en\"] = str(len(text_en))\n",
    "            summary[\"nb_characters_fr\"] = str(len(text_fr))\n",
    "        file = open(file_path, 'w', encoding='utf-8')\n",
    "        json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "        file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every time the script is casted, it must register the results in the \"results\" folder, without smashing the existing results\n",
    "# Then, it is stored in a file whose name is the number of the result, and this file contains a little .txt note describing what was the experiment.\n",
    "# In such a folder, all the generations are json files with additional information like the prompt used, the number of samples, the path of the input text\n",
    "\n",
    "# Now, what is specific to the model ? The prompt (different headers potentially), the tokenizer, the model itself, the way the output is displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir(\"results\")\n",
    "\n",
    "archives = os.listdir(\"results\")\n",
    "output_folder_path = \"results/\"+str(len(archives))\n",
    "mkdir(output_folder_path) # Create a folder for the last results.\n",
    "desc_file = open(output_folder_path + \"/desc.txt\", 'w', encoding='utf-8')\n",
    "desc_file.write(desc)\n",
    "desc_file.close()\n",
    "\n",
    "initial_time = time.time()\n",
    "\n",
    "model_total = len(used_models_name)\n",
    "model_index = 0\n",
    "for model_name in used_models_name: # For each model\n",
    "    model_index += 1\n",
    "    print(\"---- Model : \" + model_name + \" (\" + str(model_index) + \"/\" + str(model_total) +\")----              (loading tokenizer and model...)\")\n",
    "    load_model_time = time.time()\n",
    "    prompt_template = prompt_templates[model_name]\n",
    "    tokenizer = tokenizers[model_name]()\n",
    "    model = models[model_name]()\n",
    "    load_model_time = time.time() - load_model_time\n",
    "    !nvidia-smi\n",
    "    print(\"Tokenizer and model loaded in\", datetime.timedelta(seconds=int(load_model_time)), 'seconds')\n",
    "    corpora = os.listdir(\"datasets/\" + name_dataset)\n",
    "    for corpus in corpora:\n",
    "        !nvidia-smi\n",
    "        file_index = 0\n",
    "        files_name = os.listdir(\"datasets/\" + name_dataset + \"/\" + corpus)\n",
    "        file_total = len(files_name)\n",
    "        for file_name in files_name:\n",
    "            file_path = \"datasets/\" + name_dataset + \"/\" + corpus + \"/\" + file_name\n",
    "            if file_name==\"ipynb_checkpoints\" or os.path.isdir(file_path):\n",
    "                continue\n",
    "            file_time = time.time()\n",
    "            file_index+=1\n",
    "            print(\"Starting inference for text \" + str(file_index) + \"/\" + str(file_total) + \" in the \" + corpus + \" corpus.\") # Possible errors in the display: file_total also counts the .ipynb checkpoints potentially in the directory.\n",
    "            file = open(file_path, 'r', encoding='utf-8')\n",
    "            input_data = json.load(file)\n",
    "            file.close()\n",
    "            for text_key in text_keys:\n",
    "                input_text = input_data[text_key]\n",
    "                instruction_template = instruction_templates[text_key]\n",
    "                \n",
    "                full_instruction = instruction_template.format(text=input_text)\n",
    "                prompt = prompt_template.format(instruction=full_instruction)\n",
    "                # Probably specific to the model\n",
    "                input = tokenizer(prompt, return_tensors=\"pt\").to('cuda') # Le renvoie sur le GPU car au départ, c'est généré en CPU le tensuer des tokens\n",
    "                #\n",
    "                input_length = len(input['input_ids'][0])\n",
    "                output_name = corpus + \"_\" + file_name + \"_\" + text_key + \".json\"\n",
    "                output_path = output_folder_path + \"/\" + output_name\n",
    "                output_data = {\"input_path\":file_path, \"model\":model_name, \"instruction\":instruction_template.format(text=\"\"), \"success\":\"0\", \"over_context\":\"\", \"input_length\":str(input_length), \"text\":\"\", \"output_length\":\"\", \"nb_words\":\"\", \"nb_characters\":\"\", \"input_language\":input_data[\"language\"], \"output_language\":\"\"}\n",
    "                if input_length > token_limit: # In this case, a file is created but no inference is made\n",
    "                    print(\"/!\\ With model \" + model_name + \", the input length is above the token limit for \" + text_key + \" input in \" + file_path + \" (\" + str(input_length) + \" > \" + str(token_limit) + \")\")\n",
    "\n",
    "                else: # In this case, the output is inferred, treated to keep only the generation and the length information\n",
    "                    # PROBABLY SPECIFIC TO THE MODEL\n",
    "                    sample = infer[model_name](input, model)\n",
    "                    sample_length = len(sample[0])\n",
    "                    ### SPECIFIC TO THE MODEL ----------------------------------------\n",
    "                    if sample_length > context_lengths[model_name]: # There was a context window overflow\n",
    "                        output_data[\"over_context\"] = \"1\"\n",
    "                    else:\n",
    "                        output_data[\"over_context\"] = \"0\"\n",
    "                    output_length = sample_length - input_length\n",
    "                    output_data[\"output_length\"] = output_length\n",
    "                    full_output = tokenizer.decode(sample[0]).strip() # prompt + output generation\n",
    "                    # Ideally, treat_output is a dictionnary that associates model_name to a function taking full_output as an argument\n",
    "                    output = treat_output[model_name](full_output)\n",
    "                    output_data[\"text\"] = output\n",
    "                    output_data[\"nb_characters\"] = len(output)\n",
    "                    output_data[\"nb_words\"] = count_words(output)\n",
    "                    output_data[\"success\"] = \"1\"\n",
    "                    output_data[\"language_output\"] = str(classify(output))\n",
    "                # SAVING THE OUTPUT\n",
    "                output_file = open(output_path, 'w', encoding='utf-8')\n",
    "                json.dump(output_data, output_file, indent=4, ensure_ascii=False)\n",
    "                output_file.close()\n",
    "            file_time = time.time() - file_time\n",
    "            print(\"Inference done for French and English in\", datetime.timedelta(seconds=int(file_time)), \"seconds.\")\n",
    "    # HERE, clear memory... remove model, tokenizer from memory\n",
    "                \n",
    "delta = time.time() - initial_time\n",
    "print('Done! Took', datetime.timedelta(seconds=int(delta)), 'seconds')\n",
    "# Write treatOutput function / dictionnary\n",
    "\n",
    "\n",
    "# RETIRER le fichier template dans samsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m output_ids\u001b[39m.\u001b[39mappend(output_name)\n\u001b[0;32m     28\u001b[0m output_file \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mresults/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m nb_batch \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m output_name, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 29\u001b[0m output_data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mload(output_file)\n\u001b[0;32m     30\u001b[0m output_file\u001b[39m.\u001b[39mclose()\n\u001b[0;32m     31\u001b[0m language_output \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m-1\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rayci\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(fp, \u001b[39m*\u001b[39m, \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_float\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_constant\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_pairs_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[39m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[39m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[39mreturn\u001b[39;00m loads(fp\u001b[39m.\u001b[39mread(),\n\u001b[0;32m    294\u001b[0m         \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mcls\u001b[39m, object_hook\u001b[39m=\u001b[39mobject_hook,\n\u001b[0;32m    295\u001b[0m         parse_float\u001b[39m=\u001b[39mparse_float, parse_int\u001b[39m=\u001b[39mparse_int,\n\u001b[0;32m    296\u001b[0m         parse_constant\u001b[39m=\u001b[39mparse_constant, object_pairs_hook\u001b[39m=\u001b[39mobject_pairs_hook, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\rayci\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\Users\\rayci\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[0;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[1;32mc:\\Users\\rayci\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "# A FAIRE : GERER LE BERT MULTILINGUE\n",
    "# Anglais vers français : taux de foisonnement de 20%\n",
    "\n",
    "rouge = evaluate.load('rouge') # Warning ! When comparing many references to a prediction, an average is performed, not a max...\n",
    "bertscore = evaluate.load('bertscore') # The BERTScore calculation will take a lot of time... (if not performed in only one call)\n",
    "\n",
    "mkdir('results')\n",
    "\n",
    "nb_batch = \"0\" # Mettre le numéro du \"batch d'inférence\"\n",
    "\n",
    "# Define the place of storage : \"results/x/scores.csv\"\n",
    "output_names = os.listdir(\"results/\" + nb_batch)\n",
    "# Lists used for the BERT score computation\n",
    "references = {\"0\":[], \"1\":[]}\n",
    "predictions = {\"0\":[], \"1\":[]}\n",
    "# Columns of the csv file\n",
    "output_ids, rouges, input_nb_words_list, nb_words_generated_summaries, nb_words_gold_summaries, outputs_success, outputs_over_context, languages_output, languages_input = [], [], [], [], [], [], [], [], []\n",
    "\n",
    "for output_name in output_names:\n",
    "    # Makes sure the file is an output file, and opens it\n",
    "    output_path = \"results/\" + nb_batch + \"/\" + output_name\n",
    "    if output_name==\"scores.csv\" or output_name==\"desc.txt\" or os.path.isdir(output_path):\n",
    "        continue\n",
    "    output_data = load_json_into_dict(output_path)\n",
    "    output_language = str(output_data[\"output_language\"])\n",
    "    language_code = \"text_en\" \n",
    "    if output_language in number_to_code:\n",
    "        language_code = number_to_code[output_language] # Le language code est soit text_fr si output fr, soit text_en si output en ou toute autre langue ! Juste pour choisir un résumé de référence...\n",
    "    generated_summary = output_data[\"text\"]\n",
    "    find_path = output_name.split('_') # 0: corpus, 1: name json input file, 2: text (quite useless) 3: fr.json or en.json\n",
    "    input_path = \"datasets/\" + name_dataset + \"/\" + find_path[0] + \"/\" + find_path[1]\n",
    "    input_data = load_json_into_dict(input_path)\n",
    "    summaries_data = input_data[\"summaries\"]\n",
    "    # Computation of Rouge\n",
    "    golds = [] # Sert uniquement pour le calcul du BertScore avec plusieurs références\n",
    "    max_rouge2, max_rougel, nb_words_closest_gold = maxRouge(summaries_data, language_code, generated_summary, golds)\n",
    "    # For BertScore\n",
    "    if output_language in {\"0\", \"1\"}:\n",
    "        predictions[output_language].append(generated_summary)\n",
    "        references[output_language].append(golds)\n",
    "    # Fill the columns\n",
    "    rouges.append([max_rouge2, max_rougel])\n",
    "    output_ids.append(output_name)\n",
    "    nb_words_gold_summaries.append(nb_words_closest_gold)\n",
    "    nb_words_generated_summaries.append(output_data['nb_words'])\n",
    "    outputs_success.append(output_data['success'])\n",
    "    outputs_over_context.append(output_data['over_context'])\n",
    "    languages_input.append(output_data['input_language'])\n",
    "    input_nb_words_list.append(int(input_data['nb_words_' + language_code]))\n",
    "    languages_output.append(output_language)\n",
    "\n",
    "max_bertscores_fr = bertscore.compute(predictions=predictions[\"0\"], references=references[\"0\"], lang='fr', rescale_with_baseline=True, verbose=True)['f1']\n",
    "max_bertscores_en = bertscore.compute(predictions=predictions[\"1\"], references=references[\"1\"], lang='en', rescale_with_baseline=True, verbose=True)['f1']\n",
    "\n",
    "index_fr = 0\n",
    "index_en = 0\n",
    "rows = []\n",
    "for i in range(len(output_ids)):\n",
    "    max_bertscore = 0\n",
    "    if languages_output[i]==\"0\":\n",
    "        max_bertscore = max_bertscores_fr[index_fr]\n",
    "        index_fr += 1\n",
    "    elif languages_output[i]==\"1\":\n",
    "        max_bertscore = max_bertscores_fr[index_en]\n",
    "        index_en += 1\n",
    "    rows.append([output_ids[i], rouges[i][0], rouges[i][1], max_bertscore, input_nb_words_list[i], nb_words_gold_summaries[i], nb_words_generated_summaries[i], outputs_success[i], outputs_over_context[i], languages_output[i], languages_input[i]])\n",
    "\n",
    "header = [[\"input_path\", \"rouge2\", \"rougel\", \"bertscore\", \"nb_words_input\", \"nb_words_gold\", \"nb_words_generated\", \"success\", \"over_context\", \"output_language\", \"input_language\"]]\n",
    "\n",
    "storage_file = open('results/' + nb_batch + '/scores.csv', mode='w', newline='', encoding='utf-8')\n",
    "csv_writer = csv.writer(storage_file)\n",
    "csv_writer.writerows(header + rows)\n",
    "storage_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hardware use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvidia-smi' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "# model.hf_device_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "name = 'mosaicml/mpt-7b-instruct'\n",
    "name2 = \"Trelis/mpt-7b-instruct-hosted-inference-8bit\"\n",
    "\n",
    "config = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\n",
    "config.max_seq_len = 4096 # (input + output) tokens can now be up to 4096\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('legendhasit/xgen-7b-8k-inst-8bit', trust_remote_code=True)\n",
    "\n",
    "!nvidia-smi\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "  name,\n",
    "  config=config,\n",
    "  trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "input_test = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "sample = model.generate(**input_test, do_sample=True, max_new_tokens=100, top_k=20, temperature=0.3)\n",
    "print(tokenizer.decode(sample[0]).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [04:06<00:00, 123.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1107629.12 seconds, 0.00 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# A FAIRE : GERER LE BERT MULTILINGUE\n",
    "# COnserver ce 2eme programme adapté aux anciennes générations\n",
    "# Anglais vers français : taux de foisonnement de 20%\n",
    "\n",
    "rouge = evaluate.load('rouge') # Warning ! When comparing many references to a prediction, an average is performed, not a max...\n",
    "bertscore = evaluate.load('bertscore') # The BERTScore calculation will take a lot of time... (if not performed in only one call)\n",
    "\n",
    "mkdir('results')\n",
    "\n",
    "nb_batch = \"2\" # Mettre le numéro du \"batch d'inférence\"\n",
    "\n",
    "# Define the place of storage : \"results/x/scores.csv\"\n",
    "output_names = os.listdir(\"results/\" + nb_batch)\n",
    "# The texts for bertscore\n",
    "references, predictions = [], []\n",
    "# The columns of the csv file\n",
    "output_ids, rouges, input_nb_words_list, nb_words_generated_summaries, nb_words_gold_summaries, outputs_success, outputs_over_context, languages_output, languages_input = [], [], [], [], [], [], [], [], []\n",
    "\n",
    "for output_name in output_names:\n",
    "    # To ensure the read file is an output\n",
    "    output_path = \"results/\" + nb_batch + \"/\" + output_name\n",
    "    if output_name==\"scores.csv\" or output_name==\"desc.txt\" or os.path.isdir(output_path):\n",
    "        continue\n",
    "    # Extraire le dictionnaire de données de l'ouptut\n",
    "    output_data = load_json_into_dict(output_path)\n",
    "    # Obtenir l'input, le langage de l'output, les résumés de référence\n",
    "    language_output = str(classify(generated_summary))\n",
    "    language_code = number_to_code[language_output]\n",
    "    find_path = output_name.split('_') # 0: corpus, 1: name json input file, 2: text (quite useless) 3: fr.json or en.json\n",
    "    input_path = \"datasets/\" + name_dataset + \"/\" + find_path[0] + \"/\" + find_path[1]\n",
    "    input_data = load_json_into_dict(input_path)\n",
    "    summaries_data = input_data[\"summaries\"]\n",
    "    # Preparation pour Max Rouge\n",
    "    golds = [] # Sert uniquement pour le calcul du BertScore avec plusieurs références\n",
    "    max_rouge2, max_rougel, nb_words_closest_gold = maxRouge(summaries_data, language_code, generated_summary, golds)\n",
    "    # Fill in the row\n",
    "    output_ids.append(output_name)\n",
    "    languages_output.append(language_output)\n",
    "    rouges.append([max_rouge2, max_rougel])\n",
    "    predictions.append(generated_summary)\n",
    "    references.append(golds)\n",
    "    nb_words_gold_summaries.append(nb_words_closest_gold)\n",
    "    nb_words_generated_summaries.append(output_data['nb_words'])\n",
    "    outputs_success.append(output_data['success'])\n",
    "    outputs_over_context.append(output_data['over_context'])\n",
    "    input_nb_words_list.append(int(input_data['nb_words_' + language_code]))\n",
    "    languages_input.append(key_to_language[output_data['input_language']])\n",
    "\n",
    "max_bertscores = bertscore.compute(predictions=predictions, references=references, lang='en', rescale_with_baseline=True, verbose=True)['f1'] # PROBLEM WITH LANGUAGE USED\n",
    "\n",
    "storage_file = open('results/' + nb_batch + '/scores4.csv', mode='w', newline='', encoding='utf-8')\n",
    "csv_writer = csv.writer(storage_file)\n",
    "\n",
    "rows = [[output_ids[i], rouges[i][0], rouges[i][1], max_bertscores[i], input_nb_words_list[i], nb_words_gold_summaries[i], nb_words_generated_summaries[i], outputs_success[i], outputs_over_context[i], languages_output[i], languages_input[i]] for i in range(len(output_ids))]\n",
    "\n",
    "header = [[\"input_path\", \"rouge2\", \"rougel\", \"bertscore\", \"nb_words_input\", \"nb_words_gold\", \"nb_words_generated\", \"success\", \"over_context\", \"output_language\", \"input_language\"]]\n",
    "\n",
    "csv_writer.writerows(header + rows)\n",
    "\n",
    "storage_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
