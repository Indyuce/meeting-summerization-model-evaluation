{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "# A REMPLIR\n",
    "consistency_list = []\n",
    "description_list = []\n",
    "\n",
    "print(len(consistency_list))\n",
    "print(len(description_list))\n",
    "\n",
    "nb_batch = \"1\"\n",
    "path_scores_file = \"results/\" + nb_batch + \"/scores3.csv\"\n",
    "df = pd.read_csv(path_scores_file)\n",
    "df[\"consistency\"] = consistency_list\n",
    "df[\"description\"] = description_list\n",
    "df.to_csv('scores.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGEN\n",
    "#### 4bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: This inference batch is aimed at testing this code. It helped debugging. it was performed with a 4bit quantized version of XGen 7B. One take away is that for 4 out of the 5 fredsum texts, there was too many tokens (above 1800)\n",
      "Le batch 0 contient 28/40 échantillons cohérents.\n",
      "Parmi les réponses écrites en français, 1/1 des questions sont écrites en français\n",
      "Parmi les réponses écrites en anglais, 16/27 des questions sont écrites en anglais\n",
      "\n",
      "Moyennes questions françaises, réponses françaises (1 échantillons):\n",
      "Rouge 2: 0.0574162679425837 ---- Rouge L: 0.0904761904761904 ---- BertScore: -0.0642949044704437\n",
      "\n",
      "Moyennes questions anglaises, réponses anglaises (16 échantillons):\n",
      "Rouge 2: 0.10337345389955058 ---- Rouge L: 0.19059100544868074 ---- BertScore: 0.2400263991439715\n",
      "\n",
      "Moyennes questions françaises, réponses anglaises (11 échantillons):\n",
      "Rouge 2: 0.06339291110835027 ---- Rouge L: 0.13676942533994166 ---- BertScore: 0.16332498328252268\n",
      "\n",
      "\n",
      "Batch 1: This inference batch is aimed at testing XGen 8bit on summarization. We shall compare his performance on French and on English\n",
      "Le batch 1 contient 39/40 échantillons cohérents.\n",
      "Parmi les réponses écrites en français, 3/6 des questions sont écrites en français\n",
      "Parmi les réponses écrites en anglais, 17/33 des questions sont écrites en anglais\n",
      "\n",
      "Moyennes questions françaises, réponses françaises (3 échantillons):\n",
      "Rouge 2: 0.054316045818112835 ---- Rouge L: 0.13189993520640117 ---- BertScore: 0.13070441782474515\n",
      "\n",
      "Moyennes questions anglaises, réponses anglaises (17 échantillons):\n",
      "Rouge 2: 0.09118493125387647 ---- Rouge L: 0.19376968845119408 ---- BertScore: 0.24798966878477266\n",
      "\n",
      "Moyennes questions françaises, réponses anglaises (16 échantillons):\n",
      "Rouge 2: 0.05179152954198571 ---- Rouge L: 0.1476664335317936 ---- BertScore: 0.07711907161456108\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batches = {'0','1'}\n",
    "\n",
    "for nb_batch in batches:\n",
    "    name_file = \"results/\" + nb_batch + \"/desc.txt\"\n",
    "    file = open(name_file, 'r', encoding='utf-8')\n",
    "    description = file.readline()\n",
    "    print(\"Batch \" + nb_batch + \": \" + description)\n",
    "    # Je souhaite obtenir le dataset des samples cohérents -> filtrage\n",
    "    path_scores_file = \"results/\" + nb_batch + \"/scores.csv\"\n",
    "    # Extract consistency and description\n",
    "    df = pd.read_csv(path_scores_file)\n",
    "    array = df.values\n",
    "\n",
    "    consistent_df = df[df[\"consistency\"]==1]\n",
    "\n",
    "    consistent_df_en = consistent_df[consistent_df[\"output_language\"]==1]\n",
    "    consistent_df_en_en = consistent_df_en[consistent_df_en[\"input_language\"]==1]\n",
    "\n",
    "    consistent_df_en_fr = consistent_df_en[consistent_df_en[\"input_language\"]==0] # Réponses anglaises à une question française\n",
    "\n",
    "    consistent_df_fr = consistent_df[consistent_df[\"output_language\"]==0]\n",
    "    consistent_df_fr_fr = consistent_df_fr[consistent_df_fr[\"input_language\"]==0]\n",
    "\n",
    "    print(\"Le batch \" + nb_batch + \" contient \" + str(len(consistent_df)) + \"/\" + str(len(df)) + \" échantillons cohérents.\")\n",
    "    print(\"Parmi les réponses écrites en français, \" + str(len(consistent_df_fr_fr)) + \"/\" + str(len(consistent_df_fr)) + \" des questions sont écrites en français\")\n",
    "    print(\"Parmi les réponses écrites en anglais, \" + str(len(consistent_df_en_en)) + \"/\" + str(len(consistent_df_en)) + \" des questions sont écrites en anglais\")\n",
    "    print()\n",
    "\n",
    "    rouge2_en_fr = consistent_df_en_fr['rouge2'].mean()\n",
    "    rougel_en_fr = consistent_df_en_fr['rougel'].mean()\n",
    "    bert_en_fr = consistent_df_en_fr['bertscore'].mean()\n",
    "\n",
    "    rouge2_fr_fr = consistent_df_fr_fr['rouge2'].mean()\n",
    "    rougel_fr_fr = consistent_df_fr_fr['rougel'].mean()\n",
    "    bert_fr_fr = consistent_df_fr_fr['bertscore'].mean()\n",
    "\n",
    "    rouge2_en_en = consistent_df_en_en['rouge2'].mean()\n",
    "    rougel_en_en = consistent_df_en_en['rougel'].mean()\n",
    "    bert_en_en = consistent_df_en_en['bertscore'].mean()\n",
    "\n",
    "    print(\"Moyennes questions françaises, réponses françaises (\" + str(len(consistent_df_fr_fr)) + \" échantillons):\")\n",
    "    print(\"Rouge 2: \" + str(rouge2_fr_fr) + \" ---- Rouge L: \" + str(rougel_fr_fr) + \" ---- BertScore: \" + str(bert_fr_fr))\n",
    "    print()\n",
    "    print(\"Moyennes questions anglaises, réponses anglaises (\" + str(len(consistent_df_en_en)) + \" échantillons):\")\n",
    "    print(\"Rouge 2: \" + str(rouge2_en_en) + \" ---- Rouge L: \" + str(rougel_en_en) + \" ---- BertScore: \" + str(bert_en_en))\n",
    "    print()\n",
    "    print(\"Moyennes questions françaises, réponses anglaises (\" + str(len(consistent_df_en_fr)) + \" échantillons):\")\n",
    "    print(\"Rouge 2: \" + str(rouge2_en_fr) + \" ---- Rouge L: \" + str(rougel_en_fr) + \" ---- BertScore: \" + str(bert_en_fr))\n",
    "    print()\n",
    "    print()\n",
    "    # Grosse tendance à écrire en anglais. Et c'est normal, le header de finetuning est en anglais... Les tâches de finetuning ne sont pas multilingues !\n",
    "    # Du coup, problème lors du calcul des scores: calculs sur du gold fr et du generated en. De plus, problème plus général sur les métriques, un bertscore français est différent d'un bertscore anglais, pas comparable et la moyenne du rouge score entre deux textes randoms en langue anglais et française est probablement différente... Limites de ces mesures. Justifie l'analyse qualitative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
