{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-api-python-client\n",
      "  Downloading google_api_python_client-2.92.0-py2.py3-none-any.whl (11.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting httplib2<1.dev0,>=0.15.0 (from google-api-python-client)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.9/96.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth<3.0.0.dev0,>=1.19.0 (from google-api-python-client)\n",
      "  Downloading google_auth-2.21.0-py2.py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.1/182.1 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-httplib2>=0.1.0 (from google-api-python-client)\n",
      "  Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 (from google-api-python-client)\n",
      "  Downloading google_api_core-2.11.1-py3-none-any.whl (120 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.5/120.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting uritemplate<5,>=3.0.1 (from google-api-python-client)\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Downloading googleapis_common_protos-1.59.1-py2.py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Downloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /workspace/.miniconda3/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.28.1)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client)\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client)\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4 (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client)\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in /workspace/.miniconda3/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (1.16.0)\n",
      "Requirement already satisfied: urllib3<2.0 in /workspace/.miniconda3/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (1.26.13)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /workspace/.miniconda3/lib/python3.10/site-packages (from httplib2<1.dev0,>=0.15.0->google-api-python-client) (3.0.9)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client)\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<3,>=2 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2022.12.7)\n",
      "Installing collected packages: uritemplate, pyasn1, protobuf, httplib2, cachetools, rsa, pyasn1-modules, googleapis-common-protos, google-auth, google-auth-httplib2, google-api-core, google-api-python-client\n",
      "Successfully installed cachetools-5.3.1 google-api-core-2.11.1 google-api-python-client-2.92.0 google-auth-2.21.0 google-auth-httplib2-0.1.0 googleapis-common-protos-1.59.1 httplib2-0.22.0 protobuf-4.23.4 pyasn1-0.5.0 pyasn1-modules-0.3.0 rsa-4.9 uritemplate-4.1.1\n",
      "Collecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-uaf34nd3\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-uaf34nd3\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 30ed3adf474aaf2972ab56f5624089bc24a6adf3\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /workspace/.miniconda3/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers==4.31.0.dev0)\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /workspace/.miniconda3/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /workspace/.miniconda3/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (23.1)\n",
      "Collecting pyyaml>=5.1 (from transformers==4.31.0.dev0)\n",
      "  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17 (from transformers==4.31.0.dev0)\n",
      "  Downloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.4/770.4 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /workspace/.miniconda3/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (2.28.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31.0.dev0)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers==4.31.0.dev0)\n",
      "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /workspace/.miniconda3/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (4.51.0)\n",
      "Collecting fsspec (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0.dev0)\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /workspace/.miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0.dev0) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests->transformers==4.31.0.dev0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests->transformers==4.31.0.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests->transformers==4.31.0.dev0) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests->transformers==4.31.0.dev0) (2022.12.7)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.31.0.dev0-py3-none-any.whl size=7308539 sha256=572e932b29f9d5a981fbb63006ea1a3b03ef2d0abaa21807b30de10eb726b3fd\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-mc0_du5j/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\n",
      "Successfully built transformers\n",
      "Installing collected packages: tokenizers, safetensors, regex, pyyaml, fsspec, huggingface-hub, transformers\n",
      "Successfully installed fsspec-2023.6.0 huggingface-hub-0.16.4 pyyaml-6.0 regex-2023.6.3 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0.dev0\n",
      "Collecting git+https://github.com/huggingface/accelerate.git\n",
      "  Cloning https://github.com/huggingface/accelerate.git to /tmp/pip-req-build-t6nc09mj\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate.git /tmp/pip-req-build-t6nc09mj\n",
      "  Resolved https://github.com/huggingface/accelerate.git to commit 95bffdec4326acf6a5d1c3dbaa857a26502aa265\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /workspace/.miniconda3/lib/python3.10/site-packages (from accelerate==0.21.0.dev0) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /workspace/.miniconda3/lib/python3.10/site-packages (from accelerate==0.21.0.dev0) (23.1)\n",
      "Requirement already satisfied: psutil in /workspace/.miniconda3/lib/python3.10/site-packages (from accelerate==0.21.0.dev0) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /workspace/.miniconda3/lib/python3.10/site-packages (from accelerate==0.21.0.dev0) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /workspace/.miniconda3/lib/python3.10/site-packages (from accelerate==0.21.0.dev0) (2.0.0+cu118)\n",
      "Requirement already satisfied: filelock in /workspace/.miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0.dev0) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /workspace/.miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0.dev0) (4.4.0)\n",
      "Requirement already satisfied: sympy in /workspace/.miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0.dev0) (1.11.1)\n",
      "Requirement already satisfied: networkx in /workspace/.miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0.dev0) (3.0)\n",
      "Requirement already satisfied: jinja2 in /workspace/.miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0.dev0) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /workspace/.miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0.dev0) (2.0.0)\n",
      "Requirement already satisfied: cmake in /workspace/.miniconda3/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.21.0.dev0) (3.25.0)\n",
      "Requirement already satisfied: lit in /workspace/.miniconda3/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.21.0.dev0) (15.0.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /workspace/.miniconda3/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.21.0.dev0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /workspace/.miniconda3/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.21.0.dev0) (1.2.1)\n",
      "Building wheels for collected packages: accelerate\n",
      "  Building wheel for accelerate (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for accelerate: filename=accelerate-0.21.0.dev0-py3-none-any.whl size=241741 sha256=6f54a2971974f8f9c7eb365719725e8a5d0357ab55bc1718cc51e01e959ee2b1\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-im_59kwc/wheels/9c/a3/1e/47368f9b6575655fe9ee1b6350cfa7d4b0befe66a35f8a8365\n",
      "Successfully built accelerate\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.21.0.dev0\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /workspace/.miniconda3/lib/python3.10/site-packages (from tiktoken) (2023.6.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /workspace/.miniconda3/lib/python3.10/site-packages (from tiktoken) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.4.0\n",
      "Requirement already satisfied: torch in /workspace/.miniconda3/lib/python3.10/site-packages (2.0.0+cu118)\n",
      "Requirement already satisfied: filelock in /workspace/.miniconda3/lib/python3.10/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /workspace/.miniconda3/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: sympy in /workspace/.miniconda3/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /workspace/.miniconda3/lib/python3.10/site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /workspace/.miniconda3/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /workspace/.miniconda3/lib/python3.10/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: cmake in /workspace/.miniconda3/lib/python3.10/site-packages (from triton==2.0.0->torch) (3.25.0)\n",
      "Requirement already satisfied: lit in /workspace/.miniconda3/lib/python3.10/site-packages (from triton==2.0.0->torch) (15.0.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /workspace/.miniconda3/lib/python3.10/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /workspace/.miniconda3/lib/python3.10/site-packages (from sympy->torch) (1.2.1)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.3/36.3 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<1.28.0,>=1.21.6 in /workspace/.miniconda3/lib/python3.10/site-packages (from scipy) (1.24.1)\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.11.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade google-api-python-client\n",
    "!pip install bitsandbytes>=0.39.0\n",
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "!pip install git+https://github.com/huggingface/accelerate.git\n",
    "!pip install tiktoken\n",
    "!pip install torch\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul 13 13:43:54 2023       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 530.41.03              Driver Version: 530.41.03    CUDA Version: 12.1     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce RTX 3070         Off| 00000000:01:00.0 Off |                  N/A |\r\n",
      "|  0%   55C    P8               16W / 220W|     15MiB /  8192MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|    0   N/A  N/A      1165      G   /usr/lib/xorg/Xorg                            9MiB |\r\n",
      "|    0   N/A  N/A      1319      G   /usr/bin/gnome-shell                          4MiB |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:      181108932    10473132    73337164     5609268    97298636   163382608\n",
      "Swap:             0           0           0\n"
     ]
    }
   ],
   "source": [
    "!free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/linagora/.local/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/linagora/anaconda3/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /home/linagora/.local/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linagora/.local/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/home/linagora/anaconda3/lib/libcudart.so.11.0'), PosixPath('/home/linagora/anaconda3/lib/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4acd1a96ccd407ebb45b3726b7c22e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'legendhasit/xgen-7b-8k-inst-8bit'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 samples\n",
      "Found 6 instructions\n",
      "Starting computation...\n",
      "Prompting instruction N1/6 on sample N1/1\n",
      "-- Completion: 0.0%\n",
      "Prompting instruction N2/6 on sample N1/1\n",
      "-- Completion: 16.7%\n",
      "-- Estimated Remaining Time: 0:02:09 (total 0:02:35)\n",
      "Prompting instruction N3/6 on sample N1/1\n",
      "-- Completion: 33.3%\n",
      "-- Estimated Remaining Time: 0:01:28 (total 0:02:12)\n",
      "Prompting instruction N4/6 on sample N1/1\n",
      "-- Completion: 50.0%\n",
      "-- Estimated Remaining Time: 0:01:42 (total 0:03:24)\n",
      "Prompting instruction N5/6 on sample N1/1\n",
      "-- Completion: 66.7%\n",
      "-- Estimated Remaining Time: 0:01:03 (total 0:03:10)\n",
      "Prompting instruction N6/6 on sample N1/1\n",
      "-- Completion: 83.3%\n",
      "-- Estimated Remaining Time: 0:00:29 (total 0:02:54)\n",
      "Done! Took 0:03:31 seconds\n"
     ]
    }
   ],
   "source": [
    "# Parameters:\n",
    "# - DATASET_NAME\n",
    "# - HEADER\n",
    "# - PROMPT_TEMPLATE\n",
    "# - samples in dataset 'input/<dataset_name>'\n",
    "# - instructions in 'instructions.txt'\n",
    "\n",
    "def mkdir(folder_path):\n",
    "    try:\n",
    "        os.mkdir(folder_path)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "mkdir('input')\n",
    "mkdir('intermediate')\n",
    "mkdir('output')\n",
    "\n",
    "# Define prompt template\n",
    "# ==========================================================================================\n",
    "HEADER = (\n",
    "    \"A chat between a curious human and an artificial intelligence assistant. \"\n",
    "    \"The assistant gives helpful, detailed, and polite answers to the human's questions.\"\n",
    ")\n",
    "\n",
    "PROMPT_TEMPLATE = HEADER + \"\"\"\n",
    "\n",
    "### Human: {instruction}\n",
    "\n",
    "{article}\n",
    "\n",
    "### Assistant:\"\"\"\n",
    "\n",
    "# Load samples from dataset\n",
    "# ==========================================================================================\n",
    "DATASET_NAME = 'fredsum'\n",
    "samples = os.listdir('input/' + DATASET_NAME + '/texts')\n",
    "n_samples = len(samples)\n",
    "print('Found', n_samples, 'samples')\n",
    "\n",
    "# Load instructions\n",
    "# ==========================================================================================\n",
    "instruction_file = open('instructions.txt', 'r', encoding='utf-8')\n",
    "instructions = instruction_file.readlines()\n",
    "instruction_file.close()\n",
    "n_instructions = len(instructions)\n",
    "for i in range(n_instructions):\n",
    "    instructions[i] = instructions[i].replace('\\n', '')\n",
    "print('Found', n_instructions, 'instructions')\n",
    "\n",
    "# Inference\n",
    "# ==========================================================================================\n",
    "initial_time = time.time()\n",
    "skipped_samples = 0\n",
    "\n",
    "print('Starting computation...')\n",
    "\n",
    "# For each instruction\n",
    "for instruction_n in range(n_instructions):\n",
    "\n",
    "    # Read instruction and create prompt\n",
    "    instruction = instructions[instruction_n]\n",
    "    \n",
    "    # For each sample in dataset\n",
    "    for sample_n in range(n_samples):\n",
    "\n",
    "        # Estimate completion and time.\n",
    "        cur_samples = instruction_n * n_samples + sample_n - skipped_samples\n",
    "        tot_samples = n_instructions * n_samples - skipped_samples\n",
    "        progress = cur_samples / tot_samples\n",
    "        pct = round(progress * 100, 1)\n",
    "        print('Prompting instruction N' + str(instruction_n + 1) + '/' + str(n_instructions) + ' on sample N' + str(sample_n + 1) + '/' + str(n_samples))\n",
    "        print('-- Completion: ' + str(pct) + '%')\n",
    "        if cur_samples > 0:\n",
    "            approx_total = (time.time() - initial_time) / cur_samples * tot_samples\n",
    "            approx_remaining = approx_total * (1 - progress)\n",
    "            print('-- Estimated Remaining Time: ' + str(datetime.timedelta(seconds=int(approx_remaining))) + ' (total ' + str(datetime.timedelta(seconds=int(approx_total))) + ')')\n",
    "        \n",
    "        # Read sample and generate prompt\n",
    "        sample_file_path = 'input/' + DATASET_NAME + '/texts/' + samples[sample_n]\n",
    "        sample_file = open(sample_file_path, 'r', encoding='utf-8')\n",
    "        sample = sample_file.read()\n",
    "        sample_file.close()\n",
    "        prompt = PROMPT_TEMPLATE.format(instruction=instruction, article=sample)\n",
    "        \n",
    "        # Find target file\n",
    "        target_file_path = 'intermediate/' + str(instruction_n + 1) + '_' + str(sample_n + 1) + '.txt'\n",
    "        if os.path.isfile(target_file_path):\n",
    "            print('-- Found intermediate result file \\'' + target_file_path + '\\', skipped.')\n",
    "            skipped_samples += 1\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "        \n",
    "            # Sample one answer\n",
    "            input_ids = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "            sample = model.generate(**input_ids, do_sample=True, max_new_tokens=2048, top_k=100, eos_token_id=50256, temperature=0.3)\n",
    "            output = tokenizer.decode(sample[0]).strip()\n",
    "\n",
    "            # Save answer in file\n",
    "            target_file = open(target_file_path, 'w', encoding='utf-8')\n",
    "            target_file.write(output)\n",
    "            target_file.close()\n",
    "            \n",
    "            del input_ids\n",
    "            del sample\n",
    "            \n",
    "            gc.collect()\n",
    "        \n",
    "        except:\n",
    "            print('Could not compute prompt:')\n",
    "            print(prompt)\n",
    "            traceback.print_exc()\n",
    "\n",
    "delta = time.time() - initial_time\n",
    "print('Done! Took', datetime.timedelta(seconds=int(delta)), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del input_ids\n",
    "del sample\n",
    "del output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculs de scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rouge\n",
    "!pip install evaluate\n",
    "!pip install bert-score\n",
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from rouge import Rouge\n",
    "from bert_score import score\n",
    "from sacrebleu.metrics import BLEU, CHRF, TER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Starting scores computation...')\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "storage_file = open('output/scores.csv', 'w', encoding='utf-8')\n",
    "\n",
    "# For each instruction\n",
    "for instruction_n in range(n_instructions):\n",
    "\n",
    "    # Read instruction and create prompt\n",
    "    instruction = instructions[instruction_n]\n",
    "    \n",
    "    # For each sample in dataset\n",
    "    for sample_n in range(n_samples):\n",
    "        \n",
    "        # Read sample and generate prompt -> Keep summary \n",
    "        summary_file_path = 'input/' + DATASET_NAME + '/summaries/' + samples[sample_n]\n",
    "        summary_file = open(summary_file_path, 'r', encoding='utf-8')\n",
    "        summary = summary_file.read()\n",
    "        summary_file.close()\n",
    "        \n",
    "        # Find target file\n",
    "        compute_hash = hash(instruction + sample)\n",
    "        target_file_path = 'intermediate/' + str(instruction_hash) + '.txt'\n",
    "        if not os.path.isfile(target_file_path): # A MODIFIER : SI UN RESUME N'A PAS ETE GENERE\n",
    "            print('-- Did not Find intermediate result file \\'' + target_file_path + '\\', skipped.')\n",
    "            continue\n",
    "\n",
    "        # Access generated summary\n",
    "        target_file = open(target_file_path, 'r', encoding='utf-8')\n",
    "        output = target_file.read()\n",
    "        target_file.close()\n",
    "\n",
    "        answers = output.split(\"### Assistant:\") # indice 0 : header + instruction + texte, indices suivants : réponses successives de l'assistant\n",
    "        answer = \"\"\n",
    "        for a in range(1, len(answers)):\n",
    "            replica_brut = answers[a]\n",
    "            replica = answerbrut[:-14] # Retirer le token end of text et le \\n à la fin.\n",
    "            answer+=replica\n",
    "\n",
    "        # Compare summary and answer\n",
    "        ligne = compute_hash + \";\" + compare(summary, answer)\n",
    "        storage_file.write(ligne + '\\n') # REMARQUE SUR LE WRITE ?\n",
    "\n",
    "print('Done!')\n",
    "\n",
    "\n",
    "def compare(sum, ans):\n",
    "    ligne = \"\"\n",
    "    result_bleu = bleu.compute(predictions=[ans], references=[[sum]])\n",
    "    result_rouge = rouge.get_scores(ans, sum)\n",
    "    P_bert, R_bert, F1_bert = score(ans, sum, lang='en', verbose=True)# Bcp plus d'options possibles\n",
    "    ligne += result_rouge[0]['rouge-2']['r'] + \";\" + result_rouge[0]['rouge-2']['p'] + \";\" + result_rouge[0]['rouge-2']['f'] # Stats recall, precision, f de rouge-2\n",
    "    ligne += \";\" + result_rouge[0]['rouge-l']['r'] + \";\" + result_rouge[0]['rouge-l']['p'] + \";\" + result_rouge[0]['rouge-l']['f'] # Stats recall, precision, f de rouge-l\n",
    "    ligne += \";\" + result_bleu.score # BLEU score\n",
    "    ligne += \";\" + str(R_bert[0].item()) + \";\" + str(P_bert[0].item()) + \";\" + str(F1_bert[0].item()) # Stats recall, precision, f de BERTScore\n",
    "    return ligne"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
