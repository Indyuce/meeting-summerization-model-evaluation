{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-api-python-client\n",
      "  Downloading google_api_python_client-2.92.0-py2.py3-none-any.whl (11.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting httplib2<1.dev0,>=0.15.0 (from google-api-python-client)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.9/96.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth<3.0.0.dev0,>=1.19.0 (from google-api-python-client)\n",
      "  Downloading google_auth-2.21.0-py2.py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.1/182.1 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-httplib2>=0.1.0 (from google-api-python-client)\n",
      "  Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 (from google-api-python-client)\n",
      "  Downloading google_api_core-2.11.1-py3-none-any.whl (120 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.5/120.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting uritemplate<5,>=3.0.1 (from google-api-python-client)\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Downloading googleapis_common_protos-1.59.1-py2.py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Downloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /workspace/.miniconda3/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.28.1)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client)\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client)\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4 (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client)\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in /workspace/.miniconda3/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (1.16.0)\n",
      "Requirement already satisfied: urllib3<2.0 in /workspace/.miniconda3/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (1.26.13)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /workspace/.miniconda3/lib/python3.10/site-packages (from httplib2<1.dev0,>=0.15.0->google-api-python-client) (3.0.9)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client)\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<3,>=2 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2022.12.7)\n",
      "Installing collected packages: uritemplate, pyasn1, protobuf, httplib2, cachetools, rsa, pyasn1-modules, googleapis-common-protos, google-auth, google-auth-httplib2, google-api-core, google-api-python-client\n",
      "Successfully installed cachetools-5.3.1 google-api-core-2.11.1 google-api-python-client-2.92.0 google-auth-2.21.0 google-auth-httplib2-0.1.0 googleapis-common-protos-1.59.1 httplib2-0.22.0 protobuf-4.23.4 pyasn1-0.5.0 pyasn1-modules-0.3.0 rsa-4.9 uritemplate-4.1.1\n",
      "Collecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-uaf34nd3\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-uaf34nd3\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 30ed3adf474aaf2972ab56f5624089bc24a6adf3\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /workspace/.miniconda3/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers==4.31.0.dev0)\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /workspace/.miniconda3/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /workspace/.miniconda3/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (23.1)\n",
      "Collecting pyyaml>=5.1 (from transformers==4.31.0.dev0)\n",
      "  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17 (from transformers==4.31.0.dev0)\n",
      "  Downloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.4/770.4 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /workspace/.miniconda3/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (2.28.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31.0.dev0)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers==4.31.0.dev0)\n",
      "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /workspace/.miniconda3/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (4.51.0)\n",
      "Collecting fsspec (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0.dev0)\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /workspace/.miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0.dev0) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests->transformers==4.31.0.dev0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests->transformers==4.31.0.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests->transformers==4.31.0.dev0) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests->transformers==4.31.0.dev0) (2022.12.7)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.31.0.dev0-py3-none-any.whl size=7308539 sha256=572e932b29f9d5a981fbb63006ea1a3b03ef2d0abaa21807b30de10eb726b3fd\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-mc0_du5j/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\n",
      "Successfully built transformers\n",
      "Installing collected packages: tokenizers, safetensors, regex, pyyaml, fsspec, huggingface-hub, transformers\n",
      "Successfully installed fsspec-2023.6.0 huggingface-hub-0.16.4 pyyaml-6.0 regex-2023.6.3 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0.dev0\n",
      "Collecting git+https://github.com/huggingface/accelerate.git\n",
      "  Cloning https://github.com/huggingface/accelerate.git to /tmp/pip-req-build-t6nc09mj\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate.git /tmp/pip-req-build-t6nc09mj\n",
      "  Resolved https://github.com/huggingface/accelerate.git to commit 95bffdec4326acf6a5d1c3dbaa857a26502aa265\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /workspace/.miniconda3/lib/python3.10/site-packages (from accelerate==0.21.0.dev0) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /workspace/.miniconda3/lib/python3.10/site-packages (from accelerate==0.21.0.dev0) (23.1)\n",
      "Requirement already satisfied: psutil in /workspace/.miniconda3/lib/python3.10/site-packages (from accelerate==0.21.0.dev0) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /workspace/.miniconda3/lib/python3.10/site-packages (from accelerate==0.21.0.dev0) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /workspace/.miniconda3/lib/python3.10/site-packages (from accelerate==0.21.0.dev0) (2.0.0+cu118)\n",
      "Requirement already satisfied: filelock in /workspace/.miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0.dev0) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /workspace/.miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0.dev0) (4.4.0)\n",
      "Requirement already satisfied: sympy in /workspace/.miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0.dev0) (1.11.1)\n",
      "Requirement already satisfied: networkx in /workspace/.miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0.dev0) (3.0)\n",
      "Requirement already satisfied: jinja2 in /workspace/.miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0.dev0) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /workspace/.miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0.dev0) (2.0.0)\n",
      "Requirement already satisfied: cmake in /workspace/.miniconda3/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.21.0.dev0) (3.25.0)\n",
      "Requirement already satisfied: lit in /workspace/.miniconda3/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.21.0.dev0) (15.0.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /workspace/.miniconda3/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.21.0.dev0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /workspace/.miniconda3/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.21.0.dev0) (1.2.1)\n",
      "Building wheels for collected packages: accelerate\n",
      "  Building wheel for accelerate (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for accelerate: filename=accelerate-0.21.0.dev0-py3-none-any.whl size=241741 sha256=6f54a2971974f8f9c7eb365719725e8a5d0357ab55bc1718cc51e01e959ee2b1\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-im_59kwc/wheels/9c/a3/1e/47368f9b6575655fe9ee1b6350cfa7d4b0befe66a35f8a8365\n",
      "Successfully built accelerate\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.21.0.dev0\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /workspace/.miniconda3/lib/python3.10/site-packages (from tiktoken) (2023.6.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /workspace/.miniconda3/lib/python3.10/site-packages (from tiktoken) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /workspace/.miniconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.4.0\n",
      "Requirement already satisfied: torch in /workspace/.miniconda3/lib/python3.10/site-packages (2.0.0+cu118)\n",
      "Requirement already satisfied: filelock in /workspace/.miniconda3/lib/python3.10/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /workspace/.miniconda3/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: sympy in /workspace/.miniconda3/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /workspace/.miniconda3/lib/python3.10/site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /workspace/.miniconda3/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /workspace/.miniconda3/lib/python3.10/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: cmake in /workspace/.miniconda3/lib/python3.10/site-packages (from triton==2.0.0->torch) (3.25.0)\n",
      "Requirement already satisfied: lit in /workspace/.miniconda3/lib/python3.10/site-packages (from triton==2.0.0->torch) (15.0.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /workspace/.miniconda3/lib/python3.10/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /workspace/.miniconda3/lib/python3.10/site-packages (from sympy->torch) (1.2.1)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.3/36.3 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<1.28.0,>=1.21.6 in /workspace/.miniconda3/lib/python3.10/site-packages (from scipy) (1.24.1)\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.11.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade google-api-python-client\n",
    "!pip install bitsandbytes>=0.39.0\n",
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "!pip install git+https://github.com/huggingface/accelerate.git\n",
    "!pip install tiktoken\n",
    "!pip install torch\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul 13 13:43:54 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.41.03              Driver Version: 530.41.03    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3070         Off| 00000000:01:00.0 Off |                  N/A |\n",
      "|  0%   55C    P8               16W / 220W|     15MiB /  8192MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1165      G   /usr/lib/xorg/Xorg                            9MiB |\n",
      "|    0   N/A  N/A      1319      G   /usr/bin/gnome-shell                          4MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:      181108932    10473132    73337164     5609268    97298636   163382608\n",
      "Swap:             0           0           0\n"
     ]
    }
   ],
   "source": [
    "!free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "import traceback\n",
    "import gc\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /workspace/.miniconda3/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.0\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /workspace/.miniconda3/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.miniconda3/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/workspace/.miniconda3/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/workspace/.miniconda3/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//10.43.0.1'), PosixPath('443'), PosixPath('tcp')}\n",
      "  warn(msg)\n",
      "/workspace/.miniconda3/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('https')}\n",
      "  warn(msg)\n",
      "/workspace/.miniconda3/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('repo/python/xgen_find_prompt_zero_shot/6bf19858-5016-4926-a649-45880b1d8f08')}\n",
      "  warn(msg)\n",
      "/workspace/.miniconda3/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/workspace/.miniconda3/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364adf85df18484099d8dcf6aed96bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Loading checkpoint shards'), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = 'legendhasit/xgen-7b-8k-inst-8bit'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 samples\n",
      "Found 6 instructions\n"
     ]
    }
   ],
   "source": [
    "# Parameters:\n",
    "# - DATASET_NAME\n",
    "# - HEADER\n",
    "# - PROMPT_TEMPLATE\n",
    "# - samples in dataset 'input/<dataset_name>'\n",
    "# - instructions in 'instructions.txt'\n",
    "\n",
    "def mkdir(folder_path):\n",
    "    try:\n",
    "        os.mkdir(folder_path)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "mkdir('input')\n",
    "\n",
    "# Define prompt template\n",
    "# ==========================================================================================\n",
    "HEADER = (\n",
    "    \"A chat between a curious human and an artificial intelligence assistant. \"\n",
    "    \"The assistant gives helpful, detailed, and polite answers to the human's questions.\"\n",
    ")\n",
    "\n",
    "PROMPT_TEMPLATE = HEADER + \"\"\"\n",
    "\n",
    "### Human: {instruction}\n",
    "\n",
    "{article}\n",
    "\n",
    "### Assistant:\"\"\"\n",
    "\n",
    "# Load samples from dataset\n",
    "# ==========================================================================================\n",
    "DATASET_NAME = 'fredsum'\n",
    "samples = os.listdir('input/' + DATASET_NAME + '/texts')\n",
    "n_samples = len(samples)\n",
    "print('Found', n_samples, 'samples')\n",
    "\n",
    "# Load instructions\n",
    "# ==========================================================================================\n",
    "instruction_file = open('instructions.txt', 'r', encoding='utf-8')\n",
    "instructions = instruction_file.readlines()\n",
    "instruction_file.close()\n",
    "n_instructions = len(instructions)\n",
    "for i in range(n_instructions):\n",
    "    instructions[i] = instructions[i].replace('\\n', '')\n",
    "print('Found', n_instructions, 'instructions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting computation...\n",
      "Prompting instruction N1/6 on sample N1/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/1_1.txt', skipped.\n",
      "Prompting instruction N1/6 on sample N2/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/1_2.txt', skipped.\n",
      "Prompting instruction N1/6 on sample N3/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/1_3.txt', skipped.\n",
      "Prompting instruction N1/6 on sample N4/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/1_4.txt', skipped.\n",
      "Prompting instruction N1/6 on sample N5/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/1_5.txt', skipped.\n",
      "Prompting instruction N1/6 on sample N6/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/1_6.txt', skipped.\n",
      "Prompting instruction N1/6 on sample N7/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/1_7.txt', skipped.\n",
      "Prompting instruction N1/6 on sample N8/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/1_8.txt', skipped.\n",
      "Prompting instruction N1/6 on sample N9/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/1_9.txt', skipped.\n",
      "Prompting instruction N1/6 on sample N10/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/1_10.txt', skipped.\n",
      "Prompting instruction N2/6 on sample N1/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/2_1.txt', skipped.\n",
      "Prompting instruction N2/6 on sample N2/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/2_2.txt', skipped.\n",
      "Prompting instruction N2/6 on sample N3/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/2_3.txt', skipped.\n",
      "Prompting instruction N2/6 on sample N4/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/2_4.txt', skipped.\n",
      "Prompting instruction N2/6 on sample N5/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/2_5.txt', skipped.\n",
      "Prompting instruction N2/6 on sample N6/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/2_6.txt', skipped.\n",
      "Prompting instruction N2/6 on sample N7/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/2_7.txt', skipped.\n",
      "Prompting instruction N2/6 on sample N8/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/2_8.txt', skipped.\n",
      "Prompting instruction N2/6 on sample N9/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/2_9.txt', skipped.\n",
      "Prompting instruction N2/6 on sample N10/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/2_10.txt', skipped.\n",
      "Prompting instruction N3/6 on sample N1/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/3_1.txt', skipped.\n",
      "Prompting instruction N3/6 on sample N2/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/3_2.txt', skipped.\n",
      "Prompting instruction N3/6 on sample N3/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/3_3.txt', skipped.\n",
      "Prompting instruction N3/6 on sample N4/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/3_4.txt', skipped.\n",
      "Prompting instruction N3/6 on sample N5/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/3_5.txt', skipped.\n",
      "Prompting instruction N3/6 on sample N6/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/3_6.txt', skipped.\n",
      "Prompting instruction N3/6 on sample N7/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/3_7.txt', skipped.\n",
      "Prompting instruction N3/6 on sample N8/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/3_8.txt', skipped.\n",
      "Prompting instruction N3/6 on sample N9/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/3_9.txt', skipped.\n",
      "Prompting instruction N3/6 on sample N10/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/3_10.txt', skipped.\n",
      "Prompting instruction N4/6 on sample N1/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/4_1.txt', skipped.\n",
      "Prompting instruction N4/6 on sample N2/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/4_2.txt', skipped.\n",
      "Prompting instruction N4/6 on sample N3/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/4_3.txt', skipped.\n",
      "Prompting instruction N4/6 on sample N4/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/4_4.txt', skipped.\n",
      "Prompting instruction N4/6 on sample N5/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/4_5.txt', skipped.\n",
      "Prompting instruction N4/6 on sample N6/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/4_6.txt', skipped.\n",
      "Prompting instruction N4/6 on sample N7/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/4_7.txt', skipped.\n",
      "Prompting instruction N4/6 on sample N8/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/4_8.txt', skipped.\n",
      "Prompting instruction N4/6 on sample N9/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/4_9.txt', skipped.\n",
      "Prompting instruction N4/6 on sample N10/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/4_10.txt', skipped.\n",
      "Prompting instruction N5/6 on sample N1/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/5_1.txt', skipped.\n",
      "Prompting instruction N5/6 on sample N2/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/5_2.txt', skipped.\n",
      "Prompting instruction N5/6 on sample N3/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/5_3.txt', skipped.\n",
      "Prompting instruction N5/6 on sample N4/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/5_4.txt', skipped.\n",
      "Prompting instruction N5/6 on sample N5/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/5_5.txt', skipped.\n",
      "Prompting instruction N5/6 on sample N6/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/5_6.txt', skipped.\n",
      "Prompting instruction N5/6 on sample N7/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/5_7.txt', skipped.\n",
      "Prompting instruction N5/6 on sample N8/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/5_8.txt', skipped.\n",
      "Prompting instruction N5/6 on sample N9/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/5_9.txt', skipped.\n",
      "Prompting instruction N5/6 on sample N10/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/5_10.txt', skipped.\n",
      "Prompting instruction N6/6 on sample N1/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/6_1.txt', skipped.\n",
      "Prompting instruction N6/6 on sample N2/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/6_2.txt', skipped.\n",
      "Prompting instruction N6/6 on sample N3/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/6_3.txt', skipped.\n",
      "Prompting instruction N6/6 on sample N4/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/6_4.txt', skipped.\n",
      "Prompting instruction N6/6 on sample N5/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/6_5.txt', skipped.\n",
      "Prompting instruction N6/6 on sample N6/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/6_6.txt', skipped.\n",
      "Prompting instruction N6/6 on sample N7/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/6_7.txt', skipped.\n",
      "Prompting instruction N6/6 on sample N8/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/6_8.txt', skipped.\n",
      "Prompting instruction N6/6 on sample N9/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/6_9.txt', skipped.\n",
      "Prompting instruction N6/6 on sample N10/10\n",
      "-- Completion: 0.0%\n",
      "-- Found intermediate result file 'intermediate/6_10.txt', skipped.\n",
      "Done! Took 0:00:00 seconds\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "# ==========================================================================================\n",
    "initial_time = time.time()\n",
    "skipped_samples = 0\n",
    "\n",
    "mkdir('intermediate')\n",
    "mkdir('intermediate/' + DATASET_NAME)\n",
    "\n",
    "print('Starting computation...')\n",
    "\n",
    "# For each instruction\n",
    "for instruction_n in range(n_instructions):\n",
    "\n",
    "    # Read instruction and create prompt\n",
    "    instruction = instructions[instruction_n]\n",
    "    \n",
    "    # For each sample in dataset\n",
    "    for sample_n in range(n_samples):\n",
    "\n",
    "        # Estimate completion and time.\n",
    "        cur_samples = instruction_n * n_samples + sample_n - skipped_samples\n",
    "        tot_samples = n_instructions * n_samples - skipped_samples\n",
    "        progress = cur_samples / tot_samples\n",
    "        pct = round(progress * 100, 1)\n",
    "        print('Prompting instruction N' + str(instruction_n + 1) + '/' + str(n_instructions) + ' on sample N' + str(sample_n + 1) + '/' + str(n_samples))\n",
    "        print('-- Completion: ' + str(pct) + '%')\n",
    "        if cur_samples > 0:\n",
    "            approx_total = (time.time() - initial_time) / cur_samples * tot_samples\n",
    "            approx_remaining = approx_total * (1 - progress)\n",
    "            print('-- Estimated Remaining Time: ' + str(datetime.timedelta(seconds=int(approx_remaining))) + ' (total ' + str(datetime.timedelta(seconds=int(approx_total))) + ')')\n",
    "        \n",
    "        # Read sample and generate prompt\n",
    "        sample_file_path = 'input/' + DATASET_NAME + '/texts/' + samples[sample_n]\n",
    "        sample_file = open(sample_file_path, 'r', encoding='utf-8')\n",
    "        sample = sample_file.read()\n",
    "        sample_file.close()\n",
    "        prompt = PROMPT_TEMPLATE.format(instruction=instruction, article=sample)\n",
    "        \n",
    "        # Find target file\n",
    "        target_file_path = 'intermediate/' + DATASET_NAME + '/' + str(instruction_n + 1) + '_' + str(sample_n + 1) + '.txt'\n",
    "        if os.path.isfile(target_file_path):\n",
    "            print('-- Found intermediate result file \\'' + target_file_path + '\\', skipped.')\n",
    "            skipped_samples += 1\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "        \n",
    "            # Sample one answer\n",
    "            input_ids = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "            sample = model.generate(**input_ids, do_sample=True, max_new_tokens=2048, top_k=100, eos_token_id=50256, temperature=0.3)\n",
    "            output = tokenizer.decode(sample[0]).strip()\n",
    "\n",
    "            # Save answer in file\n",
    "            target_file = open(target_file_path, 'w', encoding='utf-8')\n",
    "            target_file.write(output)\n",
    "            target_file.close()\n",
    "\n",
    "            del input_ids\n",
    "            del sample\n",
    "        \n",
    "        except:\n",
    "            print('Could not compute prompt:')\n",
    "            print(prompt)\n",
    "            traceback.print_exc()\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "delta = time.time() - initial_time\n",
    "print('Done! Took', datetime.timedelta(seconds=int(delta)), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del input_ids\n",
    "del sample\n",
    "del output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculs de scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: absl-py in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rouge_score) (1.2.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rouge_score) (1.23.3)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->rouge_score) (8.1.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\jules\\appdata\\roaming\\python\\python310\\site-packages (from nltk->rouge_score) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->rouge_score) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->rouge_score) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jules\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk->rouge_score) (0.4.6)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py): started\n",
      "  Building wheel for rouge_score (setup.py): finished with status 'done'\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24936 sha256=7afcf4fd12bb20d962442dc2662012abd1ec10704de49142044f1fad7b14b3dc\n",
      "  Stored in directory: c:\\users\\jules\\appdata\\local\\pip\\cache\\wheels\\5f\\dd\\89\\461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score\n",
      "Successfully installed rouge_score-0.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from evaluate) (2.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from evaluate) (1.23.3)\n",
      "Requirement already satisfied: dill in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from evaluate) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from evaluate) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from evaluate) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from evaluate) (4.65.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from evaluate) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from evaluate) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from evaluate) (2023.5.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from evaluate) (0.15.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets>=2.0.0->evaluate) (12.0.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging->evaluate) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->evaluate) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->evaluate) (2022.9.24)\n",
      "Requirement already satisfied: colorama in c:\\users\\jules\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->evaluate) (2022.7.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-score in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: torch>=1.0.0 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bert-score) (2.0.1+cu117)\n",
      "Requirement already satisfied: pandas>=1.0.1 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bert-score) (1.5.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bert-score) (4.29.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bert-score) (1.23.3)\n",
      "Requirement already satisfied: requests in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bert-score) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bert-score) (4.65.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\jules\\appdata\\roaming\\python\\python310\\site-packages (from bert-score) (3.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bert-score) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.9->bert-score) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.0.1->bert-score) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.0.1->bert-score) (2022.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.0.0->bert-score) (4.4.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.0.0->bert-score) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\jules\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.31.1->bert-score) (0.4.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers>=3.0.0->bert-score) (0.15.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers>=3.0.0->bert-score) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers>=3.0.0->bert-score) (2023.6.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers>=3.0.0->bert-score) (0.13.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jules\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib->bert-score) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jules\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib->bert-score) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jules\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib->bert-score) (4.39.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\jules\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib->bert-score) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->bert-score) (9.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->bert-score) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->bert-score) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->bert-score) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->bert-score) (2022.9.24)\n",
      "Requirement already satisfied: fsspec in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers>=3.0.0->bert-score) (2023.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.0.1->bert-score) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch>=1.0.0->bert-score) (1.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacrebleu in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: portalocker in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacrebleu) (2.7.0)\n",
      "Requirement already satisfied: regex in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacrebleu) (2023.6.3)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacrebleu) (1.23.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\jules\\appdata\\roaming\\python\\python310\\site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacrebleu) (4.9.1)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\jules\\appdata\\roaming\\python\\python310\\site-packages (from portalocker->sacrebleu) (306)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\jules\\appdata\\roaming\\python\\python310\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jules\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jules\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score rouge\n",
    "!pip install evaluate\n",
    "!pip install bert-score\n",
    "!pip install sacrebleu\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scores computation...\n",
      "-- Found no intermediate result file 'intermediate/fredsum/fr/1_10.txt', skipped.\n",
      "-- Found no intermediate result file 'intermediate/fredsum/fr/2_10.txt', skipped.\n",
      "-- Found no intermediate result file 'intermediate/fredsum/fr/3_10.txt', skipped.\n",
      "-- Found no intermediate result file 'intermediate/fredsum/fr/4_10.txt', skipped.\n",
      "-- Found no intermediate result file 'intermediate/fredsum/fr/5_10.txt', skipped.\n",
      "-- Found no intermediate result file 'intermediate/fredsum/fr/6_10.txt', skipped.\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:42<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 72\u001b[0m\n\u001b[0;32m     70\u001b[0m result_rouge \u001b[39m=\u001b[39m rouge\u001b[39m.\u001b[39mcompute(predictions\u001b[39m=\u001b[39mpredictions, references\u001b[39m=\u001b[39mreferences)\n\u001b[0;32m     71\u001b[0m result_bleu \u001b[39m=\u001b[39m bleu\u001b[39m.\u001b[39mcompute(predictions\u001b[39m=\u001b[39mpredictions, references\u001b[39m=\u001b[39mreferences)\n\u001b[1;32m---> 72\u001b[0m result_bertscore \u001b[39m=\u001b[39m bertscore\u001b[39m.\u001b[39;49mcompute(predictions\u001b[39m=\u001b[39;49mpredictions, references\u001b[39m=\u001b[39;49mreferences, lang\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mfr\u001b[39;49m\u001b[39m'\u001b[39;49m, rescale_with_baseline\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     74\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m \n\u001b[0;32m     76\u001b[0m     \u001b[39m# Write to csv\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     \u001b[39m# Format: PATH | ROUGE2 | ROUGEL | BLEU | BERTScore\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(target_file_paths)):\n",
      "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\evaluate\\module.py:444\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    442\u001b[0m inputs \u001b[39m=\u001b[39m {input_name: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[input_name] \u001b[39mfor\u001b[39;00m input_name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feature_names()}\n\u001b[0;32m    443\u001b[0m \u001b[39mwith\u001b[39;00m temp_seed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseed):\n\u001b[1;32m--> 444\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcompute_kwargs)\n\u001b[0;32m    446\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_writer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    447\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_writer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--bertscore\\cf4907b18f8f741f202232c0f8009a3bd49ff98802c245abcb6ea51a37a8c05b\\bertscore.py:203\u001b[0m, in \u001b[0;36mBERTScore._compute\u001b[1;34m(self, predictions, references, lang, model_type, num_layers, verbose, idf, device, batch_size, nthreads, all_layers, rescale_with_baseline, baseline_path, use_fast_tokenizer)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcached_bertscorer\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcached_bertscorer\u001b[39m.\u001b[39mhash \u001b[39m!=\u001b[39m hashcode:\n\u001b[0;32m    189\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcached_bertscorer \u001b[39m=\u001b[39m scorer(\n\u001b[0;32m    190\u001b[0m             model_type\u001b[39m=\u001b[39mmodel_type,\n\u001b[0;32m    191\u001b[0m             num_layers\u001b[39m=\u001b[39mnum_layers,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    200\u001b[0m             baseline_path\u001b[39m=\u001b[39mbaseline_path,\n\u001b[0;32m    201\u001b[0m         )\n\u001b[1;32m--> 203\u001b[0m (P, R, F) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcached_bertscorer\u001b[39m.\u001b[39;49mscore(\n\u001b[0;32m    204\u001b[0m     cands\u001b[39m=\u001b[39;49mpredictions,\n\u001b[0;32m    205\u001b[0m     refs\u001b[39m=\u001b[39;49mreferences,\n\u001b[0;32m    206\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    207\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m    208\u001b[0m )\n\u001b[0;32m    209\u001b[0m output_dict \u001b[39m=\u001b[39m {\n\u001b[0;32m    210\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mprecision\u001b[39m\u001b[39m\"\u001b[39m: P\u001b[39m.\u001b[39mtolist(),\n\u001b[0;32m    211\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrecall\u001b[39m\u001b[39m\"\u001b[39m: R\u001b[39m.\u001b[39mtolist(),\n\u001b[0;32m    212\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mf1\u001b[39m\u001b[39m\"\u001b[39m: F\u001b[39m.\u001b[39mtolist(),\n\u001b[0;32m    213\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mhashcode\u001b[39m\u001b[39m\"\u001b[39m: hashcode,\n\u001b[0;32m    214\u001b[0m }\n\u001b[0;32m    215\u001b[0m \u001b[39mreturn\u001b[39;00m output_dict\n",
      "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bert_score\\scorer.py:220\u001b[0m, in \u001b[0;36mBERTScorer.score\u001b[1;34m(self, cands, refs, verbose, batch_size, return_hash)\u001b[0m\n\u001b[0;32m    217\u001b[0m     idf_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer\u001b[39m.\u001b[39msep_token_id] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    218\u001b[0m     idf_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer\u001b[39m.\u001b[39mcls_token_id] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m--> 220\u001b[0m all_preds \u001b[39m=\u001b[39m bert_cos_score_idf(\n\u001b[0;32m    221\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model,\n\u001b[0;32m    222\u001b[0m     refs,\n\u001b[0;32m    223\u001b[0m     cands,\n\u001b[0;32m    224\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer,\n\u001b[0;32m    225\u001b[0m     idf_dict,\n\u001b[0;32m    226\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    227\u001b[0m     device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice,\n\u001b[0;32m    228\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m    229\u001b[0m     all_layers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mall_layers,\n\u001b[0;32m    230\u001b[0m )\u001b[39m.\u001b[39mcpu()\n\u001b[0;32m    232\u001b[0m \u001b[39mif\u001b[39;00m ref_group_boundaries \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     max_preds \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bert_score\\utils.py:616\u001b[0m, in \u001b[0;36mbert_cos_score_idf\u001b[1;34m(model, refs, hyps, tokenizer, idf_dict, verbose, batch_size, device, all_layers)\u001b[0m\n\u001b[0;32m    614\u001b[0m \u001b[39mfor\u001b[39;00m batch_start \u001b[39min\u001b[39;00m iter_range:\n\u001b[0;32m    615\u001b[0m     sen_batch \u001b[39m=\u001b[39m sentences[batch_start : batch_start \u001b[39m+\u001b[39m batch_size]\n\u001b[1;32m--> 616\u001b[0m     embs, masks, padded_idf \u001b[39m=\u001b[39m get_bert_embedding(\n\u001b[0;32m    617\u001b[0m         sen_batch, model, tokenizer, idf_dict, device\u001b[39m=\u001b[39;49mdevice, all_layers\u001b[39m=\u001b[39;49mall_layers\n\u001b[0;32m    618\u001b[0m     )\n\u001b[0;32m    619\u001b[0m     embs \u001b[39m=\u001b[39m embs\u001b[39m.\u001b[39mcpu()\n\u001b[0;32m    620\u001b[0m     masks \u001b[39m=\u001b[39m masks\u001b[39m.\u001b[39mcpu()\n",
      "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bert_score\\utils.py:455\u001b[0m, in \u001b[0;36mget_bert_embedding\u001b[1;34m(all_sens, model, tokenizer, idf_dict, batch_size, device, all_layers)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m    454\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(all_sens), batch_size):\n\u001b[1;32m--> 455\u001b[0m         batch_embedding \u001b[39m=\u001b[39m bert_encode(\n\u001b[0;32m    456\u001b[0m             model,\n\u001b[0;32m    457\u001b[0m             padded_sens[i : i \u001b[39m+\u001b[39;49m batch_size],\n\u001b[0;32m    458\u001b[0m             attention_mask\u001b[39m=\u001b[39;49mmask[i : i \u001b[39m+\u001b[39;49m batch_size],\n\u001b[0;32m    459\u001b[0m             all_layers\u001b[39m=\u001b[39;49mall_layers,\n\u001b[0;32m    460\u001b[0m         )\n\u001b[0;32m    461\u001b[0m         embeddings\u001b[39m.\u001b[39mappend(batch_embedding)\n\u001b[0;32m    462\u001b[0m         \u001b[39mdel\u001b[39;00m batch_embedding\n",
      "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bert_score\\utils.py:351\u001b[0m, in \u001b[0;36mbert_encode\u001b[1;34m(model, x, attention_mask, all_layers)\u001b[0m\n\u001b[0;32m    349\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m    350\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 351\u001b[0m     out \u001b[39m=\u001b[39m model(x, attention_mask\u001b[39m=\u001b[39;49mattention_mask, output_hidden_states\u001b[39m=\u001b[39;49mall_layers)\n\u001b[0;32m    352\u001b[0m \u001b[39mif\u001b[39;00m all_layers:\n\u001b[0;32m    353\u001b[0m     emb \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(out[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1020\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1011\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1013\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m   1014\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1015\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1019\u001b[0m )\n\u001b[1;32m-> 1020\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m   1021\u001b[0m     embedding_output,\n\u001b[0;32m   1022\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   1023\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1024\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1025\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   1026\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1027\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1028\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1029\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1030\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1031\u001b[0m )\n\u001b[0;32m   1032\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1033\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    601\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    603\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    608\u001b[0m     )\n\u001b[0;32m    609\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 610\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    611\u001b[0m         hidden_states,\n\u001b[0;32m    612\u001b[0m         attention_mask,\n\u001b[0;32m    613\u001b[0m         layer_head_mask,\n\u001b[0;32m    614\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    615\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    616\u001b[0m         past_key_value,\n\u001b[0;32m    617\u001b[0m         output_attentions,\n\u001b[0;32m    618\u001b[0m     )\n\u001b[0;32m    620\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    621\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:537\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    534\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    535\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 537\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[0;32m    538\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[0;32m    539\u001b[0m )\n\u001b[0;32m    540\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[0;32m    542\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pytorch_utils.py:236\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    234\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 236\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:550\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[0;32m    549\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 550\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput(intermediate_output, attention_output)\n\u001b[0;32m    551\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:462\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor, input_tensor: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m--> 462\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[0;32m    463\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    464\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(hidden_states \u001b[39m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Method and variables\n",
    "# ==========================================================================================\n",
    "print('Starting scores computation...')\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load('rouge')\n",
    "bertscore = evaluate.load('bertscore')\n",
    "\n",
    "SUMMARIES_PER_SAMPLE = 4\n",
    "INTERMEDIATE_SUBPATH = 'fr/'\n",
    "STORAGE_FILE_NAME = 'scores_fr'\n",
    "PREPROCESS_SUMMARIES = False\n",
    "\n",
    "# Script itself\n",
    "# ==========================================================================================\n",
    "\n",
    "# Find output file for CSV scores\n",
    "mkdir('output')\n",
    "mkdir('output/' + DATASET_NAME)\n",
    "storage_file = open('output/' + DATASET_NAME + '/' + STORAGE_FILE_NAME + '.csv', 'w', encoding='utf-8')\n",
    "storage_file.write('path;rouge2;rougel;bleu;bertscore\\n')\n",
    "\n",
    "target_file_paths = []\n",
    "references = []\n",
    "predictions = []\n",
    "\n",
    "# For each instruction\n",
    "for instruction_n in range(n_instructions):\n",
    "\n",
    "    # Read instruction and create prompt\n",
    "    instruction = instructions[instruction_n]\n",
    "    \n",
    "    # For each sample in dataset\n",
    "    for sample_n in range(n_samples):\n",
    "        \n",
    "        # Find target file\n",
    "        target_file_path = 'intermediate/' + DATASET_NAME + '/' + INTERMEDIATE_SUBPATH + str(instruction_n + 1) + '_' + str(sample_n + 1) + '.txt'\n",
    "        if not os.path.isfile(target_file_path): # A MODIFIER : SI UN RESUME N'A PAS ETE GENERE\n",
    "            print('-- Found no intermediate result file \\'' + target_file_path + '\\', skipped.')\n",
    "            continue\n",
    "        \n",
    "        # Read sample and generate prompt -> Keep summary\n",
    "        sub_references = []\n",
    "        for reference in range(SUMMARIES_PER_SAMPLE):\n",
    "            summary_file_path = 'input/' + DATASET_NAME + '/summaries/sample_' + str(sample_n + 1) + '_' + str(reference + 1) + '.txt'\n",
    "            summary_file = open(summary_file_path, 'r', encoding='utf-8')\n",
    "            sub_references.append(summary_file.read())\n",
    "            summary_file.close()\n",
    "        references.append(sub_references)\n",
    "\n",
    "        # Access generated summary\n",
    "        target_file = open(target_file_path, 'r', encoding='utf-8')\n",
    "        prediction = target_file.read()\n",
    "        target_file.close()\n",
    "\n",
    "        # Process answer\n",
    "        if PREPROCESS_SUMMARIES:\n",
    "            separator = \"### Assistant:\"\n",
    "            prediction = prediction[prediction.index(separator) + len(separator):]\n",
    "            if prediction[0] == \" \": # Enlever l'espace devant\n",
    "                prediction = prediction[1:]\n",
    "            prediction = prediction[:-len(\"<|endoftext|>\") - 2]\n",
    "            #print(prediction)\n",
    "            #print('---------------------------------------')\n",
    "        \n",
    "        # Add prediction\n",
    "        target_file_paths.append(target_file_path)\n",
    "        predictions.append(prediction)\n",
    "\n",
    "# Calculate metrics\n",
    "result_rouge = rouge.compute(predictions=predictions, references=references)\n",
    "result_bleu = bleu.compute(predictions=predictions, references=references)\n",
    "result_bertscore = bertscore.compute(predictions=predictions, references=references, lang='fr', rescale_with_baseline=True, verbose=True)\n",
    "\n",
    "print(result_rouge)\n",
    "print(result_bleu)\n",
    "print(result_bertscore)\n",
    "\n",
    "if False:\n",
    "\n",
    "    # Write to csv\n",
    "    # Format: PATH | ROUGE2 | ROUGEL | BLEU | BERTScore\n",
    "    for i in range(len(target_file_paths)):\n",
    "        ligne = target_file_path[i] + ';'\n",
    "        ligne += ';' + str(result_rouge['rouge2']) + \";\" + str(result_rouge['rougeL'])\n",
    "        ligne += \";\" + str(result_bleu['bleu'])\n",
    "        ligne += \";\" + str(result_bertscore['f1'][0])\n",
    "        \n",
    "        storage_file.write(ligne + '\\n')\n",
    "\n",
    "    storage_file.close()\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch 2.0.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
