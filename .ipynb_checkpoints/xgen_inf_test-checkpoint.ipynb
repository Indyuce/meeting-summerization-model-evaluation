{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-api-python-client in ./.miniconda3/lib/python3.10/site-packages (2.92.0)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.15.0 in ./.miniconda3/lib/python3.10/site-packages (from google-api-python-client) (0.22.0)\n",
      "Requirement already satisfied: google-auth<3.0.0.dev0,>=1.19.0 in ./.miniconda3/lib/python3.10/site-packages (from google-api-python-client) (2.21.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.1.0 in ./.miniconda3/lib/python3.10/site-packages (from google-api-python-client) (0.1.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in ./.miniconda3/lib/python3.10/site-packages (from google-api-python-client) (2.11.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in ./.miniconda3/lib/python3.10/site-packages (from google-api-python-client) (4.1.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in ./.miniconda3/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.59.1)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in ./.miniconda3/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (4.23.4)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in ./.miniconda3/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.28.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./.miniconda3/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.miniconda3/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./.miniconda3/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (4.9)\n",
      "Requirement already satisfied: six>=1.9.0 in ./.miniconda3/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (1.16.0)\n",
      "Requirement already satisfied: urllib3<2.0 in ./.miniconda3/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (1.26.13)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in ./.miniconda3/lib/python3.10/site-packages (from httplib2<1.dev0,>=0.15.0->google-api-python-client) (3.0.9)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in ./.miniconda3/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.5.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./.miniconda3/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.miniconda3/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.miniconda3/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2022.12.7)\n",
      "Collecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-j_l0v4y9\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-j_l0v4y9\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 30ed3adf474aaf2972ab56f5624089bc24a6adf3\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in ./.miniconda3/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in ./.miniconda3/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.miniconda3/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.miniconda3/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.miniconda3/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.miniconda3/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (2023.6.3)\n",
      "Requirement already satisfied: requests in ./.miniconda3/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./.miniconda3/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.miniconda3/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (0.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.miniconda3/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (4.51.0)\n",
      "Requirement already satisfied: fsspec in ./.miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0.dev0) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0.dev0) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./.miniconda3/lib/python3.10/site-packages (from requests->transformers==4.31.0.dev0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.miniconda3/lib/python3.10/site-packages (from requests->transformers==4.31.0.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.miniconda3/lib/python3.10/site-packages (from requests->transformers==4.31.0.dev0) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.miniconda3/lib/python3.10/site-packages (from requests->transformers==4.31.0.dev0) (2022.12.7)\n",
      "Collecting git+https://github.com/huggingface/accelerate.git\n",
      "  Cloning https://github.com/huggingface/accelerate.git to /tmp/pip-req-build-fqiuygwj\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate.git /tmp/pip-req-build-fqiuygwj\n",
      "  Resolved https://github.com/huggingface/accelerate.git to commit 95bffdec4326acf6a5d1c3dbaa857a26502aa265\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in ./.miniconda3/lib/python3.10/site-packages (from accelerate==0.21.0.dev0) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.miniconda3/lib/python3.10/site-packages (from accelerate==0.21.0.dev0) (23.1)\n",
      "Requirement already satisfied: psutil in ./.miniconda3/lib/python3.10/site-packages (from accelerate==0.21.0.dev0) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in ./.miniconda3/lib/python3.10/site-packages (from accelerate==0.21.0.dev0) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in ./.miniconda3/lib/python3.10/site-packages (from accelerate==0.21.0.dev0) (2.0.0+cu118)\n",
      "Requirement already satisfied: filelock in ./.miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0.dev0) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in ./.miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0.dev0) (4.4.0)\n",
      "Requirement already satisfied: sympy in ./.miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0.dev0) (1.11.1)\n",
      "Requirement already satisfied: networkx in ./.miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0.dev0) (3.0)\n",
      "Requirement already satisfied: jinja2 in ./.miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0.dev0) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in ./.miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0.dev0) (2.0.0)\n",
      "Requirement already satisfied: cmake in ./.miniconda3/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.21.0.dev0) (3.25.0)\n",
      "Requirement already satisfied: lit in ./.miniconda3/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.21.0.dev0) (15.0.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.miniconda3/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.21.0.dev0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.miniconda3/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.21.0.dev0) (1.2.1)\n",
      "Requirement already satisfied: tiktoken in ./.miniconda3/lib/python3.10/site-packages (0.4.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.miniconda3/lib/python3.10/site-packages (from tiktoken) (2023.6.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./.miniconda3/lib/python3.10/site-packages (from tiktoken) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./.miniconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.miniconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.miniconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.miniconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
      "Requirement already satisfied: torch in ./.miniconda3/lib/python3.10/site-packages (2.0.0+cu118)\n",
      "Requirement already satisfied: filelock in ./.miniconda3/lib/python3.10/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in ./.miniconda3/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: sympy in ./.miniconda3/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in ./.miniconda3/lib/python3.10/site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in ./.miniconda3/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in ./.miniconda3/lib/python3.10/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: cmake in ./.miniconda3/lib/python3.10/site-packages (from triton==2.0.0->torch) (3.25.0)\n",
      "Requirement already satisfied: lit in ./.miniconda3/lib/python3.10/site-packages (from triton==2.0.0->torch) (15.0.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.miniconda3/lib/python3.10/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.miniconda3/lib/python3.10/site-packages (from sympy->torch) (1.2.1)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.3/36.3 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<1.28.0,>=1.21.6 in ./.miniconda3/lib/python3.10/site-packages (from scipy) (1.24.1)\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.11.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade google-api-python-client\n",
    "!pip install bitsandbytes>=0.39.0\n",
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "!pip install git+https://github.com/huggingface/accelerate.git\n",
    "!pip install tiktoken\n",
    "!pip install torch\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul 10 12:07:10 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100S-PCI...  Off  | 00000000:00:0B.0 Off |                    0 |\n",
      "| N/A   40C    P0    25W / 250W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:      181108932    10473132    73337164     5609268    97298636   163382608\n",
      "Swap:             0           0           0\n"
     ]
    }
   ],
   "source": [
    "!free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82126bfe7bc4648b7b6f990c8d7e1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading (…)model.bin.index.json'), FloatProgress(value=0.0, max=26788.0), HTML(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43932c4da0844906b50cccfc62095583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading shards'), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72757b7281ff4ba195e2b4b3ad038607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading (…)l-00001-of-00002.bin'), FloatProgress(value=0.0, max=9953548122.0), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10886008193c4116b5c32b6e0922a36b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading (…)l-00002-of-00002.bin'), FloatProgress(value=0.0, max=3837974747.0), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f92245f35755450da01d2e052cc7f405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Loading checkpoint shards'), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf32e1e5285441281a316b0ea582bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading (…)neration_config.json'), FloatProgress(value=0.0, max=140.0), HTML(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = 'legendhasit/xgen-7b-8k-inst-8bit'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\n",
      "\n",
      "### Human: Please summarize the most essential information from the following dialogue.\n",
      "\n",
      "E2: Dis voir Christiane, tu vas descendre bientôt chez ta soeur à Istres?\n",
      "C: (bruit de papier) Ah ben euh, on descend pour le mariage de Myriam et, et Emmanuel.\n",
      "C: Oui, (XX) sont en plein préparatifs, bien entendu.\n",
      "C: La petite Arlésienne, elle est mignonne comme tout, enfin bon.\n",
      "E2: Et alors elle aura la robe de l'Arlésienne non?\n",
      "C: Oh non, je pense pas qu'elle ait un costume régional, je pense.\n",
      "C: Je pense qu'elle aura une tenue euh <E2: De mariage traditionnel.> classique de mariée. Je pense.\n",
      "C: Remarque, on sait jamais hein, ça peut être la surprise, j'aimerais bien parce que c'est,\n",
      "C: c'est vrai que c'est joli, le costume d'Arlésienne. <Y: C'est très folklorique, oui, oui.>\n",
      "C: Je sais pas si elle, <Y: C'est très, très joli.> si elle en a un, (bruit de papier) j'ai jamais eu l'occasion de, <Y: Mais c'est fort possible que pour le mariage elle ait euh.> de voir.\n",
      "C: Oui, c'est possible, oui, oui, parce que sa grand-mère est native,\n",
      "C: est de là-bas aussi hein, donc elle est Arlésienne pur sucre.\n",
      "C: Et bon euh, c'est bien possible que dans la famille, ils aient un, un costume d'Arlésienne,\n",
      "C: mais. <E2: Et qu'elle en hérite un.> Oui, c'est possible. J'en, j'en n'ai pas entendu parler, je sais que euh, que euh\n",
      "C: la voiture sera la, la voiture du grand-père costellois, la euh.\n",
      "E2: Ah la, la traction? <C: Euh oui, la traction, la fameuse traction avant.>\n",
      "\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "header = (\n",
    "    \"A chat between a curious human and an artificial intelligence assistant. \"\n",
    "    \"The assistant gives helpful, detailed, and polite answers to the human's questions.\\n\"\n",
    ")\n",
    "\n",
    "instruction = \"Please summarize the most essential information from the following dialogue.\"\n",
    "\n",
    "in_context_learning = f\"\"\"\n",
    "### Human: {instruction}\n",
    "\n",
    "Les déboires du Groupe Casino sont une illustration spectaculaire de l’impact d’une remontée durable des taux d’intérêt sur les entreprises. Outre la fragilité de sa structure financière et sa stratégie internationale hasardeuse, le groupe souffre, hors des zones où il est un acteur dominant (essentiellement à Paris), d’un positionnement prix et d’un manque de modernisation des magasins qui ne lui permettent pas de conquérir de nouveaux clients. Avec une dette classée en catégorie spéculative par les agences de notation, Casino devait payer une importante prime à ses créanciers.\n",
    "\n",
    "Cette dette demeurait cependant soutenable tant que les taux de base restaient faibles. Au-delà du cas de Casino, l’argent « gratuit » permettait aux entreprises peu productives de maintenir leur activité ; pire, leurs créanciers étaient tentés par un « gambling for resurrection », c’est-à-dire un prolongement du financement des entreprises « zombies » en pariant sur un éventuel redressement du débiteur pour récupérer leur mise.\n",
    "\n",
    "### Assistant: Le groupe Casino est confronté à des problèmes liés à sa structure financière fragile et à une stratégie internationale risquée. En dehors des zones où il est un acteur dominant, principalement à Paris, Casino souffre d'une position tarifaire défavorable et d'un manque de modernisation de ses magasins, ce qui l'empêche de conquérir de nouveaux clients.\n",
    "\n",
    "Le groupe Casino a une dette qui est considérée comme spéculative par les agences de notation, ce qui signifie qu'il doit payer des intérêts plus élevés à ses créanciers. Cette dette était supportable tant que les taux d'intérêt de base restaient bas. Cependant, l'article souligne que l'argent bon marché permettait aux entreprises peu productives de maintenir leur activité. De plus, les créanciers étaient tentés de continuer à financer ces entreprises en difficulté en espérant qu'elles se redressent pour récupérer leur argent.\n",
    "\"\"\"\n",
    "\n",
    "article = \"\"\"\n",
    "E2: Dis voir Christiane, tu vas descendre bientôt chez ta soeur à Istres?\n",
    "C: (bruit de papier) Ah ben euh, on descend pour le mariage de Myriam et, et Emmanuel.\n",
    "C: Oui, (XX) sont en plein préparatifs, bien entendu.\n",
    "C: La petite Arlésienne, elle est mignonne comme tout, enfin bon.\n",
    "E2: Et alors elle aura la robe de l'Arlésienne non?\n",
    "C: Oh non, je pense pas qu'elle ait un costume régional, je pense.\n",
    "C: Je pense qu'elle aura une tenue euh <E2: De mariage traditionnel.> classique de mariée. Je pense.\n",
    "C: Remarque, on sait jamais hein, ça peut être la surprise, j'aimerais bien parce que c'est,\n",
    "C: c'est vrai que c'est joli, le costume d'Arlésienne. <Y: C'est très folklorique, oui, oui.>\n",
    "C: Je sais pas si elle, <Y: C'est très, très joli.> si elle en a un, (bruit de papier) j'ai jamais eu l'occasion de, <Y: Mais c'est fort possible que pour le mariage elle ait euh.> de voir.\n",
    "C: Oui, c'est possible, oui, oui, parce que sa grand-mère est native,\n",
    "C: est de là-bas aussi hein, donc elle est Arlésienne pur sucre.\n",
    "C: Et bon euh, c'est bien possible que dans la famille, ils aient un, un costume d'Arlésienne,\n",
    "C: mais. <E2: Et qu'elle en hérite un.> Oui, c'est possible. J'en, j'en n'ai pas entendu parler, je sais que euh, que euh\n",
    "C: la voiture sera la, la voiture du grand-père costellois, la euh.\n",
    "E2: Ah la, la traction? <C: Euh oui, la traction, la fameuse traction avant.>\n",
    "\"\"\"\n",
    "prompt = header + f\"\\n### Human: {instruction}\\n{article}\\n###\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\n",
      "\n",
      "### Human: Please summarize the most essential information from the following dialogue.\n",
      "\n",
      "E2: Dis voir Christiane, tu vas descendre bientôt chez ta soeur à Istres?\n",
      "C: (bruit de papier) Ah ben euh, on descend pour le mariage de Myriam et, et Emmanuel.\n",
      "C: Oui, (XX) sont en plein préparatifs, bien entendu.\n",
      "C: La petite Arlésienne, elle est mignonne comme tout, enfin bon.\n",
      "E2: Et alors elle aura la robe de l'Arlésienne non?\n",
      "C: Oh non, je pense pas qu'elle ait un costume régional, je pense.\n",
      "C: Je pense qu'elle aura une tenue euh <E2: De mariage traditionnel.> classique de mariée. Je pense.\n",
      "C: Remarque, on sait jamais hein, ça peut être la surprise, j'aimerais bien parce que c'est,\n",
      "C: c'est vrai que c'est joli, le costume d'Arlésienne. <Y: C'est très folklorique, oui, oui.>\n",
      "C: Je sais pas si elle, <Y: C'est très, très joli.> si elle en a un, (bruit de papier) j'ai jamais eu l'occasion de, <Y: Mais c'est fort possible que pour le mariage elle ait euh.> de voir.\n",
      "C: Oui, c'est possible, oui, oui, parce que sa grand-mère est native,\n",
      "C: est de là-bas aussi hein, donc elle est Arlésienne pur sucre.\n",
      "C: Et bon euh, c'est bien possible que dans la famille, ils aient un, un costume d'Arlésienne,\n",
      "C: mais. <E2: Et qu'elle en hérite un.> Oui, c'est possible. J'en, j'en n'ai pas entendu parler, je sais que euh, que euh\n",
      "C: la voiture sera la, la voiture du grand-père costellois, la euh.\n",
      "E2: Ah la, la traction? <C: Euh oui, la traction, la fameuse traction avant.>\n",
      "\n",
      "### Assistant: The conversation revolves around a woman named Myriam's upcoming wedding in Istres. The speaker mentions that her sister has a vehicle with a particular transmission known as a \"costellois\" transmission.\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = input_ids.to('cuda')\n",
    "sample = model.generate(**input_ids, do_sample=True, max_new_tokens=2048, top_k=100, eos_token_id=50256)\n",
    "output = tokenizer.decode(sample[0])\n",
    "print(output.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m input_ids\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m sample\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m output\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample' is not defined"
     ]
    }
   ],
   "source": [
    "del input_ids\n",
    "del sample\n",
    "del output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch 2.0.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
